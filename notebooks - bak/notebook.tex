
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{CNN on raw all}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{h5py}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{c+c1}{\PYZsh{}from plotnine import *}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k}{import} \PY{n}{KMeans}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{precision\PYZus{}score}\PY{p}{,} \PY{n}{recall\PYZus{}score}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{cohen\PYZus{}kappa\PYZus{}score}
        \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{PolynomialFeatures}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{LinearSVC}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeClassifier}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{export\PYZus{}graphviz}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{BaggingClassifier}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeClassifier}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{make\PYZus{}scorer}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingClassifier}
            
        \PY{c+c1}{\PYZsh{}import statsmodels.api as sm}
        
        \PY{c+c1}{\PYZsh{}from scipy.stats import mode}
        
        \PY{k+kn}{import} \PY{n+nn}{random}
        \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{k+kn}{import} \PY{n+nn}{pydot}
        \PY{k+kn}{from} \PY{n+nn}{pydot} \PY{k}{import} \PY{n}{Dot}
         
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{classic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{p}{)}
        
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
        \PY{c+c1}{\PYZsh{}import keras\PYZhy{}gpu}
        \PY{k+kn}{import} \PY{n+nn}{keras}
        \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{backend} \PY{k}{as} \PY{n}{K}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}\PY{p}{,} \PY{n}{Model}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Activation}\PY{p}{,} \PY{n}{Flatten}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{optimizers} \PY{k}{import} \PY{n}{SGD}\PY{p}{,} \PY{n}{Adam}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{to\PYZus{}categorical}
        \PY{c+c1}{\PYZsh{}from keras.np\PYZus{}utils import probas\PYZus{}to\PYZus{}classes }
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Input}\PY{p}{,} \PY{n}{Reshape}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Embedding}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Conv1D}\PY{p}{,} \PY{n}{GlobalAveragePooling1D}\PY{p}{,} \PY{n}{MaxPooling1D}\PY{p}{,} \PY{n}{merge}\PY{p}{,} \PY{n}{Concatenate}\PY{p}{,} \PY{n}{Conv2D}\PY{p}{,} \PY{n}{MaxPooling2D}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{merge} \PY{k}{import} \PY{n}{concatenate}\PY{p}{,} \PY{n}{add}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{load\PYZus{}model}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{plot\PYZus{}model}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{TensorBoard}
        \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{regularizers}
        
        \PY{k+kn}{import} \PY{n+nn}{os}
        \PY{n}{os}\PY{o}{.}\PY{n}{environ}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PATH}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{pathsep} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C:}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{Program Files (x86)}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{Graphviz2.38}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{bin}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Using TensorFlow backend.

    \end{Verbatim}

    Ask tenserflow not to be too gready on GPU

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{config} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{ConfigProto}\PY{p}{(}\PY{p}{)}
        \PY{n}{config}\PY{o}{.}\PY{n}{gpu\PYZus{}options}\PY{o}{.}\PY{n}{allow\PYZus{}growth} \PY{o}{=} \PY{k+kc}{True}
        \PY{n}{sess} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{n}{config}\PY{o}{=}\PY{n}{config}\PY{p}{)}
        
        \PY{n}{K}\PY{o}{.}\PY{n}{set\PYZus{}session}\PY{p}{(}\PY{n}{sess}\PY{p}{)}
        
        \PY{n}{keras}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}
        
        \PY{c+c1}{\PYZsh{}with eeg only 5 filters, kernel size 100}
        \PY{c+c1}{\PYZsh{}Test score:  1.3631985359422534}
        \PY{c+c1}{\PYZsh{}Test accuracy:  0.6440793987471519}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:} '2.2.0'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} \PYZpc{}load \PYZdq{}C:\PYZbs{}\PYZbs{}Users\PYZbs{}\PYZbs{}i053131\PYZbs{}Desktop\PYZbs{}\PYZbs{}Epilepsie\PYZbs{}\PYZbs{}Dreem\PYZbs{}\PYZbs{}src\PYZbs{}\PYZbs{}utils\PYZbs{}\PYZbs{}error.py\PYZdq{}}
        
        \PY{k}{def} \PY{n+nf}{AnalyzeError}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
            \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{sns}\PY{o}{.}\PY{n}{countplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{4000}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
            \PY{n}{sns}\PY{o}{.}\PY{n}{countplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{4000}\PY{p}{)}
            \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Actual and predicted distribution}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size} \PY{o}{=}  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x\PYZhy{}large}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
            
            \PY{n}{df\PYZus{}} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}
            \PY{n}{df\PYZus{}}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{=} \PY{n}{y\PYZus{}true}
            \PY{n}{df\PYZus{}}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pred}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}pred}
            \PY{n}{df\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test} \PY{o}{!=} \PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Pred}
            \PY{c+c1}{\PYZsh{}sns.countplot(x=\PYZdq{}Test\PYZdq{}, data=df\PYZus{}[df\PYZus{}.error])}
            
            \PY{n}{error0} \PY{o}{=} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{error}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{error1} \PY{o}{=} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{error}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{error2} \PY{o}{=} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{error}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{error3} \PY{o}{=} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{error}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{3}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{error4} \PY{o}{=} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{error}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/} \PY{n}{df\PYZus{}}\PY{p}{[}\PY{n}{df\PYZus{}}\PY{o}{.}\PY{n}{Test}\PY{o}{==}\PY{l+m+mi}{4}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        
            \PY{n}{Lerror} \PY{o}{=} \PY{p}{[}\PY{n}{error0}\PY{p}{,} \PY{n}{error1}\PY{p}{,} \PY{n}{error2}\PY{p}{,} \PY{n}{error3}\PY{p}{,} \PY{n}{error4}\PY{p}{]}
            \PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{Lerror}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Wrongly classified in a phase in percent of the test population for this phase}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{dataPath} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C:}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{Users}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{i053131}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{Desktop}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{Epilepsie}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{Dreem}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{data}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{raw}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{\PYZdq{}}
        
        \PY{n}{trainOutput} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{dataPath} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{challenge\PYZus{}fichier\PYZus{}de\PYZus{}sortie\PYZus{}dentrainement\PYZus{}classification\PYZus{}en\PYZus{}stade\PYZus{}de\PYZus{}sommeil\PYZus{}a\PYZus{}laide\PYZus{}de\PYZus{}signaux\PYZus{}mesures\PYZus{}par\PYZus{}le\PYZus{}bandeau\PYZus{}dreem.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{;}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{Y} \PY{o}{=} \PY{n}{trainOutput}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{label}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{filetrain}\PY{o}{=} \PY{n}{dataPath} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train.h5}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{filetest}\PY{o}{=} \PY{n}{dataPath} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{test.h5}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{h5} \PY{o}{=} \PY{n}{h5py}\PY{o}{.}\PY{n}{File}\PY{p}{(}\PY{n}{filetrain}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{eeg\PYZus{}1} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        \PY{n}{eeg\PYZus{}2} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        \PY{n}{eeg\PYZus{}3} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        \PY{n}{eeg\PYZus{}4} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        \PY{n}{po\PYZus{}ir} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{po\PYZus{}ir}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        \PY{n}{po\PYZus{}r} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{po\PYZus{}r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        \PY{n}{accelerometer\PYZus{}x} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accelerometer\PYZus{}x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        \PY{n}{accelerometer\PYZus{}y} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accelerometer\PYZus{}y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        \PY{n}{accelerometer\PYZus{}z} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{h5}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accelerometer\PYZus{}z}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    Make a train/test (80/20) sample of the whole data

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} EEG channels sampled at 125Hz (4 x 125 x 30 = 15000 size per sample)}
        \PY{c+c1}{\PYZsh{} Pulse Oxymeter channels (red and infra\PYZhy{}red) sampled at 50 Hz (2 x 50 x 30 = 3000 size per sample)}
        \PY{c+c1}{\PYZsh{} Accelerometer channels sampled at 50Hz (3 x 50 x 30 = 4500 size per sample)}
        
        \PY{c+c1}{\PYZsh{}step 1 eeg alone}
        \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}1}\PY{p}{,} \PY{n}{eeg\PYZus{}2}\PY{p}{,} \PY{n}{eeg\PYZus{}3}\PY{p}{,} \PY{n}{eeg\PYZus{}4}\PY{p}{,} \PY{n}{po\PYZus{}ir}\PY{p}{,} \PY{n}{po\PYZus{}r}\PY{p}{,} \PY{n}{accelerometer\PYZus{}x}\PY{p}{,} \PY{n}{accelerometer\PYZus{}y}\PY{p}{,} \PY{n}{accelerometer\PYZus{}z}\PY{p}{]}\PY{p}{,} 
                       \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{sort} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}
        \PY{n}{df}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{15000} \PY{o}{+} \PY{l+m+mi}{3000} \PY{o}{+} \PY{l+m+mi}{4500}\PY{p}{)}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{Y}
        
        \PY{n}{training}\PY{p}{,} \PY{n}{test}  \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
        \PY{n}{X} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{values}
        \PY{n}{y} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
        \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
        \PY{n}{y\PYZus{}true} \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
        \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


    check the sleep phases distribution on test and train sample

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{sns}\PY{o}{.}\PY{n}{countplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{test}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x1c70643a6a0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{sns}\PY{o}{.}\PY{n}{countplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{training}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x1c712f3f588>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Reshape data set - eeg: 4 layers of 125\emph{30 columns - pulse: 2 layer
of 50}columns - accelerometer: 3 layer of 50*columns

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{}Input to keras.layers.Conv1D should be 3\PYZhy{}d with dimensions (nb\PYZus{}of\PYZus{}examples, timesteps, features)}
         \PY{c+c1}{\PYZsh{}X is (nb\PYZus{}of\PYZus{}examples, timesteps)}
         
         \PY{c+c1}{\PYZsh{} EEG channels sampled at 125Hz (4 x 125 x 30 = 15000 size per sample)}
         \PY{c+c1}{\PYZsh{} Pulse Oxymeter channels (red and infra\PYZhy{}red) sampled at 50 Hz (2 x 50 x 30 = 3000 size per sample)}
         \PY{c+c1}{\PYZsh{} Accelerometer channels sampled at 50Hz (3 x 50 x 30 = 4500 size per sample)}
         
         \PY{n}{eeg\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{35064}\PY{p}{,} \PY{l+m+mi}{3750}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
         \PY{n}{eeg\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{3750}\PY{p}{]} \PY{c+c1}{\PYZsh{}eeg\PYZus{}1}
         \PY{n}{eeg\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{p}{:}\PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{]} \PY{c+c1}{\PYZsh{}eeg\PYZus{}2}
         \PY{n}{eeg\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{:}\PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{]} \PY{c+c1}{\PYZsh{}eeg\PYZus{}3}
         \PY{n}{eeg\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{:}\PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4}\PY{p}{]} \PY{c+c1}{\PYZsh{}eeg\PYZus{}4}
         
         \PY{n}{eeg\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8766}\PY{p}{,} \PY{l+m+mi}{3750}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
         \PY{n}{eeg\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{3750}\PY{p}{]} \PY{c+c1}{\PYZsh{}eeg\PYZus{}1}
         \PY{n}{eeg\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{p}{:}\PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{]} \PY{c+c1}{\PYZsh{}eeg\PYZus{}2}
         \PY{n}{eeg\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{:}\PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{]} \PY{c+c1}{\PYZsh{}eeg\PYZus{}3}
         \PY{n}{eeg\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{:}\PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4}\PY{p}{]} \PY{c+c1}{\PYZsh{}eeg\PYZus{}4}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eeg\PYZus{}train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         
         \PY{n}{pulse\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{35064}\PY{p}{,} \PY{l+m+mi}{1500}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{pulse\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{p}{]} \PY{c+c1}{\PYZsh{}po\PYZus{}ir}
         \PY{n}{pulse\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+}\PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{]} \PY{c+c1}{\PYZsh{}po\PYZus{}r}
         
         \PY{n}{pulse\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8766}\PY{p}{,} \PY{l+m+mi}{1500}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{pulse\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{p}{]} \PY{c+c1}{\PYZsh{}po\PYZus{}ir}
         \PY{n}{pulse\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{]} \PY{c+c1}{\PYZsh{}po\PYZus{}r}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pulse\PYZus{}train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         
         \PY{n}{accelerometer\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{35064}\PY{p}{,} \PY{l+m+mi}{1500}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
         \PY{n}{accelerometer\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{2} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{]} \PY{c+c1}{\PYZsh{}accelerometer\PYZus{}x}
         \PY{n}{accelerometer\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{3} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{4}\PY{p}{]} \PY{c+c1}{\PYZsh{}accelerometer\PYZus{}y}
         \PY{n}{accelerometer\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{4} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{]} \PY{c+c1}{\PYZsh{}accelerometer\PYZus{}z}
         
         \PY{n}{accelerometer\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8766}\PY{p}{,} \PY{l+m+mi}{1500}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
         \PY{n}{accelerometer\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{2} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{]} \PY{c+c1}{\PYZsh{}accelerometer\PYZus{}x}
         \PY{n}{accelerometer\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{3} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{4}\PY{p}{]} \PY{c+c1}{\PYZsh{}accelerometer\PYZus{}y}
         \PY{n}{accelerometer\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{4} \PY{p}{:} \PY{l+m+mi}{3750}\PY{o}{*}\PY{l+m+mi}{4} \PY{o}{+} \PY{l+m+mi}{1500}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{]} \PY{c+c1}{\PYZsh{}accelerometer\PYZus{}z}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accelerometer\PYZus{}train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
eeg\_train (35064, 3750, 4)
pulse\_train (35064, 1500, 2)
accelerometer\_train (35064, 1500, 2)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k}{def} \PY{n+nf}{fitmodel}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,}
                      \PY{n}{dropout}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{modelfile}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
         
             \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv1D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling1D}\PY{p}{(}\PY{n}{maxPool1}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv1D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel2}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling1D}\PY{p}{(}\PY{n}{maxPool2}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{k}{if} \PY{p}{(}\PY{n}{dropout}\PY{p}{)}\PY{p}{:}
                 \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{}optimizer = SGD(lr=0.0001)}
             \PY{c+c1}{\PYZsh{}optimizer=\PYZsq{}rmsprop\PYZsq{}}
             \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}
         
             \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                           \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,}
                           \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
             \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
             \PY{n}{score} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test score: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test accuracy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             
             \PY{k}{if} \PY{p}{(}\PY{n}{modelfile}\PY{p}{)}\PY{p}{:}
                 \PY{n}{model}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C:}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{Users}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{i053131}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{Desktop}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{Epilepsie}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{Dreem}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{data}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{models}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{modelfile}\PY{p}{)}\PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
             \PY{k}{return} \PY{p}{[}\PY{n}{model}\PY{p}{,} \PY{n}{history}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{def} \PY{n+nf}{modelPerf}\PY{p}{(}\PY{n}{L}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}true} \PY{o}{=} \PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{sequential}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} L = [model, history]}
             \PY{n}{model} \PY{o}{=} \PY{n}{L}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{history} \PY{o}{=} \PY{n}{L}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
             
             \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             
             \PY{k}{if} \PY{p}{(}\PY{n}{sequential}\PY{p}{)}\PY{p}{:}
                 \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}classes}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{)}\PY{p}{)}
             \PY{k}{else}\PY{p}{:}
                 \PY{n}{y\PYZus{}probas} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
                 \PY{n}{y\PYZus{}classes} \PY{o}{=} \PY{n}{y\PYZus{}probas}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{}keras.np\PYZus{}utils.probas\PYZus{}to\PYZus{}classes(y\PYZus{}probas)}
                 \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{y\PYZus{}classes}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accurancy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kappa: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cohen\PYZus{}kappa\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
             \PY{k}{if} \PY{p}{(}\PY{n}{verbose}\PY{p}{)}\PY{p}{:}
                 \PY{n}{AnalyzeError}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{}model = load\PYZus{}model(\PYZsq{}C:\PYZbs{}\PYZbs{}Users\PYZbs{}\PYZbs{}i053131\PYZbs{}\PYZbs{}Desktop\PYZbs{}\PYZbs{}Epilepsie\PYZbs{}\PYZbs{}Dreem\PYZbs{}\PYZbs{}data\PYZbs{}\PYZbs{}interim\PYZbs{}\PYZbs{}model\PYZus{}eeg1\PYZus{}5\PYZus{}100.h5\PYZsq{})}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{}plot\PYZus{}model(toto, to\PYZus{}file=\PYZsq{}convolutional\PYZus{}neural\PYZus{}network.png\PYZsq{})}
\end{Verbatim}


    \section{Separate model for eeg, pulse and
accelerometers}\label{separate-model-for-eeg-pulse-and-accelerometers}

\begin{itemize}
\tightlist
\item
  first convolution nb\_filter1=5, kernel1=25, maxPool1=25
\item
  second convolution nb\_filter2=20, kernel1=10, maxPool1=10
\item
  both hidden dense layer have 500 nodes
\end{itemize}

optimizer=Adam(0.0001)

\begin{figure}
\centering
\includegraphics{conv1 eeg.png}
\caption{title}
\end{figure}

Score after 50 epochs for eeg - accurancy: 0.627 - kappa: 0.472

\includegraphics{eeg_acc1.png} \includegraphics{eeg_loss1.png}

for pulse - accurancy: 0.37 - kappa: 0.0027

\includegraphics{pulse_acc1.png} \includegraphics{pulse_loss1.png}

for accelerometer - accurancy: 0.487 - kappa: 0.27

\includegraphics{accel_acc1.png} \includegraphics{accel_loss1.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{meeg1} \PY{o}{=} \PY{n}{fitmodel}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", input\_shape=(3750, 4), filters=5)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  \# Remove the CWD from sys.path while we load stuff.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/50
35064/35064 [==============================] - 15s 420us/step - loss: 5.3473 - acc: 0.3899 - val\_loss: 4.4427 - val\_acc: 0.4212
Epoch 2/50
35064/35064 [==============================] - 14s 406us/step - loss: 4.4105 - acc: 0.4310 - val\_loss: 3.9750 - val\_acc: 0.4273
Epoch 3/50
35064/35064 [==============================] - 14s 407us/step - loss: 3.7684 - acc: 0.4574 - val\_loss: 3.7890 - val\_acc: 0.4593
Epoch 4/50
35064/35064 [==============================] - 14s 408us/step - loss: 3.3564 - acc: 0.4784 - val\_loss: 3.1426 - val\_acc: 0.4830
Epoch 5/50
35064/35064 [==============================] - 14s 404us/step - loss: 2.9892 - acc: 0.4976 - val\_loss: 2.9019 - val\_acc: 0.4994
Epoch 6/50
35064/35064 [==============================] - 14s 405us/step - loss: 2.9387 - acc: 0.4957 - val\_loss: 2.9322 - val\_acc: 0.4774
Epoch 7/50
35064/35064 [==============================] - 14s 404us/step - loss: 2.6958 - acc: 0.5209 - val\_loss: 2.9023 - val\_acc: 0.4993
Epoch 8/50
35064/35064 [==============================] - 14s 405us/step - loss: 2.6150 - acc: 0.5298 - val\_loss: 2.5570 - val\_acc: 0.5224
Epoch 9/50
35064/35064 [==============================] - 14s 412us/step - loss: 2.6738 - acc: 0.5331 - val\_loss: 2.7831 - val\_acc: 0.4910
Epoch 10/50
35064/35064 [==============================] - 14s 404us/step - loss: 2.5371 - acc: 0.5390 - val\_loss: 2.6232 - val\_acc: 0.5224
Epoch 11/50
35064/35064 [==============================] - 14s 404us/step - loss: 2.2816 - acc: 0.5629 - val\_loss: 2.3860 - val\_acc: 0.5181
Epoch 12/50
35064/35064 [==============================] - 14s 403us/step - loss: 2.1242 - acc: 0.5717 - val\_loss: 2.2850 - val\_acc: 0.5449
Epoch 13/50
35064/35064 [==============================] - 14s 404us/step - loss: 2.0293 - acc: 0.5738 - val\_loss: 2.1518 - val\_acc: 0.5576
Epoch 14/50
35064/35064 [==============================] - 15s 430us/step - loss: 2.1013 - acc: 0.5648 - val\_loss: 2.7527 - val\_acc: 0.4889
Epoch 15/50
35064/35064 [==============================] - 15s 436us/step - loss: 2.1427 - acc: 0.5608 - val\_loss: 2.0992 - val\_acc: 0.5647
Epoch 16/50
35064/35064 [==============================] - 14s 404us/step - loss: 2.1609 - acc: 0.5511 - val\_loss: 2.1905 - val\_acc: 0.5362
Epoch 17/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.8556 - acc: 0.5824 - val\_loss: 1.9846 - val\_acc: 0.5688
Epoch 18/50
35064/35064 [==============================] - 14s 405us/step - loss: 1.7942 - acc: 0.5875 - val\_loss: 2.0113 - val\_acc: 0.5679
Epoch 19/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.6555 - acc: 0.6072 - val\_loss: 1.9616 - val\_acc: 0.5763
Epoch 20/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.7458 - acc: 0.5973 - val\_loss: 2.1804 - val\_acc: 0.5508
Epoch 21/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.6714 - acc: 0.6107 - val\_loss: 1.9592 - val\_acc: 0.5735
Epoch 22/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.5273 - acc: 0.6240 - val\_loss: 2.1136 - val\_acc: 0.5388
Epoch 23/50
35064/35064 [==============================] - 14s 409us/step - loss: 1.6024 - acc: 0.6125 - val\_loss: 1.9098 - val\_acc: 0.5860
Epoch 24/50
35064/35064 [==============================] - 14s 408us/step - loss: 1.5290 - acc: 0.6186 - val\_loss: 1.8755 - val\_acc: 0.5828
Epoch 25/50
35064/35064 [==============================] - 14s 406us/step - loss: 1.4577 - acc: 0.6320 - val\_loss: 1.7885 - val\_acc: 0.5969
Epoch 26/50
35064/35064 [==============================] - 14s 406us/step - loss: 1.3862 - acc: 0.6386 - val\_loss: 1.7956 - val\_acc: 0.5925
Epoch 27/50
35064/35064 [==============================] - 14s 408us/step - loss: 1.3444 - acc: 0.6443 - val\_loss: 1.8121 - val\_acc: 0.5986
Epoch 28/50
35064/35064 [==============================] - 14s 405us/step - loss: 1.3904 - acc: 0.6422 - val\_loss: 1.7982 - val\_acc: 0.5907
Epoch 29/50
35064/35064 [==============================] - 14s 405us/step - loss: 1.4018 - acc: 0.6403 - val\_loss: 1.8651 - val\_acc: 0.5909
Epoch 30/50
35064/35064 [==============================] - 15s 435us/step - loss: 1.3642 - acc: 0.6472 - val\_loss: 1.8037 - val\_acc: 0.5957
Epoch 31/50
35064/35064 [==============================] - 14s 401us/step - loss: 1.4555 - acc: 0.6344 - val\_loss: 1.7650 - val\_acc: 0.6015
Epoch 32/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.3450 - acc: 0.6494 - val\_loss: 1.7739 - val\_acc: 0.5988
Epoch 33/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.2904 - acc: 0.6572 - val\_loss: 1.7407 - val\_acc: 0.6048
Epoch 34/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.3017 - acc: 0.6600 - val\_loss: 1.7372 - val\_acc: 0.6026
Epoch 35/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.2073 - acc: 0.6743 - val\_loss: 1.7183 - val\_acc: 0.6086
Epoch 36/50
35064/35064 [==============================] - 14s 403us/step - loss: 1.1753 - acc: 0.6783 - val\_loss: 1.7888 - val\_acc: 0.5983
Epoch 37/50
35064/35064 [==============================] - 14s 403us/step - loss: 1.1598 - acc: 0.6802 - val\_loss: 1.7416 - val\_acc: 0.6112
Epoch 38/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.2140 - acc: 0.6722 - val\_loss: 1.7742 - val\_acc: 0.6110
Epoch 39/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.1384 - acc: 0.6863 - val\_loss: 1.6381 - val\_acc: 0.6227
Epoch 40/50
35064/35064 [==============================] - 14s 405us/step - loss: 1.0878 - acc: 0.6966 - val\_loss: 1.7196 - val\_acc: 0.6178
Epoch 41/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.1281 - acc: 0.6876 - val\_loss: 1.6825 - val\_acc: 0.6215
Epoch 42/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.0921 - acc: 0.6958 - val\_loss: 1.6905 - val\_acc: 0.6226
Epoch 43/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.2148 - acc: 0.6774 - val\_loss: 1.6736 - val\_acc: 0.6240
Epoch 44/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.0830 - acc: 0.6964 - val\_loss: 1.7775 - val\_acc: 0.6115
Epoch 45/50
35064/35064 [==============================] - 14s 404us/step - loss: 1.0805 - acc: 0.7019 - val\_loss: 1.7161 - val\_acc: 0.6274
Epoch 46/50
35064/35064 [==============================] - 14s 405us/step - loss: 1.0555 - acc: 0.7055 - val\_loss: 1.6688 - val\_acc: 0.6262
Epoch 47/50
35064/35064 [==============================] - 14s 405us/step - loss: 1.0506 - acc: 0.7091 - val\_loss: 1.6525 - val\_acc: 0.6353
Epoch 48/50
35064/35064 [==============================] - 14s 403us/step - loss: 1.1147 - acc: 0.7029 - val\_loss: 1.7094 - val\_acc: 0.6189
Epoch 49/50
35064/35064 [==============================] - 14s 403us/step - loss: 1.0281 - acc: 0.7141 - val\_loss: 1.6700 - val\_acc: 0.6283
Epoch 50/50
35064/35064 [==============================] - 14s 403us/step - loss: 0.9882 - acc: 0.7209 - val\_loss: 1.6391 - val\_acc: 0.6287
8766/8766 [==============================] - 2s 205us/step
Test score:  1.6390563114957397
Test accuracy:  0.628678989157458

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{n}{meeg1}\PY{p}{,} \PY{n}{eeg\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv1d\_5 (Conv1D)            (None, 3726, 5)           505       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_5 (MaxPooling1 (None, 149, 5)            0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_6 (Conv1D)            (None, 140, 20)           1020      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_6 (MaxPooling1 (None, 14, 20)            0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_3 (Flatten)          (None, 280)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_7 (Dense)              (None, 500)               140500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_8 (Dense)              (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_9 (Dense)              (None, 5)                 2505      
=================================================================
Total params: 395,030
Trainable params: 395,030
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.6274241387177732
kappa:  0.47158363728453456

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{n}{mpulse1} \PY{o}{=} \PY{n}{fitmodel}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                            \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{epoch}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
         \PY{n}{macc1} \PY{o}{=} \PY{n}{fitmodel}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                            \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{epoch}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", input\_shape=(1500, 2), filters=5)`
  
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  \# Remove the CWD from sys.path while we load stuff.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/50
35064/35064 [==============================] - 20s 575us/step - loss: 3.0915 - acc: 0.3238 - val\_loss: 2.5509 - val\_acc: 0.3603
Epoch 2/50
35064/35064 [==============================] - 20s 580us/step - loss: 2.2818 - acc: 0.3466 - val\_loss: 2.1889 - val\_acc: 0.3634
Epoch 3/50
35064/35064 [==============================] - 21s 585us/step - loss: 2.1173 - acc: 0.3592 - val\_loss: 2.2279 - val\_acc: 0.3252
Epoch 4/50
35064/35064 [==============================] - 21s 603us/step - loss: 2.1003 - acc: 0.3590 - val\_loss: 2.2054 - val\_acc: 0.3460
Epoch 5/50
35064/35064 [==============================] - 20s 585us/step - loss: 2.1494 - acc: 0.3617 - val\_loss: 2.1882 - val\_acc: 0.3507
Epoch 6/50
35064/35064 [==============================] - 19s 553us/step - loss: 2.1239 - acc: 0.3647 - val\_loss: 2.2427 - val\_acc: 0.3436
Epoch 7/50
35064/35064 [==============================] - 19s 556us/step - loss: 2.1788 - acc: 0.3646 - val\_loss: 2.3449 - val\_acc: 0.3401
Epoch 8/50
35064/35064 [==============================] - 19s 554us/step - loss: 2.0254 - acc: 0.3614 - val\_loss: 1.9877 - val\_acc: 0.3698
Epoch 9/50
35064/35064 [==============================] - 19s 550us/step - loss: 1.9310 - acc: 0.3714 - val\_loss: 1.8925 - val\_acc: 0.3752
Epoch 10/50
35064/35064 [==============================] - 19s 555us/step - loss: 2.1174 - acc: 0.3603 - val\_loss: 1.9150 - val\_acc: 0.3653
Epoch 11/50
35064/35064 [==============================] - 19s 554us/step - loss: 1.9153 - acc: 0.3679 - val\_loss: 1.9060 - val\_acc: 0.3705
Epoch 12/50
35064/35064 [==============================] - 20s 557us/step - loss: 1.9359 - acc: 0.3646 - val\_loss: 1.9309 - val\_acc: 0.3698
Epoch 13/50
35064/35064 [==============================] - 20s 559us/step - loss: 1.8337 - acc: 0.3708 - val\_loss: 1.8413 - val\_acc: 0.3730
Epoch 14/50
35064/35064 [==============================] - 19s 555us/step - loss: 1.8388 - acc: 0.3742 - val\_loss: 1.9594 - val\_acc: 0.3720
Epoch 15/50
35064/35064 [==============================] - 19s 551us/step - loss: 1.9434 - acc: 0.3672 - val\_loss: 2.0285 - val\_acc: 0.3725
Epoch 16/50
35064/35064 [==============================] - 20s 571us/step - loss: 1.9985 - acc: 0.3632 - val\_loss: 1.9901 - val\_acc: 0.3692
Epoch 17/50
35064/35064 [==============================] - 19s 552us/step - loss: 1.9007 - acc: 0.3684 - val\_loss: 1.8977 - val\_acc: 0.3766
Epoch 18/50
35064/35064 [==============================] - 19s 545us/step - loss: 1.8678 - acc: 0.3703 - val\_loss: 1.8881 - val\_acc: 0.3761
Epoch 19/50
35064/35064 [==============================] - 19s 542us/step - loss: 1.8412 - acc: 0.3700 - val\_loss: 1.8549 - val\_acc: 0.3766
Epoch 20/50
35064/35064 [==============================] - 19s 542us/step - loss: 1.8465 - acc: 0.3692 - val\_loss: 1.9088 - val\_acc: 0.3726
Epoch 21/50
35064/35064 [==============================] - 19s 545us/step - loss: 1.8630 - acc: 0.3682 - val\_loss: 1.8882 - val\_acc: 0.3747
Epoch 22/50
35064/35064 [==============================] - 19s 543us/step - loss: 1.8256 - acc: 0.3709 - val\_loss: 1.9205 - val\_acc: 0.3731
Epoch 23/50
35064/35064 [==============================] - 19s 541us/step - loss: 1.8614 - acc: 0.3687 - val\_loss: 1.8710 - val\_acc: 0.3760
Epoch 24/50
35064/35064 [==============================] - 19s 544us/step - loss: 1.8674 - acc: 0.3696 - val\_loss: 1.9345 - val\_acc: 0.3745
Epoch 25/50
35064/35064 [==============================] - 19s 543us/step - loss: 1.9214 - acc: 0.3606 - val\_loss: 1.9412 - val\_acc: 0.3706
Epoch 26/50
35064/35064 [==============================] - 19s 542us/step - loss: 1.8655 - acc: 0.3686 - val\_loss: 1.8654 - val\_acc: 0.3738
Epoch 27/50
35064/35064 [==============================] - 19s 542us/step - loss: 1.8355 - acc: 0.3684 - val\_loss: 1.8464 - val\_acc: 0.3761
Epoch 28/50
35064/35064 [==============================] - 19s 545us/step - loss: 1.8274 - acc: 0.3685 - val\_loss: 1.8791 - val\_acc: 0.3739
Epoch 29/50
35064/35064 [==============================] - 19s 546us/step - loss: 1.8205 - acc: 0.3679 - val\_loss: 1.8338 - val\_acc: 0.3749
Epoch 30/50
35064/35064 [==============================] - 19s 541us/step - loss: 1.7925 - acc: 0.3698 - val\_loss: 1.8200 - val\_acc: 0.3741
Epoch 31/50
35064/35064 [==============================] - 19s 552us/step - loss: 1.7791 - acc: 0.3705 - val\_loss: 1.8135 - val\_acc: 0.3737
Epoch 32/50
35064/35064 [==============================] - 20s 562us/step - loss: 1.7661 - acc: 0.3712 - val\_loss: 1.7997 - val\_acc: 0.3747
Epoch 33/50
35064/35064 [==============================] - 20s 571us/step - loss: 1.8129 - acc: 0.3677 - val\_loss: 1.8127 - val\_acc: 0.3750
Epoch 34/50
35064/35064 [==============================] - 20s 559us/step - loss: 1.8020 - acc: 0.3677 - val\_loss: 1.8236 - val\_acc: 0.3749
Epoch 35/50
35064/35064 [==============================] - 19s 539us/step - loss: 1.7823 - acc: 0.3679 - val\_loss: 1.8108 - val\_acc: 0.3741
Epoch 36/50
35064/35064 [==============================] - 19s 536us/step - loss: 1.7853 - acc: 0.3671 - val\_loss: 1.8019 - val\_acc: 0.3743
Epoch 37/50
35064/35064 [==============================] - 19s 539us/step - loss: 1.7836 - acc: 0.3677 - val\_loss: 1.8139 - val\_acc: 0.3735
Epoch 38/50
35064/35064 [==============================] - 19s 540us/step - loss: 1.7881 - acc: 0.3678 - val\_loss: 1.8038 - val\_acc: 0.3736
Epoch 39/50
35064/35064 [==============================] - 19s 537us/step - loss: 1.7707 - acc: 0.3679 - val\_loss: 1.7882 - val\_acc: 0.3739
Epoch 40/50
35064/35064 [==============================] - 19s 539us/step - loss: 1.7734 - acc: 0.3669 - val\_loss: 1.7877 - val\_acc: 0.3751
Epoch 41/50
35064/35064 [==============================] - 19s 537us/step - loss: 1.7595 - acc: 0.3679 - val\_loss: 1.7837 - val\_acc: 0.3739
Epoch 42/50
35064/35064 [==============================] - 19s 537us/step - loss: 1.7530 - acc: 0.3687 - val\_loss: 1.7899 - val\_acc: 0.3744
Epoch 43/50
35064/35064 [==============================] - 19s 541us/step - loss: 1.7446 - acc: 0.3697 - val\_loss: 1.7817 - val\_acc: 0.3736
Epoch 44/50
35064/35064 [==============================] - 19s 539us/step - loss: 1.7570 - acc: 0.3686 - val\_loss: 1.7883 - val\_acc: 0.3739
Epoch 45/50
35064/35064 [==============================] - 19s 540us/step - loss: 1.7518 - acc: 0.3686 - val\_loss: 1.8157 - val\_acc: 0.3719
Epoch 46/50
35064/35064 [==============================] - 19s 539us/step - loss: 1.7487 - acc: 0.3681 - val\_loss: 1.7801 - val\_acc: 0.3720
Epoch 47/50
35064/35064 [==============================] - 19s 545us/step - loss: 1.7676 - acc: 0.3674 - val\_loss: 1.8235 - val\_acc: 0.3703
Epoch 48/50
35064/35064 [==============================] - 19s 541us/step - loss: 1.7430 - acc: 0.3688 - val\_loss: 1.8164 - val\_acc: 0.3705
Epoch 49/50
35064/35064 [==============================] - 19s 538us/step - loss: 1.7355 - acc: 0.3692 - val\_loss: 1.8102 - val\_acc: 0.3702
Epoch 50/50
35064/35064 [==============================] - 19s 536us/step - loss: 1.7285 - acc: 0.3697 - val\_loss: 1.8145 - val\_acc: 0.3700
8766/8766 [==============================] - 2s 191us/step
Test score:  1.814532836387072
Test accuracy:  0.3699520881211896

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", input\_shape=(1500, 3), filters=5)`
  
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  \# Remove the CWD from sys.path while we load stuff.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/50
35064/35064 [==============================] - 27s 762us/step - loss: 1.4225 - acc: 0.3719 - val\_loss: 1.3625 - val\_acc: 0.3862
Epoch 2/50
35064/35064 [==============================] - 24s 674us/step - loss: 1.3659 - acc: 0.3769 - val\_loss: 1.3590 - val\_acc: 0.3881
Epoch 3/50
35064/35064 [==============================] - 25s 706us/step - loss: 1.3628 - acc: 0.3778 - val\_loss: 1.3562 - val\_acc: 0.3890
Epoch 4/50
35064/35064 [==============================] - 25s 711us/step - loss: 1.3594 - acc: 0.3786 - val\_loss: 1.3533 - val\_acc: 0.3900
Epoch 5/50
35064/35064 [==============================] - 24s 676us/step - loss: 1.3562 - acc: 0.3795 - val\_loss: 1.3507 - val\_acc: 0.3912
Epoch 6/50
35064/35064 [==============================] - 24s 672us/step - loss: 1.3515 - acc: 0.3814 - val\_loss: 1.3465 - val\_acc: 0.3925
Epoch 7/50
35064/35064 [==============================] - 24s 673us/step - loss: 1.3440 - acc: 0.3860 - val\_loss: 1.3340 - val\_acc: 0.4054
Epoch 8/50
35064/35064 [==============================] - 24s 676us/step - loss: 1.3274 - acc: 0.3971 - val\_loss: 1.3153 - val\_acc: 0.4102
Epoch 9/50
35064/35064 [==============================] - 24s 672us/step - loss: 1.3070 - acc: 0.4032 - val\_loss: 1.3002 - val\_acc: 0.3986
Epoch 10/50
35064/35064 [==============================] - 24s 675us/step - loss: 1.2942 - acc: 0.4041 - val\_loss: 1.2877 - val\_acc: 0.4005
Epoch 11/50
35064/35064 [==============================] - 24s 673us/step - loss: 1.2852 - acc: 0.4058 - val\_loss: 1.2800 - val\_acc: 0.4078
Epoch 12/50
35064/35064 [==============================] - 24s 673us/step - loss: 1.2800 - acc: 0.4089 - val\_loss: 1.2768 - val\_acc: 0.4010
Epoch 13/50
35064/35064 [==============================] - 24s 682us/step - loss: 1.2758 - acc: 0.4104 - val\_loss: 1.2758 - val\_acc: 0.4036
Epoch 14/50
35064/35064 [==============================] - 24s 688us/step - loss: 1.2719 - acc: 0.4112 - val\_loss: 1.2698 - val\_acc: 0.4182
Epoch 15/50
35064/35064 [==============================] - 24s 672us/step - loss: 1.2682 - acc: 0.4131 - val\_loss: 1.2654 - val\_acc: 0.4183
Epoch 16/50
35064/35064 [==============================] - 23s 670us/step - loss: 1.2650 - acc: 0.4162 - val\_loss: 1.2632 - val\_acc: 0.4172
Epoch 17/50
35064/35064 [==============================] - 24s 671us/step - loss: 1.2622 - acc: 0.4170 - val\_loss: 1.2610 - val\_acc: 0.4187
Epoch 18/50
35064/35064 [==============================] - 23s 669us/step - loss: 1.2589 - acc: 0.4190 - val\_loss: 1.2600 - val\_acc: 0.4181
Epoch 19/50
35064/35064 [==============================] - 23s 667us/step - loss: 1.2561 - acc: 0.4209 - val\_loss: 1.2564 - val\_acc: 0.4212
Epoch 20/50
35064/35064 [==============================] - 23s 665us/step - loss: 1.2527 - acc: 0.4252 - val\_loss: 1.2519 - val\_acc: 0.4262
Epoch 21/50
35064/35064 [==============================] - 23s 667us/step - loss: 1.2493 - acc: 0.4255 - val\_loss: 1.2545 - val\_acc: 0.4237
Epoch 22/50
35064/35064 [==============================] - 23s 664us/step - loss: 1.2458 - acc: 0.4291 - val\_loss: 1.2460 - val\_acc: 0.4262
Epoch 23/50
35064/35064 [==============================] - 23s 666us/step - loss: 1.2420 - acc: 0.4308 - val\_loss: 1.2421 - val\_acc: 0.4335
Epoch 24/50
35064/35064 [==============================] - 23s 668us/step - loss: 1.2385 - acc: 0.4330 - val\_loss: 1.2418 - val\_acc: 0.4329
Epoch 25/50
35064/35064 [==============================] - 23s 665us/step - loss: 1.2352 - acc: 0.4355 - val\_loss: 1.2412 - val\_acc: 0.4296
Epoch 26/50
35064/35064 [==============================] - 23s 667us/step - loss: 1.2318 - acc: 0.4380 - val\_loss: 1.2336 - val\_acc: 0.4371
Epoch 27/50
35064/35064 [==============================] - 23s 665us/step - loss: 1.2311 - acc: 0.4399 - val\_loss: 1.2347 - val\_acc: 0.4352
Epoch 28/50
35064/35064 [==============================] - 23s 666us/step - loss: 1.2263 - acc: 0.4428 - val\_loss: 1.2384 - val\_acc: 0.4337
Epoch 29/50
35064/35064 [==============================] - 24s 680us/step - loss: 1.2241 - acc: 0.4471 - val\_loss: 1.2325 - val\_acc: 0.4324
Epoch 30/50
35064/35064 [==============================] - 23s 669us/step - loss: 1.2207 - acc: 0.4481 - val\_loss: 1.2275 - val\_acc: 0.4418
Epoch 31/50
35064/35064 [==============================] - 24s 671us/step - loss: 1.2196 - acc: 0.4466 - val\_loss: 1.2276 - val\_acc: 0.4420
Epoch 32/50
35064/35064 [==============================] - 23s 669us/step - loss: 1.2156 - acc: 0.4487 - val\_loss: 1.2242 - val\_acc: 0.4428
Epoch 33/50
35064/35064 [==============================] - 23s 667us/step - loss: 1.2141 - acc: 0.4519 - val\_loss: 1.2248 - val\_acc: 0.4394
Epoch 34/50
35064/35064 [==============================] - 23s 667us/step - loss: 1.2134 - acc: 0.4490 - val\_loss: 1.2249 - val\_acc: 0.4423
Epoch 35/50
35064/35064 [==============================] - 23s 668us/step - loss: 1.2101 - acc: 0.4514 - val\_loss: 1.2208 - val\_acc: 0.4441
Epoch 36/50
35064/35064 [==============================] - 23s 667us/step - loss: 1.2073 - acc: 0.4546 - val\_loss: 1.2175 - val\_acc: 0.4487
Epoch 37/50
35064/35064 [==============================] - 23s 668us/step - loss: 1.2058 - acc: 0.4572 - val\_loss: 1.2158 - val\_acc: 0.4513
Epoch 38/50
35064/35064 [==============================] - 23s 667us/step - loss: 1.2034 - acc: 0.4590 - val\_loss: 1.2180 - val\_acc: 0.4507
Epoch 39/50
35064/35064 [==============================] - 26s 734us/step - loss: 1.2019 - acc: 0.4607 - val\_loss: 1.2179 - val\_acc: 0.4501
Epoch 40/50
35064/35064 [==============================] - 25s 720us/step - loss: 1.1994 - acc: 0.4630 - val\_loss: 1.2152 - val\_acc: 0.4455
Epoch 41/50
35064/35064 [==============================] - 25s 713us/step - loss: 1.1980 - acc: 0.4630 - val\_loss: 1.2214 - val\_acc: 0.4418
Epoch 42/50
35064/35064 [==============================] - 25s 699us/step - loss: 1.1952 - acc: 0.4663 - val\_loss: 1.2209 - val\_acc: 0.4576
Epoch 43/50
35064/35064 [==============================] - 26s 745us/step - loss: 1.1924 - acc: 0.4685 - val\_loss: 1.2125 - val\_acc: 0.4595
Epoch 44/50
35064/35064 [==============================] - 27s 780us/step - loss: 1.1910 - acc: 0.4684 - val\_loss: 1.2220 - val\_acc: 0.4562
Epoch 45/50
35064/35064 [==============================] - 25s 720us/step - loss: 1.1898 - acc: 0.4695 - val\_loss: 1.2114 - val\_acc: 0.4609
Epoch 46/50
35064/35064 [==============================] - 25s 716us/step - loss: 1.1878 - acc: 0.4708 - val\_loss: 1.2101 - val\_acc: 0.4638
Epoch 47/50
35064/35064 [==============================] - 24s 696us/step - loss: 1.1840 - acc: 0.4722 - val\_loss: 1.2078 - val\_acc: 0.4613
Epoch 48/50
35064/35064 [==============================] - 25s 701us/step - loss: 1.1833 - acc: 0.4711 - val\_loss: 1.2077 - val\_acc: 0.4605
Epoch 49/50
35064/35064 [==============================] - 25s 713us/step - loss: 1.1796 - acc: 0.4763 - val\_loss: 1.2119 - val\_acc: 0.4648
Epoch 50/50
35064/35064 [==============================] - 25s 707us/step - loss: 1.1788 - acc: 0.4775 - val\_loss: 1.2184 - val\_acc: 0.4476
8766/8766 [==============================] - 2s 234us/step
Test score:  1.2184318886917258
Test accuracy:  0.4476386011666749

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{n}{mpulse1}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv1d\_9 (Conv1D)            (None, 1476, 5)           255       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_9 (MaxPooling1 (None, 59, 5)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_10 (Conv1D)           (None, 50, 20)            1020      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_10 (MaxPooling (None, 5, 20)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_5 (Flatten)          (None, 100)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_13 (Dense)             (None, 500)               50500     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_14 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_15 (Dense)             (None, 5)                 2505      
=================================================================
Total params: 304,780
Trainable params: 304,780
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.36995208761122517
kappa:  0.002725712306952066

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{n}{macc1}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv1d\_11 (Conv1D)           (None, 1476, 5)           380       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_11 (MaxPooling (None, 59, 5)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_12 (Conv1D)           (None, 50, 20)            1020      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_12 (MaxPooling (None, 5, 20)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_6 (Flatten)          (None, 100)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_16 (Dense)             (None, 500)               50500     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_17 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_18 (Dense)             (None, 5)                 2505      
=================================================================
Total params: 304,905
Trainable params: 304,905
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.44763860369609854
kappa:  0.2028833163321101

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{n}{mpulse2} \PY{o}{=} \PY{n}{fitmodel}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                            \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{dropout} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{macc2} \PY{o}{=} \PY{n}{fitmodel}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                            \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{dropout} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", input\_shape=(1500, 2), filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  \# Remove the CWD from sys.path while we load stuff.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 23s 669us/step - loss: 4.5629 - acc: 0.3165 - val\_loss: 2.4095 - val\_acc: 0.3379
Epoch 2/200
35064/35064 [==============================] - 23s 645us/step - loss: 2.4463 - acc: 0.3327 - val\_loss: 2.0606 - val\_acc: 0.3496
Epoch 3/200
35064/35064 [==============================] - 23s 649us/step - loss: 2.1576 - acc: 0.3439 - val\_loss: 1.9297 - val\_acc: 0.3528
Epoch 4/200
35064/35064 [==============================] - 23s 650us/step - loss: 2.0283 - acc: 0.3515 - val\_loss: 1.8915 - val\_acc: 0.3668
Epoch 5/200
35064/35064 [==============================] - 23s 652us/step - loss: 1.9461 - acc: 0.3584 - val\_loss: 1.8586 - val\_acc: 0.3682
Epoch 6/200
35064/35064 [==============================] - 23s 661us/step - loss: 1.8949 - acc: 0.3603 - val\_loss: 1.8241 - val\_acc: 0.3736
Epoch 7/200
35064/35064 [==============================] - 24s 683us/step - loss: 1.8823 - acc: 0.3610 - val\_loss: 1.8242 - val\_acc: 0.3719
Epoch 8/200
35064/35064 [==============================] - 24s 681us/step - loss: 1.8870 - acc: 0.3583 - val\_loss: 1.8088 - val\_acc: 0.3670
Epoch 9/200
35064/35064 [==============================] - 23s 646us/step - loss: 1.8438 - acc: 0.3615 - val\_loss: 1.8564 - val\_acc: 0.3709
Epoch 10/200
35064/35064 [==============================] - 23s 643us/step - loss: 1.8240 - acc: 0.3640 - val\_loss: 1.8151 - val\_acc: 0.3769
Epoch 11/200
35064/35064 [==============================] - 23s 645us/step - loss: 1.8044 - acc: 0.3655 - val\_loss: 1.7937 - val\_acc: 0.3755
Epoch 12/200
35064/35064 [==============================] - 23s 649us/step - loss: 1.7695 - acc: 0.3679 - val\_loss: 1.7678 - val\_acc: 0.3770
Epoch 13/200
35064/35064 [==============================] - 23s 642us/step - loss: 1.7925 - acc: 0.3649 - val\_loss: 1.8018 - val\_acc: 0.3721
Epoch 14/200
35064/35064 [==============================] - 23s 645us/step - loss: 1.7720 - acc: 0.3667 - val\_loss: 1.7633 - val\_acc: 0.3745
Epoch 15/200
35064/35064 [==============================] - 21s 606us/step - loss: 1.7528 - acc: 0.3691 - val\_loss: 1.7750 - val\_acc: 0.3796
Epoch 16/200
35064/35064 [==============================] - 21s 592us/step - loss: 1.7592 - acc: 0.3672 - val\_loss: 1.7808 - val\_acc: 0.3759
Epoch 17/200
35064/35064 [==============================] - 23s 653us/step - loss: 1.7566 - acc: 0.3659 - val\_loss: 1.7585 - val\_acc: 0.3757
Epoch 18/200
35064/35064 [==============================] - 22s 636us/step - loss: 1.7431 - acc: 0.3658 - val\_loss: 1.7414 - val\_acc: 0.3800
Epoch 19/200
35064/35064 [==============================] - 21s 594us/step - loss: 1.7241 - acc: 0.3684 - val\_loss: 1.7413 - val\_acc: 0.3785
Epoch 20/200
35064/35064 [==============================] - 21s 595us/step - loss: 1.7432 - acc: 0.3683 - val\_loss: 1.7447 - val\_acc: 0.3751
Epoch 21/200
35064/35064 [==============================] - 21s 594us/step - loss: 1.7265 - acc: 0.3689 - val\_loss: 1.7350 - val\_acc: 0.3760
Epoch 22/200
35064/35064 [==============================] - 21s 595us/step - loss: 1.7332 - acc: 0.3690 - val\_loss: 1.7548 - val\_acc: 0.3765
Epoch 23/200
35064/35064 [==============================] - 21s 594us/step - loss: 1.7289 - acc: 0.3699 - val\_loss: 1.7475 - val\_acc: 0.3754
Epoch 24/200
35064/35064 [==============================] - 21s 595us/step - loss: 1.7260 - acc: 0.3702 - val\_loss: 1.7502 - val\_acc: 0.3747
Epoch 25/200
35064/35064 [==============================] - 21s 602us/step - loss: 1.7299 - acc: 0.3684 - val\_loss: 1.7379 - val\_acc: 0.3745
Epoch 26/200
35064/35064 [==============================] - 21s 598us/step - loss: 1.7133 - acc: 0.3693 - val\_loss: 1.7287 - val\_acc: 0.3746
Epoch 27/200
35064/35064 [==============================] - 21s 596us/step - loss: 1.7113 - acc: 0.3708 - val\_loss: 1.7291 - val\_acc: 0.3730
Epoch 28/200
35064/35064 [==============================] - 21s 597us/step - loss: 1.7145 - acc: 0.3705 - val\_loss: 1.7209 - val\_acc: 0.3741
Epoch 29/200
35064/35064 [==============================] - 21s 599us/step - loss: 1.7216 - acc: 0.3692 - val\_loss: 1.7371 - val\_acc: 0.3733
Epoch 30/200
35064/35064 [==============================] - 21s 598us/step - loss: 1.7177 - acc: 0.3696 - val\_loss: 1.7164 - val\_acc: 0.3737
Epoch 31/200
35064/35064 [==============================] - 21s 602us/step - loss: 1.7055 - acc: 0.3704 - val\_loss: 1.7151 - val\_acc: 0.3745
Epoch 32/200
35064/35064 [==============================] - 21s 609us/step - loss: 1.7019 - acc: 0.3704 - val\_loss: 1.7133 - val\_acc: 0.3768
Epoch 33/200
35064/35064 [==============================] - 21s 593us/step - loss: 1.7058 - acc: 0.3700 - val\_loss: 1.7054 - val\_acc: 0.3774
Epoch 34/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.7077 - acc: 0.3699 - val\_loss: 1.6966 - val\_acc: 0.3775
Epoch 35/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.7097 - acc: 0.3693 - val\_loss: 1.6945 - val\_acc: 0.3786
Epoch 36/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.7009 - acc: 0.3705 - val\_loss: 1.6998 - val\_acc: 0.3775
Epoch 37/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.7067 - acc: 0.3696 - val\_loss: 1.7016 - val\_acc: 0.3754
Epoch 38/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.7097 - acc: 0.3698 - val\_loss: 1.7050 - val\_acc: 0.3758
Epoch 39/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.7149 - acc: 0.3694 - val\_loss: 1.7045 - val\_acc: 0.3752
Epoch 40/200
35064/35064 [==============================] - 21s 592us/step - loss: 1.6993 - acc: 0.3708 - val\_loss: 1.7121 - val\_acc: 0.3750
Epoch 41/200
35064/35064 [==============================] - 21s 599us/step - loss: 1.7031 - acc: 0.3704 - val\_loss: 1.7110 - val\_acc: 0.3754
Epoch 42/200
35064/35064 [==============================] - 21s 596us/step - loss: 1.7007 - acc: 0.3704 - val\_loss: 1.7010 - val\_acc: 0.3763
Epoch 43/200
35064/35064 [==============================] - 21s 600us/step - loss: 1.7035 - acc: 0.3692 - val\_loss: 1.7031 - val\_acc: 0.3750
Epoch 44/200
35064/35064 [==============================] - 21s 604us/step - loss: 1.7013 - acc: 0.3698 - val\_loss: 1.7105 - val\_acc: 0.3737
Epoch 45/200
35064/35064 [==============================] - 21s 597us/step - loss: 1.6921 - acc: 0.3700 - val\_loss: 1.7027 - val\_acc: 0.3749
Epoch 46/200
35064/35064 [==============================] - 21s 592us/step - loss: 1.6835 - acc: 0.3704 - val\_loss: 1.6972 - val\_acc: 0.3758
Epoch 47/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.6923 - acc: 0.3712 - val\_loss: 1.6881 - val\_acc: 0.3755
Epoch 48/200
35064/35064 [==============================] - 21s 591us/step - loss: 1.6839 - acc: 0.3708 - val\_loss: 1.6949 - val\_acc: 0.3758
Epoch 49/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.6913 - acc: 0.3718 - val\_loss: 1.6974 - val\_acc: 0.3755
Epoch 50/200
35064/35064 [==============================] - 21s 592us/step - loss: 1.6973 - acc: 0.3702 - val\_loss: 1.6900 - val\_acc: 0.3766
Epoch 51/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.6824 - acc: 0.3706 - val\_loss: 1.6814 - val\_acc: 0.3760
Epoch 52/200
35064/35064 [==============================] - 22s 618us/step - loss: 1.6775 - acc: 0.3707 - val\_loss: 1.6945 - val\_acc: 0.3774
Epoch 53/200
35064/35064 [==============================] - 22s 627us/step - loss: 1.6808 - acc: 0.3721 - val\_loss: 1.6635 - val\_acc: 0.3769
Epoch 54/200
35064/35064 [==============================] - 21s 604us/step - loss: 1.6708 - acc: 0.3703 - val\_loss: 1.6825 - val\_acc: 0.3753
Epoch 55/200
35064/35064 [==============================] - 21s 591us/step - loss: 1.6656 - acc: 0.3709 - val\_loss: 1.6820 - val\_acc: 0.3751
Epoch 56/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.6825 - acc: 0.3690 - val\_loss: 1.6698 - val\_acc: 0.3768
Epoch 57/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.6773 - acc: 0.3713 - val\_loss: 1.6708 - val\_acc: 0.3773
Epoch 58/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.6591 - acc: 0.3716 - val\_loss: 1.6736 - val\_acc: 0.3762
Epoch 59/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.6559 - acc: 0.3710 - val\_loss: 1.6636 - val\_acc: 0.3767
Epoch 60/200
35064/35064 [==============================] - 21s 594us/step - loss: 1.6529 - acc: 0.3712 - val\_loss: 1.6673 - val\_acc: 0.3752
Epoch 61/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.6477 - acc: 0.3717 - val\_loss: 1.6624 - val\_acc: 0.3751
Epoch 62/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.6528 - acc: 0.3719 - val\_loss: 1.6714 - val\_acc: 0.3731
Epoch 63/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.6589 - acc: 0.3708 - val\_loss: 1.6661 - val\_acc: 0.3762
Epoch 64/200
35064/35064 [==============================] - 21s 597us/step - loss: 1.6512 - acc: 0.3718 - val\_loss: 1.6726 - val\_acc: 0.3754
Epoch 65/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.6511 - acc: 0.3718 - val\_loss: 1.6825 - val\_acc: 0.3765
Epoch 66/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.6488 - acc: 0.3724 - val\_loss: 1.6674 - val\_acc: 0.3761
Epoch 67/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.6411 - acc: 0.3727 - val\_loss: 1.6806 - val\_acc: 0.3755
Epoch 68/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.6372 - acc: 0.3736 - val\_loss: 1.6612 - val\_acc: 0.3765
Epoch 69/200
35064/35064 [==============================] - 20s 585us/step - loss: 1.6428 - acc: 0.3725 - val\_loss: 1.6694 - val\_acc: 0.3768
Epoch 70/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.6523 - acc: 0.3715 - val\_loss: 1.6542 - val\_acc: 0.3761
Epoch 71/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.6390 - acc: 0.3720 - val\_loss: 1.6639 - val\_acc: 0.3758
Epoch 72/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.6491 - acc: 0.3716 - val\_loss: 1.6704 - val\_acc: 0.3763
Epoch 73/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.6346 - acc: 0.3732 - val\_loss: 1.6603 - val\_acc: 0.3730
Epoch 74/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.6344 - acc: 0.3725 - val\_loss: 1.6495 - val\_acc: 0.3743
Epoch 75/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.6347 - acc: 0.3723 - val\_loss: 1.6459 - val\_acc: 0.3744
Epoch 76/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.6360 - acc: 0.3720 - val\_loss: 1.6558 - val\_acc: 0.3734
Epoch 77/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.6336 - acc: 0.3728 - val\_loss: 1.6493 - val\_acc: 0.3726
Epoch 78/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.6265 - acc: 0.3726 - val\_loss: 1.6557 - val\_acc: 0.3729
Epoch 79/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.6227 - acc: 0.3734 - val\_loss: 1.6523 - val\_acc: 0.3747
Epoch 80/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.6210 - acc: 0.3741 - val\_loss: 1.6553 - val\_acc: 0.3730
Epoch 81/200
35064/35064 [==============================] - 21s 599us/step - loss: 1.6254 - acc: 0.3739 - val\_loss: 1.6390 - val\_acc: 0.3744
Epoch 82/200
35064/35064 [==============================] - 21s 593us/step - loss: 1.6234 - acc: 0.3739 - val\_loss: 1.6456 - val\_acc: 0.3742
Epoch 83/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.6152 - acc: 0.3743 - val\_loss: 1.6407 - val\_acc: 0.3713
Epoch 84/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.6230 - acc: 0.3742 - val\_loss: 1.6490 - val\_acc: 0.3766
Epoch 85/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.6246 - acc: 0.3737 - val\_loss: 1.6585 - val\_acc: 0.3737
Epoch 86/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.6340 - acc: 0.3734 - val\_loss: 1.6476 - val\_acc: 0.3776
Epoch 87/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.6002 - acc: 0.3737 - val\_loss: 1.6338 - val\_acc: 0.3727
Epoch 88/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.6014 - acc: 0.3744 - val\_loss: 1.6333 - val\_acc: 0.3753
Epoch 89/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5972 - acc: 0.3745 - val\_loss: 1.6250 - val\_acc: 0.3766
Epoch 90/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5883 - acc: 0.3743 - val\_loss: 1.6293 - val\_acc: 0.3781
Epoch 91/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.5926 - acc: 0.3759 - val\_loss: 1.6348 - val\_acc: 0.3752
Epoch 92/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.5881 - acc: 0.3767 - val\_loss: 1.6344 - val\_acc: 0.3775
Epoch 93/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5809 - acc: 0.3760 - val\_loss: 1.6211 - val\_acc: 0.3767
Epoch 94/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.5860 - acc: 0.3756 - val\_loss: 1.6353 - val\_acc: 0.3770
Epoch 95/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.5875 - acc: 0.3750 - val\_loss: 1.6360 - val\_acc: 0.3754
Epoch 96/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5785 - acc: 0.3747 - val\_loss: 1.6306 - val\_acc: 0.3758
Epoch 97/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5811 - acc: 0.3749 - val\_loss: 1.6307 - val\_acc: 0.3712
Epoch 98/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.5838 - acc: 0.3747 - val\_loss: 1.6159 - val\_acc: 0.3736
Epoch 99/200
35064/35064 [==============================] - 22s 623us/step - loss: 1.5735 - acc: 0.3760 - val\_loss: 1.6214 - val\_acc: 0.3736
Epoch 100/200
35064/35064 [==============================] - 22s 619us/step - loss: 1.5816 - acc: 0.3740 - val\_loss: 1.6425 - val\_acc: 0.3761
Epoch 101/200
35064/35064 [==============================] - 21s 599us/step - loss: 1.5880 - acc: 0.3743 - val\_loss: 1.6404 - val\_acc: 0.3746
Epoch 102/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5798 - acc: 0.3737 - val\_loss: 1.6317 - val\_acc: 0.3758
Epoch 103/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5786 - acc: 0.3735 - val\_loss: 1.5726 - val\_acc: 0.3768
Epoch 104/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5548 - acc: 0.3746 - val\_loss: 1.5743 - val\_acc: 0.3766
Epoch 105/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.5498 - acc: 0.3753 - val\_loss: 1.5826 - val\_acc: 0.3752
Epoch 106/200
35064/35064 [==============================] - 21s 599us/step - loss: 1.5527 - acc: 0.3755 - val\_loss: 1.5856 - val\_acc: 0.3757
Epoch 107/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.5604 - acc: 0.3741 - val\_loss: 1.5861 - val\_acc: 0.3777
Epoch 108/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.5703 - acc: 0.3745 - val\_loss: 1.5826 - val\_acc: 0.3791
Epoch 109/200
35064/35064 [==============================] - 21s 591us/step - loss: 1.5542 - acc: 0.3748 - val\_loss: 1.5865 - val\_acc: 0.3781
Epoch 110/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5548 - acc: 0.3756 - val\_loss: 1.6032 - val\_acc: 0.3781
Epoch 111/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5641 - acc: 0.3746 - val\_loss: 1.5963 - val\_acc: 0.3770
Epoch 112/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.5576 - acc: 0.3752 - val\_loss: 1.6001 - val\_acc: 0.3773
Epoch 113/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5589 - acc: 0.3753 - val\_loss: 1.6133 - val\_acc: 0.3769
Epoch 114/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5700 - acc: 0.3747 - val\_loss: 1.6111 - val\_acc: 0.3714
Epoch 115/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5699 - acc: 0.3751 - val\_loss: 1.6254 - val\_acc: 0.3730
Epoch 116/200
35064/35064 [==============================] - 20s 585us/step - loss: 1.5679 - acc: 0.3749 - val\_loss: 1.6207 - val\_acc: 0.3760
Epoch 117/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5693 - acc: 0.3747 - val\_loss: 1.6144 - val\_acc: 0.3755
Epoch 118/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5627 - acc: 0.3760 - val\_loss: 1.6177 - val\_acc: 0.3761
Epoch 119/200
35064/35064 [==============================] - 21s 591us/step - loss: 1.5613 - acc: 0.3749 - val\_loss: 1.6203 - val\_acc: 0.3762
Epoch 120/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.5640 - acc: 0.3754 - val\_loss: 1.6169 - val\_acc: 0.3757
Epoch 121/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.5641 - acc: 0.3758 - val\_loss: 1.6135 - val\_acc: 0.3769
Epoch 122/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.5583 - acc: 0.3752 - val\_loss: 1.5932 - val\_acc: 0.3762
Epoch 123/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5548 - acc: 0.3742 - val\_loss: 1.6080 - val\_acc: 0.3749
Epoch 124/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.5532 - acc: 0.3748 - val\_loss: 1.6122 - val\_acc: 0.3758
Epoch 125/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5508 - acc: 0.3755 - val\_loss: 1.5986 - val\_acc: 0.3754
Epoch 126/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5450 - acc: 0.3762 - val\_loss: 1.6022 - val\_acc: 0.3763
Epoch 127/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.5452 - acc: 0.3765 - val\_loss: 1.5970 - val\_acc: 0.3771
Epoch 128/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5426 - acc: 0.3769 - val\_loss: 1.6121 - val\_acc: 0.3689
Epoch 129/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5512 - acc: 0.3769 - val\_loss: 1.6232 - val\_acc: 0.3719
Epoch 130/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5542 - acc: 0.3761 - val\_loss: 1.6177 - val\_acc: 0.3744
Epoch 131/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5541 - acc: 0.3773 - val\_loss: 1.6216 - val\_acc: 0.3712
Epoch 132/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5561 - acc: 0.3779 - val\_loss: 1.6186 - val\_acc: 0.3733
Epoch 133/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5545 - acc: 0.3774 - val\_loss: 1.6258 - val\_acc: 0.3702
Epoch 134/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5568 - acc: 0.3790 - val\_loss: 1.6220 - val\_acc: 0.3767
Epoch 135/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5652 - acc: 0.3779 - val\_loss: 1.6266 - val\_acc: 0.3769
Epoch 136/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.5596 - acc: 0.3786 - val\_loss: 1.6270 - val\_acc: 0.3701
Epoch 137/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5488 - acc: 0.3780 - val\_loss: 1.6214 - val\_acc: 0.3762
Epoch 138/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5648 - acc: 0.3764 - val\_loss: 1.6269 - val\_acc: 0.3725
Epoch 139/200
35064/35064 [==============================] - 21s 592us/step - loss: 1.5630 - acc: 0.3777 - val\_loss: 1.6244 - val\_acc: 0.3715
Epoch 140/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5618 - acc: 0.3769 - val\_loss: 1.6200 - val\_acc: 0.3727
Epoch 141/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5580 - acc: 0.3767 - val\_loss: 1.6120 - val\_acc: 0.3751
Epoch 142/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.5472 - acc: 0.3782 - val\_loss: 1.6118 - val\_acc: 0.3778
Epoch 143/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5481 - acc: 0.3790 - val\_loss: 1.6107 - val\_acc: 0.3759
Epoch 144/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5478 - acc: 0.3786 - val\_loss: 1.6064 - val\_acc: 0.3778
Epoch 145/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.5497 - acc: 0.3778 - val\_loss: 1.6181 - val\_acc: 0.3731
Epoch 146/200
35064/35064 [==============================] - 22s 620us/step - loss: 1.5536 - acc: 0.3774 - val\_loss: 1.6193 - val\_acc: 0.3743
Epoch 147/200
35064/35064 [==============================] - 22s 627us/step - loss: 1.5510 - acc: 0.3780 - val\_loss: 1.6271 - val\_acc: 0.3678
Epoch 148/200
35064/35064 [==============================] - 21s 597us/step - loss: 1.5518 - acc: 0.3780 - val\_loss: 1.6136 - val\_acc: 0.3776
Epoch 149/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5496 - acc: 0.3788 - val\_loss: 1.6193 - val\_acc: 0.3734
Epoch 150/200
35064/35064 [==============================] - 21s 591us/step - loss: 1.5619 - acc: 0.3772 - val\_loss: 1.6397 - val\_acc: 0.3703
Epoch 151/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5471 - acc: 0.3806 - val\_loss: 1.6239 - val\_acc: 0.3763
Epoch 152/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5417 - acc: 0.3781 - val\_loss: 1.6238 - val\_acc: 0.3757
Epoch 153/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5406 - acc: 0.3803 - val\_loss: 1.6533 - val\_acc: 0.3712
Epoch 154/200
35064/35064 [==============================] - 20s 579us/step - loss: 1.5437 - acc: 0.3795 - val\_loss: 1.6129 - val\_acc: 0.3773
Epoch 155/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5469 - acc: 0.3787 - val\_loss: 1.6167 - val\_acc: 0.3719
Epoch 156/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5544 - acc: 0.3805 - val\_loss: 1.6180 - val\_acc: 0.3690
Epoch 157/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5515 - acc: 0.3788 - val\_loss: 1.6358 - val\_acc: 0.3738
Epoch 158/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5690 - acc: 0.3745 - val\_loss: 1.6391 - val\_acc: 0.3744
Epoch 159/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.5678 - acc: 0.3747 - val\_loss: 1.6248 - val\_acc: 0.3769
Epoch 160/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5598 - acc: 0.3759 - val\_loss: 1.6355 - val\_acc: 0.3718
Epoch 161/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5461 - acc: 0.3793 - val\_loss: 1.6323 - val\_acc: 0.3671
Epoch 162/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5417 - acc: 0.3807 - val\_loss: 1.6569 - val\_acc: 0.3706
Epoch 163/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5531 - acc: 0.3796 - val\_loss: 1.6144 - val\_acc: 0.3695
Epoch 164/200
35064/35064 [==============================] - 21s 585us/step - loss: 1.5455 - acc: 0.3810 - val\_loss: 1.6330 - val\_acc: 0.3664
Epoch 165/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.5503 - acc: 0.3806 - val\_loss: 1.6260 - val\_acc: 0.3730
Epoch 166/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5438 - acc: 0.3797 - val\_loss: 1.6218 - val\_acc: 0.3720
Epoch 167/200
35064/35064 [==============================] - 20s 579us/step - loss: 1.5455 - acc: 0.3800 - val\_loss: 1.6251 - val\_acc: 0.3698
Epoch 168/200
35064/35064 [==============================] - 20s 584us/step - loss: 1.5425 - acc: 0.3805 - val\_loss: 1.6410 - val\_acc: 0.3710
Epoch 169/200
35064/35064 [==============================] - 21s 596us/step - loss: 1.5541 - acc: 0.3794 - val\_loss: 1.6199 - val\_acc: 0.3698
Epoch 170/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5448 - acc: 0.3799 - val\_loss: 1.6282 - val\_acc: 0.3710
Epoch 171/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5439 - acc: 0.3806 - val\_loss: 1.6330 - val\_acc: 0.3719
Epoch 172/200
35064/35064 [==============================] - 20s 579us/step - loss: 1.5451 - acc: 0.3808 - val\_loss: 1.6400 - val\_acc: 0.3721
Epoch 173/200
35064/35064 [==============================] - 20s 579us/step - loss: 1.5415 - acc: 0.3817 - val\_loss: 1.6337 - val\_acc: 0.3706
Epoch 174/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5401 - acc: 0.3829 - val\_loss: 1.6382 - val\_acc: 0.3754
Epoch 175/200
35064/35064 [==============================] - 20s 577us/step - loss: 1.5367 - acc: 0.3830 - val\_loss: 1.6285 - val\_acc: 0.3698
Epoch 176/200
35064/35064 [==============================] - 20s 577us/step - loss: 1.5361 - acc: 0.3834 - val\_loss: 1.6218 - val\_acc: 0.3733
Epoch 177/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5413 - acc: 0.3822 - val\_loss: 1.6286 - val\_acc: 0.3705
Epoch 178/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5401 - acc: 0.3820 - val\_loss: 1.6279 - val\_acc: 0.3731
Epoch 179/200
35064/35064 [==============================] - 20s 578us/step - loss: 1.5374 - acc: 0.3820 - val\_loss: 1.6150 - val\_acc: 0.3750
Epoch 180/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5319 - acc: 0.3834 - val\_loss: 1.6106 - val\_acc: 0.3753
Epoch 181/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5287 - acc: 0.3841 - val\_loss: 1.6295 - val\_acc: 0.3738
Epoch 182/200
35064/35064 [==============================] - 20s 579us/step - loss: 1.5335 - acc: 0.3844 - val\_loss: 1.6353 - val\_acc: 0.3709
Epoch 183/200
35064/35064 [==============================] - 21s 594us/step - loss: 1.5358 - acc: 0.3826 - val\_loss: 1.6320 - val\_acc: 0.3687
Epoch 184/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5364 - acc: 0.3839 - val\_loss: 1.6185 - val\_acc: 0.3715
Epoch 185/200
35064/35064 [==============================] - 20s 578us/step - loss: 1.5287 - acc: 0.3836 - val\_loss: 1.6197 - val\_acc: 0.3730
Epoch 186/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5450 - acc: 0.3829 - val\_loss: 1.6664 - val\_acc: 0.3625
Epoch 187/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5381 - acc: 0.3827 - val\_loss: 1.6039 - val\_acc: 0.3775
Epoch 188/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5245 - acc: 0.3846 - val\_loss: 1.6046 - val\_acc: 0.3733
Epoch 189/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5299 - acc: 0.3837 - val\_loss: 1.5961 - val\_acc: 0.3696
Epoch 190/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5218 - acc: 0.3846 - val\_loss: 1.5997 - val\_acc: 0.3698
Epoch 191/200
35064/35064 [==============================] - 20s 582us/step - loss: 1.5282 - acc: 0.3840 - val\_loss: 1.6184 - val\_acc: 0.3697
Epoch 192/200
35064/35064 [==============================] - 20s 585us/step - loss: 1.5265 - acc: 0.3856 - val\_loss: 1.6401 - val\_acc: 0.3676
Epoch 193/200
35064/35064 [==============================] - 21s 611us/step - loss: 1.5249 - acc: 0.3857 - val\_loss: 1.6256 - val\_acc: 0.3712
Epoch 194/200
35064/35064 [==============================] - 22s 621us/step - loss: 1.5283 - acc: 0.3840 - val\_loss: 1.6405 - val\_acc: 0.3729
Epoch 195/200
35064/35064 [==============================] - 21s 602us/step - loss: 1.5276 - acc: 0.3849 - val\_loss: 1.6279 - val\_acc: 0.3717
Epoch 196/200
35064/35064 [==============================] - 20s 580us/step - loss: 1.5364 - acc: 0.3850 - val\_loss: 1.6445 - val\_acc: 0.3678
Epoch 197/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5328 - acc: 0.3864 - val\_loss: 1.6403 - val\_acc: 0.3676
Epoch 198/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.5267 - acc: 0.3854 - val\_loss: 1.6285 - val\_acc: 0.3725
Epoch 199/200
35064/35064 [==============================] - 20s 581us/step - loss: 1.5203 - acc: 0.3869 - val\_loss: 1.6218 - val\_acc: 0.3686
Epoch 200/200
35064/35064 [==============================] - 20s 583us/step - loss: 1.5180 - acc: 0.3843 - val\_loss: 1.6271 - val\_acc: 0.3690
8766/8766 [==============================] - 2s 212us/step
Test score:  1.6270745850958401
Test accuracy:  0.3690394709269641

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", input\_shape=(1500, 3), filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  \# Remove the CWD from sys.path while we load stuff.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 25s 700us/step - loss: 1.4137 - acc: 0.3686 - val\_loss: 1.3607 - val\_acc: 0.3876
Epoch 2/200
35064/35064 [==============================] - 24s 676us/step - loss: 1.3649 - acc: 0.3773 - val\_loss: 1.3570 - val\_acc: 0.3887
Epoch 3/200
35064/35064 [==============================] - 24s 679us/step - loss: 1.3612 - acc: 0.3786 - val\_loss: 1.3537 - val\_acc: 0.3900
Epoch 4/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.3580 - acc: 0.3802 - val\_loss: 1.3514 - val\_acc: 0.3917
Epoch 5/200
35064/35064 [==============================] - 24s 675us/step - loss: 1.3542 - acc: 0.3800 - val\_loss: 1.3465 - val\_acc: 0.3919
Epoch 6/200
35064/35064 [==============================] - 24s 687us/step - loss: 1.3470 - acc: 0.3826 - val\_loss: 1.3362 - val\_acc: 0.3943
Epoch 7/200
35064/35064 [==============================] - 24s 683us/step - loss: 1.3295 - acc: 0.3964 - val\_loss: 1.3119 - val\_acc: 0.4086
Epoch 8/200
35064/35064 [==============================] - 24s 685us/step - loss: 1.3073 - acc: 0.4032 - val\_loss: 1.2974 - val\_acc: 0.4028
Epoch 9/200
35064/35064 [==============================] - 24s 679us/step - loss: 1.2959 - acc: 0.4042 - val\_loss: 1.2884 - val\_acc: 0.4118
Epoch 10/200
35064/35064 [==============================] - 24s 676us/step - loss: 1.2910 - acc: 0.4057 - val\_loss: 1.2828 - val\_acc: 0.4075
Epoch 11/200
35064/35064 [==============================] - 24s 678us/step - loss: 1.2855 - acc: 0.4066 - val\_loss: 1.2865 - val\_acc: 0.4144
Epoch 12/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.2815 - acc: 0.4100 - val\_loss: 1.2783 - val\_acc: 0.4136
Epoch 13/200
35064/35064 [==============================] - 24s 674us/step - loss: 1.2784 - acc: 0.4102 - val\_loss: 1.2863 - val\_acc: 0.4136
Epoch 14/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.2761 - acc: 0.4117 - val\_loss: 1.2813 - val\_acc: 0.4076
Epoch 15/200
35064/35064 [==============================] - 24s 678us/step - loss: 1.2741 - acc: 0.4142 - val\_loss: 1.2749 - val\_acc: 0.4094
Epoch 16/200
35064/35064 [==============================] - 24s 675us/step - loss: 1.2713 - acc: 0.4134 - val\_loss: 1.2698 - val\_acc: 0.4154
Epoch 17/200
35064/35064 [==============================] - 24s 676us/step - loss: 1.2699 - acc: 0.4157 - val\_loss: 1.2693 - val\_acc: 0.4155
Epoch 18/200
35064/35064 [==============================] - 24s 675us/step - loss: 1.2691 - acc: 0.4181 - val\_loss: 1.2674 - val\_acc: 0.4185
Epoch 19/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.2653 - acc: 0.4187 - val\_loss: 1.2676 - val\_acc: 0.4199
Epoch 20/200
35064/35064 [==============================] - 24s 680us/step - loss: 1.2636 - acc: 0.4167 - val\_loss: 1.2714 - val\_acc: 0.4128
Epoch 21/200
35064/35064 [==============================] - 24s 678us/step - loss: 1.2622 - acc: 0.4191 - val\_loss: 1.2633 - val\_acc: 0.4238
Epoch 22/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.2609 - acc: 0.4213 - val\_loss: 1.2628 - val\_acc: 0.4232
Epoch 23/200
35064/35064 [==============================] - 24s 674us/step - loss: 1.2579 - acc: 0.4239 - val\_loss: 1.2600 - val\_acc: 0.4272
Epoch 24/200
35064/35064 [==============================] - 24s 676us/step - loss: 1.2555 - acc: 0.4261 - val\_loss: 1.2592 - val\_acc: 0.4281
Epoch 25/200
35064/35064 [==============================] - 24s 675us/step - loss: 1.2528 - acc: 0.4286 - val\_loss: 1.2565 - val\_acc: 0.4322
Epoch 26/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.2499 - acc: 0.4323 - val\_loss: 1.2563 - val\_acc: 0.4302
Epoch 27/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.2492 - acc: 0.4325 - val\_loss: 1.2571 - val\_acc: 0.4264
Epoch 28/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.2483 - acc: 0.4327 - val\_loss: 1.2529 - val\_acc: 0.4370
Epoch 29/200
35064/35064 [==============================] - 24s 683us/step - loss: 1.2450 - acc: 0.4330 - val\_loss: 1.2537 - val\_acc: 0.4335
Epoch 30/200
35064/35064 [==============================] - 24s 678us/step - loss: 1.2438 - acc: 0.4339 - val\_loss: 1.2495 - val\_acc: 0.4383
Epoch 31/200
35064/35064 [==============================] - 24s 676us/step - loss: 1.2418 - acc: 0.4353 - val\_loss: 1.2518 - val\_acc: 0.4350
Epoch 32/200
35064/35064 [==============================] - 24s 679us/step - loss: 1.2393 - acc: 0.4384 - val\_loss: 1.2464 - val\_acc: 0.4362
Epoch 33/200
35064/35064 [==============================] - 24s 675us/step - loss: 1.2392 - acc: 0.4368 - val\_loss: 1.2462 - val\_acc: 0.4338
Epoch 34/200
35064/35064 [==============================] - 24s 688us/step - loss: 1.2350 - acc: 0.4409 - val\_loss: 1.2456 - val\_acc: 0.4337
Epoch 35/200
35064/35064 [==============================] - 27s 781us/step - loss: 1.2326 - acc: 0.4428 - val\_loss: 1.2445 - val\_acc: 0.4362
Epoch 36/200
35064/35064 [==============================] - 26s 743us/step - loss: 1.2309 - acc: 0.4438 - val\_loss: 1.2412 - val\_acc: 0.4464
Epoch 37/200
35064/35064 [==============================] - 25s 717us/step - loss: 1.2292 - acc: 0.4437 - val\_loss: 1.2412 - val\_acc: 0.4374
Epoch 38/200
35064/35064 [==============================] - 25s 706us/step - loss: 1.2273 - acc: 0.4433 - val\_loss: 1.2501 - val\_acc: 0.4248
Epoch 39/200
35064/35064 [==============================] - 25s 706us/step - loss: 1.2241 - acc: 0.4473 - val\_loss: 1.2417 - val\_acc: 0.4369
Epoch 40/200
35064/35064 [==============================] - 24s 677us/step - loss: 1.2240 - acc: 0.4469 - val\_loss: 1.2420 - val\_acc: 0.4467
Epoch 41/200
35064/35064 [==============================] - 24s 692us/step - loss: 1.2194 - acc: 0.4505 - val\_loss: 1.2370 - val\_acc: 0.4486
Epoch 42/200
35064/35064 [==============================] - 25s 700us/step - loss: 1.2182 - acc: 0.4498 - val\_loss: 1.2377 - val\_acc: 0.4385
Epoch 43/200
35064/35064 [==============================] - 25s 699us/step - loss: 1.2170 - acc: 0.4499 - val\_loss: 1.2404 - val\_acc: 0.4490
Epoch 44/200
35064/35064 [==============================] - 24s 697us/step - loss: 1.2143 - acc: 0.4556 - val\_loss: 1.2350 - val\_acc: 0.4504
Epoch 45/200
35064/35064 [==============================] - 24s 685us/step - loss: 1.2131 - acc: 0.4550 - val\_loss: 1.2350 - val\_acc: 0.4555
Epoch 46/200
35064/35064 [==============================] - 24s 688us/step - loss: 1.2123 - acc: 0.4533 - val\_loss: 1.2353 - val\_acc: 0.4508
Epoch 47/200
35064/35064 [==============================] - 24s 693us/step - loss: 1.2083 - acc: 0.4579 - val\_loss: 1.2425 - val\_acc: 0.4317
Epoch 48/200
35064/35064 [==============================] - 24s 697us/step - loss: 1.2057 - acc: 0.4604 - val\_loss: 1.2335 - val\_acc: 0.4512
Epoch 49/200
35064/35064 [==============================] - 24s 676us/step - loss: 1.2049 - acc: 0.4584 - val\_loss: 1.2298 - val\_acc: 0.4509
Epoch 50/200
35064/35064 [==============================] - 24s 676us/step - loss: 1.2011 - acc: 0.4659 - val\_loss: 1.2416 - val\_acc: 0.4354
Epoch 51/200
35064/35064 [==============================] - 24s 683us/step - loss: 1.2005 - acc: 0.4624 - val\_loss: 1.2256 - val\_acc: 0.4573
Epoch 52/200
35064/35064 [==============================] - 24s 674us/step - loss: 1.1993 - acc: 0.4630 - val\_loss: 1.2240 - val\_acc: 0.4592
Epoch 53/200
35064/35064 [==============================] - 24s 674us/step - loss: 1.1949 - acc: 0.4680 - val\_loss: 1.2302 - val\_acc: 0.4503
Epoch 54/200
35064/35064 [==============================] - 24s 675us/step - loss: 1.1946 - acc: 0.4668 - val\_loss: 1.2274 - val\_acc: 0.4495
Epoch 55/200
35064/35064 [==============================] - 25s 702us/step - loss: 1.1948 - acc: 0.4671 - val\_loss: 1.2280 - val\_acc: 0.4543
Epoch 56/200
35064/35064 [==============================] - 25s 707us/step - loss: 1.1930 - acc: 0.4672 - val\_loss: 1.2283 - val\_acc: 0.4581
Epoch 57/200
35064/35064 [==============================] - 25s 701us/step - loss: 1.1877 - acc: 0.4715 - val\_loss: 1.2304 - val\_acc: 0.4594
Epoch 58/200
35064/35064 [==============================] - 24s 690us/step - loss: 1.1880 - acc: 0.4705 - val\_loss: 1.2224 - val\_acc: 0.4633
Epoch 59/200
35064/35064 [==============================] - 24s 687us/step - loss: 1.1867 - acc: 0.4713 - val\_loss: 1.2237 - val\_acc: 0.4606
Epoch 60/200
35064/35064 [==============================] - 24s 686us/step - loss: 1.1834 - acc: 0.4754 - val\_loss: 1.2344 - val\_acc: 0.4457
Epoch 61/200
35064/35064 [==============================] - 24s 683us/step - loss: 1.1807 - acc: 0.4751 - val\_loss: 1.2226 - val\_acc: 0.4636
Epoch 62/200
35064/35064 [==============================] - 24s 683us/step - loss: 1.1798 - acc: 0.4780 - val\_loss: 1.2243 - val\_acc: 0.4650
Epoch 63/200
35064/35064 [==============================] - 24s 682us/step - loss: 1.1791 - acc: 0.4759 - val\_loss: 1.2285 - val\_acc: 0.4532
Epoch 64/200
35064/35064 [==============================] - 24s 683us/step - loss: 1.1789 - acc: 0.4766 - val\_loss: 1.2285 - val\_acc: 0.4637
Epoch 65/200
35064/35064 [==============================] - 24s 684us/step - loss: 1.1771 - acc: 0.4780 - val\_loss: 1.2226 - val\_acc: 0.4635
Epoch 66/200
35064/35064 [==============================] - 24s 683us/step - loss: 1.1738 - acc: 0.4798 - val\_loss: 1.2224 - val\_acc: 0.4662
Epoch 67/200
35064/35064 [==============================] - 24s 686us/step - loss: 1.1717 - acc: 0.4798 - val\_loss: 1.2262 - val\_acc: 0.4633
Epoch 68/200
35064/35064 [==============================] - 24s 686us/step - loss: 1.1709 - acc: 0.4797 - val\_loss: 1.2214 - val\_acc: 0.4682
Epoch 69/200
35064/35064 [==============================] - 24s 694us/step - loss: 1.1686 - acc: 0.4839 - val\_loss: 1.2295 - val\_acc: 0.4564
Epoch 70/200
35064/35064 [==============================] - 24s 695us/step - loss: 1.1685 - acc: 0.4830 - val\_loss: 1.2197 - val\_acc: 0.4685
Epoch 71/200
35064/35064 [==============================] - 24s 687us/step - loss: 1.1677 - acc: 0.4828 - val\_loss: 1.2396 - val\_acc: 0.4459
Epoch 72/200
35064/35064 [==============================] - 25s 700us/step - loss: 1.1644 - acc: 0.4849 - val\_loss: 1.2217 - val\_acc: 0.4684
Epoch 73/200
35064/35064 [==============================] - 24s 697us/step - loss: 1.1621 - acc: 0.4855 - val\_loss: 1.2188 - val\_acc: 0.4731
Epoch 74/200
35064/35064 [==============================] - 25s 709us/step - loss: 1.1600 - acc: 0.4876 - val\_loss: 1.2369 - val\_acc: 0.4504
Epoch 75/200
35064/35064 [==============================] - 27s 761us/step - loss: 1.1606 - acc: 0.4887 - val\_loss: 1.2218 - val\_acc: 0.4619
Epoch 76/200
35064/35064 [==============================] - 27s 779us/step - loss: 1.1575 - acc: 0.4904 - val\_loss: 1.2316 - val\_acc: 0.4595
Epoch 77/200
35064/35064 [==============================] - 26s 750us/step - loss: 1.1575 - acc: 0.4910 - val\_loss: 1.2243 - val\_acc: 0.4657
Epoch 78/200
35064/35064 [==============================] - 27s 759us/step - loss: 1.1557 - acc: 0.4877 - val\_loss: 1.2296 - val\_acc: 0.4633
Epoch 79/200
35064/35064 [==============================] - 27s 764us/step - loss: 1.1539 - acc: 0.4925 - val\_loss: 1.2214 - val\_acc: 0.4678
Epoch 80/200
35064/35064 [==============================] - 27s 766us/step - loss: 1.1542 - acc: 0.4876 - val\_loss: 1.2275 - val\_acc: 0.4650
Epoch 81/200
35064/35064 [==============================] - 27s 766us/step - loss: 1.1510 - acc: 0.4911 - val\_loss: 1.2263 - val\_acc: 0.4706
Epoch 82/200
35064/35064 [==============================] - 27s 762us/step - loss: 1.1522 - acc: 0.4913 - val\_loss: 1.2262 - val\_acc: 0.4713
Epoch 83/200
35064/35064 [==============================] - 27s 768us/step - loss: 1.1470 - acc: 0.4945 - val\_loss: 1.2231 - val\_acc: 0.4726
Epoch 84/200
35064/35064 [==============================] - 27s 764us/step - loss: 1.1476 - acc: 0.4938 - val\_loss: 1.2260 - val\_acc: 0.4710
Epoch 85/200
35064/35064 [==============================] - 27s 758us/step - loss: 1.1448 - acc: 0.4959 - val\_loss: 1.2303 - val\_acc: 0.4669
Epoch 86/200
35064/35064 [==============================] - 26s 752us/step - loss: 1.1443 - acc: 0.4990 - val\_loss: 1.2236 - val\_acc: 0.4723
Epoch 87/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.1418 - acc: 0.4990 - val\_loss: 1.2252 - val\_acc: 0.4681
Epoch 88/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.1439 - acc: 0.4963 - val\_loss: 1.2265 - val\_acc: 0.4743
Epoch 89/200
35064/35064 [==============================] - 27s 759us/step - loss: 1.1414 - acc: 0.4965 - val\_loss: 1.2297 - val\_acc: 0.4714
Epoch 90/200
35064/35064 [==============================] - 27s 760us/step - loss: 1.1387 - acc: 0.4990 - val\_loss: 1.2295 - val\_acc: 0.4723
Epoch 91/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.1383 - acc: 0.5008 - val\_loss: 1.2242 - val\_acc: 0.4724
Epoch 92/200
35064/35064 [==============================] - 27s 757us/step - loss: 1.1367 - acc: 0.4978 - val\_loss: 1.2257 - val\_acc: 0.4727
Epoch 93/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.1372 - acc: 0.5007 - val\_loss: 1.2291 - val\_acc: 0.4766
Epoch 94/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.1344 - acc: 0.5013 - val\_loss: 1.2489 - val\_acc: 0.4633
Epoch 95/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.1331 - acc: 0.5034 - val\_loss: 1.2325 - val\_acc: 0.4719
Epoch 96/200
35064/35064 [==============================] - 27s 756us/step - loss: 1.1313 - acc: 0.5043 - val\_loss: 1.2336 - val\_acc: 0.4660
Epoch 97/200
35064/35064 [==============================] - 26s 752us/step - loss: 1.1304 - acc: 0.5032 - val\_loss: 1.2340 - val\_acc: 0.4743
Epoch 98/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.1276 - acc: 0.5050 - val\_loss: 1.2493 - val\_acc: 0.4652
Epoch 99/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.1277 - acc: 0.5052 - val\_loss: 1.2334 - val\_acc: 0.4771
Epoch 100/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.1272 - acc: 0.5038 - val\_loss: 1.2257 - val\_acc: 0.4808
Epoch 101/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.1242 - acc: 0.5061 - val\_loss: 1.2289 - val\_acc: 0.4756
Epoch 102/200
35064/35064 [==============================] - 27s 762us/step - loss: 1.1208 - acc: 0.5079 - val\_loss: 1.2332 - val\_acc: 0.4776
Epoch 103/200
35064/35064 [==============================] - 26s 754us/step - loss: 1.1233 - acc: 0.5059 - val\_loss: 1.2365 - val\_acc: 0.4757
Epoch 104/200
35064/35064 [==============================] - 27s 760us/step - loss: 1.1203 - acc: 0.5118 - val\_loss: 1.2308 - val\_acc: 0.4794
Epoch 105/200
35064/35064 [==============================] - 26s 751us/step - loss: 1.1193 - acc: 0.5100 - val\_loss: 1.2308 - val\_acc: 0.4783
Epoch 106/200
35064/35064 [==============================] - 26s 754us/step - loss: 1.1192 - acc: 0.5104 - val\_loss: 1.2367 - val\_acc: 0.4795
Epoch 107/200
35064/35064 [==============================] - 27s 757us/step - loss: 1.1181 - acc: 0.5102 - val\_loss: 1.2301 - val\_acc: 0.4820
Epoch 108/200
35064/35064 [==============================] - 26s 752us/step - loss: 1.1157 - acc: 0.5117 - val\_loss: 1.2349 - val\_acc: 0.4755
Epoch 109/200
35064/35064 [==============================] - 27s 757us/step - loss: 1.1171 - acc: 0.5114 - val\_loss: 1.2351 - val\_acc: 0.4804
Epoch 110/200
35064/35064 [==============================] - 27s 756us/step - loss: 1.1117 - acc: 0.5140 - val\_loss: 1.2329 - val\_acc: 0.4795
Epoch 111/200
35064/35064 [==============================] - 28s 798us/step - loss: 1.1114 - acc: 0.5144 - val\_loss: 1.2377 - val\_acc: 0.4790
Epoch 112/200
35064/35064 [==============================] - 27s 783us/step - loss: 1.1110 - acc: 0.5175 - val\_loss: 1.2403 - val\_acc: 0.4788
Epoch 113/200
35064/35064 [==============================] - 27s 760us/step - loss: 1.1110 - acc: 0.5139 - val\_loss: 1.2367 - val\_acc: 0.4811
Epoch 114/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.1063 - acc: 0.5195 - val\_loss: 1.2500 - val\_acc: 0.4730
Epoch 115/200
35064/35064 [==============================] - 27s 764us/step - loss: 1.1077 - acc: 0.5164 - val\_loss: 1.2370 - val\_acc: 0.4782
Epoch 116/200
35064/35064 [==============================] - 27s 773us/step - loss: 1.1064 - acc: 0.5187 - val\_loss: 1.2458 - val\_acc: 0.4772
Epoch 117/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.1046 - acc: 0.5144 - val\_loss: 1.2510 - val\_acc: 0.4784
Epoch 118/200
35064/35064 [==============================] - 27s 767us/step - loss: 1.1057 - acc: 0.5131 - val\_loss: 1.2385 - val\_acc: 0.4827
Epoch 119/200
35064/35064 [==============================] - 26s 754us/step - loss: 1.1034 - acc: 0.5177 - val\_loss: 1.2401 - val\_acc: 0.4839
Epoch 120/200
35064/35064 [==============================] - 26s 751us/step - loss: 1.0996 - acc: 0.5217 - val\_loss: 1.2481 - val\_acc: 0.4820
Epoch 121/200
35064/35064 [==============================] - 26s 749us/step - loss: 1.0996 - acc: 0.5192 - val\_loss: 1.2455 - val\_acc: 0.4820
Epoch 122/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.0956 - acc: 0.5259 - val\_loss: 1.2472 - val\_acc: 0.4815
Epoch 123/200
35064/35064 [==============================] - 26s 748us/step - loss: 1.0953 - acc: 0.5214 - val\_loss: 1.2528 - val\_acc: 0.4825
Epoch 124/200
35064/35064 [==============================] - 26s 749us/step - loss: 1.0972 - acc: 0.5246 - val\_loss: 1.2443 - val\_acc: 0.4864
Epoch 125/200
35064/35064 [==============================] - 26s 749us/step - loss: 1.0943 - acc: 0.5254 - val\_loss: 1.2518 - val\_acc: 0.4796
Epoch 126/200
35064/35064 [==============================] - 26s 752us/step - loss: 1.0926 - acc: 0.5256 - val\_loss: 1.2478 - val\_acc: 0.4851
Epoch 127/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0928 - acc: 0.5231 - val\_loss: 1.2509 - val\_acc: 0.4835
Epoch 128/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0917 - acc: 0.5226 - val\_loss: 1.2487 - val\_acc: 0.4836
Epoch 129/200
35064/35064 [==============================] - 26s 751us/step - loss: 1.0907 - acc: 0.5270 - val\_loss: 1.2551 - val\_acc: 0.4824
Epoch 130/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0916 - acc: 0.5258 - val\_loss: 1.2512 - val\_acc: 0.4840
Epoch 131/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0877 - acc: 0.5287 - val\_loss: 1.2510 - val\_acc: 0.4872
Epoch 132/200
35064/35064 [==============================] - 26s 744us/step - loss: 1.0861 - acc: 0.5287 - val\_loss: 1.2561 - val\_acc: 0.4823
Epoch 133/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0867 - acc: 0.5272 - val\_loss: 1.2521 - val\_acc: 0.4859
Epoch 134/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0812 - acc: 0.5320 - val\_loss: 1.2590 - val\_acc: 0.4844
Epoch 135/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0825 - acc: 0.5311 - val\_loss: 1.2558 - val\_acc: 0.4867
Epoch 136/200
35064/35064 [==============================] - 26s 744us/step - loss: 1.0806 - acc: 0.5323 - val\_loss: 1.2596 - val\_acc: 0.4781
Epoch 137/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0797 - acc: 0.5320 - val\_loss: 1.2604 - val\_acc: 0.4865
Epoch 138/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0788 - acc: 0.5327 - val\_loss: 1.2583 - val\_acc: 0.4878
Epoch 139/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0787 - acc: 0.5331 - val\_loss: 1.2556 - val\_acc: 0.4881
Epoch 140/200
35064/35064 [==============================] - 26s 750us/step - loss: 1.0773 - acc: 0.5345 - val\_loss: 1.2596 - val\_acc: 0.4838
Epoch 141/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0756 - acc: 0.5348 - val\_loss: 1.2597 - val\_acc: 0.4871
Epoch 142/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0722 - acc: 0.5359 - val\_loss: 1.2618 - val\_acc: 0.4887
Epoch 143/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.0732 - acc: 0.5364 - val\_loss: 1.2629 - val\_acc: 0.4872
Epoch 144/200
35064/35064 [==============================] - 26s 748us/step - loss: 1.0721 - acc: 0.5354 - val\_loss: 1.2616 - val\_acc: 0.4883
Epoch 145/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0724 - acc: 0.5341 - val\_loss: 1.2630 - val\_acc: 0.4870
Epoch 146/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0708 - acc: 0.5365 - val\_loss: 1.2659 - val\_acc: 0.4861
Epoch 147/200
35064/35064 [==============================] - 27s 763us/step - loss: 1.0674 - acc: 0.5368 - val\_loss: 1.2622 - val\_acc: 0.4881
Epoch 148/200
35064/35064 [==============================] - 28s 802us/step - loss: 1.0657 - acc: 0.5407 - val\_loss: 1.2638 - val\_acc: 0.4869
Epoch 149/200
35064/35064 [==============================] - 27s 779us/step - loss: 1.0675 - acc: 0.5362 - val\_loss: 1.2664 - val\_acc: 0.4868
Epoch 150/200
35064/35064 [==============================] - 26s 750us/step - loss: 1.0636 - acc: 0.5406 - val\_loss: 1.2806 - val\_acc: 0.4814
Epoch 151/200
35064/35064 [==============================] - 26s 750us/step - loss: 1.0627 - acc: 0.5405 - val\_loss: 1.2745 - val\_acc: 0.4841
Epoch 152/200
35064/35064 [==============================] - 27s 763us/step - loss: 1.0618 - acc: 0.5433 - val\_loss: 1.2735 - val\_acc: 0.4867
Epoch 153/200
35064/35064 [==============================] - 27s 757us/step - loss: 1.0612 - acc: 0.5433 - val\_loss: 1.2727 - val\_acc: 0.4879
Epoch 154/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0593 - acc: 0.5410 - val\_loss: 1.2712 - val\_acc: 0.4900
Epoch 155/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0608 - acc: 0.5447 - val\_loss: 1.2778 - val\_acc: 0.4843
Epoch 156/200
35064/35064 [==============================] - 26s 749us/step - loss: 1.0560 - acc: 0.5459 - val\_loss: 1.2684 - val\_acc: 0.4879
Epoch 157/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0557 - acc: 0.5422 - val\_loss: 1.2756 - val\_acc: 0.4853
Epoch 158/200
35064/35064 [==============================] - 26s 748us/step - loss: 1.0544 - acc: 0.5457 - val\_loss: 1.2740 - val\_acc: 0.4896
Epoch 159/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0527 - acc: 0.5486 - val\_loss: 1.2767 - val\_acc: 0.4875
Epoch 160/200
35064/35064 [==============================] - 26s 748us/step - loss: 1.0521 - acc: 0.5464 - val\_loss: 1.2767 - val\_acc: 0.4895
Epoch 161/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.0523 - acc: 0.5459 - val\_loss: 1.2814 - val\_acc: 0.4892
Epoch 162/200
35064/35064 [==============================] - 26s 751us/step - loss: 1.0500 - acc: 0.5477 - val\_loss: 1.2800 - val\_acc: 0.4873
Epoch 163/200
35064/35064 [==============================] - 27s 766us/step - loss: 1.0506 - acc: 0.5473 - val\_loss: 1.2848 - val\_acc: 0.4879
Epoch 164/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.0471 - acc: 0.5509 - val\_loss: 1.2832 - val\_acc: 0.4887
Epoch 165/200
35064/35064 [==============================] - 26s 748us/step - loss: 1.0487 - acc: 0.5506 - val\_loss: 1.2837 - val\_acc: 0.4871
Epoch 166/200
35064/35064 [==============================] - 26s 744us/step - loss: 1.0446 - acc: 0.5505 - val\_loss: 1.2947 - val\_acc: 0.4879
Epoch 167/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0439 - acc: 0.5485 - val\_loss: 1.2856 - val\_acc: 0.4913
Epoch 168/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0450 - acc: 0.5546 - val\_loss: 1.2848 - val\_acc: 0.4908
Epoch 169/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0432 - acc: 0.5524 - val\_loss: 1.2817 - val\_acc: 0.4930
Epoch 170/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.0388 - acc: 0.5542 - val\_loss: 1.2916 - val\_acc: 0.4889
Epoch 171/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0393 - acc: 0.5531 - val\_loss: 1.2931 - val\_acc: 0.4878
Epoch 172/200
35064/35064 [==============================] - 26s 750us/step - loss: 1.0400 - acc: 0.5526 - val\_loss: 1.2875 - val\_acc: 0.4878
Epoch 173/200
35064/35064 [==============================] - 26s 744us/step - loss: 1.0353 - acc: 0.5559 - val\_loss: 1.2963 - val\_acc: 0.4897
Epoch 174/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0346 - acc: 0.5566 - val\_loss: 1.2951 - val\_acc: 0.4897
Epoch 175/200
35064/35064 [==============================] - 26s 749us/step - loss: 1.0351 - acc: 0.5557 - val\_loss: 1.2947 - val\_acc: 0.4863
Epoch 176/200
35064/35064 [==============================] - 26s 749us/step - loss: 1.0341 - acc: 0.5580 - val\_loss: 1.2956 - val\_acc: 0.4906
Epoch 177/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0327 - acc: 0.5580 - val\_loss: 1.3026 - val\_acc: 0.4897
Epoch 178/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0317 - acc: 0.5590 - val\_loss: 1.3028 - val\_acc: 0.4862
Epoch 179/200
35064/35064 [==============================] - 26s 755us/step - loss: 1.0293 - acc: 0.5586 - val\_loss: 1.3000 - val\_acc: 0.4862
Epoch 180/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0281 - acc: 0.5589 - val\_loss: 1.2988 - val\_acc: 0.4868
Epoch 181/200
35064/35064 [==============================] - 26s 750us/step - loss: 1.0267 - acc: 0.5620 - val\_loss: 1.2996 - val\_acc: 0.4920
Epoch 182/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0254 - acc: 0.5590 - val\_loss: 1.3042 - val\_acc: 0.4887
Epoch 183/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.0263 - acc: 0.5599 - val\_loss: 1.3012 - val\_acc: 0.4916
Epoch 184/200
35064/35064 [==============================] - 27s 778us/step - loss: 1.0241 - acc: 0.5610 - val\_loss: 1.3071 - val\_acc: 0.4885
Epoch 185/200
35064/35064 [==============================] - 27s 781us/step - loss: 1.0246 - acc: 0.5597 - val\_loss: 1.3076 - val\_acc: 0.4898
Epoch 186/200
35064/35064 [==============================] - 27s 773us/step - loss: 1.0191 - acc: 0.5631 - val\_loss: 1.3079 - val\_acc: 0.4871
Epoch 187/200
35064/35064 [==============================] - 27s 764us/step - loss: 1.0228 - acc: 0.5615 - val\_loss: 1.3075 - val\_acc: 0.4901
Epoch 188/200
35064/35064 [==============================] - 26s 751us/step - loss: 1.0181 - acc: 0.5649 - val\_loss: 1.3097 - val\_acc: 0.4918
Epoch 189/200
35064/35064 [==============================] - 26s 749us/step - loss: 1.0197 - acc: 0.5640 - val\_loss: 1.3136 - val\_acc: 0.4887
Epoch 190/200
35064/35064 [==============================] - 26s 746us/step - loss: 1.0161 - acc: 0.5643 - val\_loss: 1.3060 - val\_acc: 0.4921
Epoch 191/200
35064/35064 [==============================] - 26s 744us/step - loss: 1.0128 - acc: 0.5660 - val\_loss: 1.3110 - val\_acc: 0.4959
Epoch 192/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0128 - acc: 0.5654 - val\_loss: 1.3101 - val\_acc: 0.4926
Epoch 193/200
35064/35064 [==============================] - 26s 744us/step - loss: 1.0137 - acc: 0.5670 - val\_loss: 1.3195 - val\_acc: 0.4892
Epoch 194/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0136 - acc: 0.5652 - val\_loss: 1.3188 - val\_acc: 0.4883
Epoch 195/200
35064/35064 [==============================] - 27s 757us/step - loss: 1.0092 - acc: 0.5681 - val\_loss: 1.3272 - val\_acc: 0.4895
Epoch 196/200
35064/35064 [==============================] - 26s 753us/step - loss: 1.0113 - acc: 0.5699 - val\_loss: 1.3204 - val\_acc: 0.4885
Epoch 197/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0084 - acc: 0.5700 - val\_loss: 1.3238 - val\_acc: 0.4897
Epoch 198/200
35064/35064 [==============================] - 26s 747us/step - loss: 1.0065 - acc: 0.5710 - val\_loss: 1.3195 - val\_acc: 0.4893
Epoch 199/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0052 - acc: 0.5707 - val\_loss: 1.3208 - val\_acc: 0.4933
Epoch 200/200
35064/35064 [==============================] - 26s 745us/step - loss: 1.0069 - acc: 0.5691 - val\_loss: 1.3194 - val\_acc: 0.4867
8766/8766 [==============================] - 2s 270us/step
Test score:  1.3194146647508793
Test accuracy:  0.4866529756380547

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{n}{mpulse2}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv1d\_88 (Conv1D)           (None, 1491, 20)          420       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_87 (MaxPooling (None, 149, 20)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_89 (Conv1D)           (None, 140, 20)           4020      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_88 (MaxPooling (None, 14, 20)            0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_44 (Flatten)         (None, 280)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_47 (Dense)             (None, 500)               140500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_4 (Dropout)          (None, 500)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_48 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_49 (Dense)             (None, 5)                 2505      
=================================================================
Total params: 397,945
Trainable params: 397,945
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.3690394706821812
kappa:  0.011722820441458115

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{n}{macc2}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv1d\_90 (Conv1D)           (None, 1491, 20)          620       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_89 (MaxPooling (None, 149, 20)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_91 (Conv1D)           (None, 140, 20)           4020      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_90 (MaxPooling (None, 14, 20)            0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_45 (Flatten)         (None, 280)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_50 (Dense)             (None, 500)               140500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_5 (Dropout)          (None, 500)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_51 (Dense)             (None, 500)               250500    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_52 (Dense)             (None, 5)                 2505      
=================================================================
Total params: 398,145
Trainable params: 398,145
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.486652977412731
kappa:  0.2703531328434333

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k}{def} \PY{n+nf}{submodel}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
             \PY{n}{inputs} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{conv1} \PY{o}{=} \PY{n}{Conv1D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}
             \PY{n}{pool1} \PY{o}{=} \PY{n}{MaxPooling1D}\PY{p}{(}\PY{n}{maxPool1}\PY{p}{)}\PY{p}{(}\PY{n}{conv1}\PY{p}{)}
             \PY{n}{conv2} \PY{o}{=} \PY{n}{Conv1D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel2}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{pool1}\PY{p}{)}
             \PY{n}{pool2} \PY{o}{=} \PY{n}{MaxPooling1D}\PY{p}{(}\PY{n}{maxPool2}\PY{p}{)}\PY{p}{(}\PY{n}{conv2}\PY{p}{)}
         
             \PY{n}{outputs} \PY{o}{=} \PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{(}\PY{n}{pool2}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{Model}\PY{p}{(}\PY{n}{inputs}\PY{o}{=}\PY{n}{inputs}\PY{p}{,} \PY{n}{outputs}\PY{o}{=}\PY{n}{outputs}\PY{p}{)} 
         
         \PY{k}{def} \PY{n+nf}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,}  \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{)}\PY{p}{:}
             \PY{n}{inputs} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{conv1} \PY{o}{=} \PY{n}{Conv1D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}
             \PY{n}{pool1} \PY{o}{=} \PY{n}{MaxPooling1D}\PY{p}{(}\PY{n}{maxPool1}\PY{p}{)}\PY{p}{(}\PY{n}{conv1}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}conv2 = Conv1D(nb\PYZus{}filter=20, kernel\PYZus{}size=kernel2, activation=\PYZsq{}relu\PYZsq{}, strides = 1, padding=\PYZsq{}valid\PYZsq{})(pool1)}
             \PY{c+c1}{\PYZsh{}pool2 = MaxPooling1D(maxPool2)(conv2)}
             
             \PY{k}{return} \PY{n}{Model}\PY{p}{(}\PY{n}{inputs}\PY{o}{=}\PY{n}{inputs}\PY{p}{,} \PY{n}{outputs}\PY{o}{=}\PY{n}{pool1}\PY{p}{)} 
\end{Verbatim}


    \section{model with eeg, pulse and
accelerometer}\label{model-with-eeg-pulse-and-accelerometer}

 On top of concatenation layer there is two 500 dense layers

\begin{figure}
\centering
\includegraphics{mergeModel1.png}
\caption{title}
\end{figure}

with model1, model2 and model3 with two 1D convolution layers - for eeg:
nb\_filter1=20, nb\_filter2=20, kernel1=25, kernel2=10, maxPool1=25,
maxPool2=10 - for pulse and accelerometer: nb\_filter1=20,
nb\_filter2=20, kernel1=10, kernel2=10, maxPool1=25, maxPool2=10

\includegraphics{model2_eeg.png}

Performances after 200 epochs (start to overfeed before 100 epochs -
accurancy: 0.69 - kappa: 0.56

Overfitting:

\includegraphics{mergeModel1_acc.png}
\includegraphics{mergeModel1_loss.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} 3 convolution subnet are concatenated before being inputed to a dense layer}
         
         \PY{n}{model\PYZus{}eeg} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{model\PYZus{}pulse} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{model\PYZus{}acc} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                              \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{n}{in\PYZus{}eeg} \PY{o}{=}  \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}pulse} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}acc} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{accelerometer\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{out\PYZus{}eeg} \PY{o}{=} \PY{n}{model\PYZus{}eeg}\PY{p}{(}\PY{n}{in\PYZus{}eeg}\PY{p}{)}
         \PY{n}{out\PYZus{}pulse} \PY{o}{=} \PY{n}{model\PYZus{}pulse}\PY{p}{(}\PY{n}{in\PYZus{}pulse}\PY{p}{)}
         \PY{n}{out\PYZus{}acc} \PY{o}{=} \PY{n}{model\PYZus{}acc}\PY{p}{(}\PY{n}{in\PYZus{}acc}\PY{p}{)}
         
         \PY{n}{concatenated} \PY{o}{=} \PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{out\PYZus{}eeg}\PY{p}{,} \PY{n}{out\PYZus{}pulse}\PY{p}{,} \PY{n}{out\PYZus{}acc}\PY{p}{]}\PY{p}{)}
         \PY{n}{dense1} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{concatenated}\PY{p}{)}
         \PY{n}{dense2} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense1}\PY{p}{)}
         \PY{n}{out} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense2}\PY{p}{)}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{p}{[}\PY{n}{in\PYZus{}eeg}\PY{p}{,} \PY{n}{in\PYZus{}pulse}\PY{p}{,} \PY{n}{in\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{out}\PY{p}{)}
         
         
         \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{200}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{score} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test score: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test accuracy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}plot\PYZus{}model(model, to\PYZus{}file=\PYZsq{}convolutional\PYZus{}neural\PYZus{}network.png\PYZsq{})}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=20)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 144s 4ms/step - loss: 5.2762 - acc: 0.4281 - val\_loss: 4.0137 - val\_acc: 0.4444
Epoch 2/200
35064/35064 [==============================] - 142s 4ms/step - loss: 3.7405 - acc: 0.4830 - val\_loss: 3.8436 - val\_acc: 0.4582
Epoch 3/200
35064/35064 [==============================] - 142s 4ms/step - loss: 3.2359 - acc: 0.5146 - val\_loss: 2.9879 - val\_acc: 0.5414
Epoch 4/200
35064/35064 [==============================] - 144s 4ms/step - loss: 3.0345 - acc: 0.5272 - val\_loss: 2.9616 - val\_acc: 0.5302
Epoch 5/200
35064/35064 [==============================] - 141s 4ms/step - loss: 2.7980 - acc: 0.5560 - val\_loss: 2.6277 - val\_acc: 0.5615
Epoch 6/200
35064/35064 [==============================] - 141s 4ms/step - loss: 2.4267 - acc: 0.5828 - val\_loss: 2.3842 - val\_acc: 0.5829
Epoch 7/200
35064/35064 [==============================] - 139s 4ms/step - loss: 3.4395 - acc: 0.5090 - val\_loss: 2.4533 - val\_acc: 0.5807
Epoch 8/200
35064/35064 [==============================] - 138s 4ms/step - loss: 2.4292 - acc: 0.5822 - val\_loss: 2.5135 - val\_acc: 0.5864
Epoch 9/200
35064/35064 [==============================] - 138s 4ms/step - loss: 2.2948 - acc: 0.5926 - val\_loss: 2.4930 - val\_acc: 0.5708
Epoch 10/200
35064/35064 [==============================] - 140s 4ms/step - loss: 2.2197 - acc: 0.6016 - val\_loss: 2.2469 - val\_acc: 0.5933
Epoch 11/200
35064/35064 [==============================] - 140s 4ms/step - loss: 2.0302 - acc: 0.6185 - val\_loss: 2.1603 - val\_acc: 0.6069
Epoch 12/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.9074 - acc: 0.6274 - val\_loss: 2.0985 - val\_acc: 0.6057
Epoch 13/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.9026 - acc: 0.6279 - val\_loss: 2.1693 - val\_acc: 0.5999
Epoch 14/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.8615 - acc: 0.6371 - val\_loss: 2.0164 - val\_acc: 0.6237
Epoch 15/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.8104 - acc: 0.6415 - val\_loss: 2.0232 - val\_acc: 0.6216
Epoch 16/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.8018 - acc: 0.6394 - val\_loss: 2.1198 - val\_acc: 0.6052
Epoch 17/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.6930 - acc: 0.6504 - val\_loss: 1.9883 - val\_acc: 0.6182
Epoch 18/200
35064/35064 [==============================] - 140s 4ms/step - loss: 1.6343 - acc: 0.6553 - val\_loss: 1.9655 - val\_acc: 0.6191
Epoch 19/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.5969 - acc: 0.6626 - val\_loss: 1.9258 - val\_acc: 0.6319
Epoch 20/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.5416 - acc: 0.6734 - val\_loss: 1.9194 - val\_acc: 0.6334
Epoch 21/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.6915 - acc: 0.6516 - val\_loss: 1.9553 - val\_acc: 0.6219
Epoch 22/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.6082 - acc: 0.6622 - val\_loss: 1.8484 - val\_acc: 0.6368
Epoch 23/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.6998 - acc: 0.6529 - val\_loss: 1.8939 - val\_acc: 0.6328
Epoch 24/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.5584 - acc: 0.6670 - val\_loss: 1.9653 - val\_acc: 0.6064
Epoch 25/200
35064/35064 [==============================] - 140s 4ms/step - loss: 1.5496 - acc: 0.6686 - val\_loss: 1.8101 - val\_acc: 0.6425
Epoch 26/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.4546 - acc: 0.6841 - val\_loss: 1.7980 - val\_acc: 0.6431
Epoch 27/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.4486 - acc: 0.6851 - val\_loss: 1.8781 - val\_acc: 0.6384
Epoch 28/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.4099 - acc: 0.6927 - val\_loss: 1.9131 - val\_acc: 0.6292
Epoch 29/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.4724 - acc: 0.6841 - val\_loss: 1.8114 - val\_acc: 0.6452
Epoch 30/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.3342 - acc: 0.7024 - val\_loss: 1.7627 - val\_acc: 0.6590
Epoch 31/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.4186 - acc: 0.6918 - val\_loss: 1.7446 - val\_acc: 0.6533
Epoch 32/200
35064/35064 [==============================] - 140s 4ms/step - loss: 1.2855 - acc: 0.7103 - val\_loss: 1.9028 - val\_acc: 0.6336
Epoch 33/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.3166 - acc: 0.7052 - val\_loss: 1.7471 - val\_acc: 0.6597
Epoch 34/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.2744 - acc: 0.7146 - val\_loss: 1.7289 - val\_acc: 0.6579
Epoch 35/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.2727 - acc: 0.7155 - val\_loss: 1.8769 - val\_acc: 0.6404
Epoch 36/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.2663 - acc: 0.7183 - val\_loss: 1.7427 - val\_acc: 0.6622
Epoch 37/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.2253 - acc: 0.7287 - val\_loss: 1.6900 - val\_acc: 0.6655
Epoch 38/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.3542 - acc: 0.7093 - val\_loss: 2.0176 - val\_acc: 0.6433
Epoch 39/200
35064/35064 [==============================] - 140s 4ms/step - loss: 1.4564 - acc: 0.6928 - val\_loss: 1.7884 - val\_acc: 0.6622
Epoch 40/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.3123 - acc: 0.7142 - val\_loss: 1.7400 - val\_acc: 0.6615
Epoch 41/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.2192 - acc: 0.7287 - val\_loss: 1.6969 - val\_acc: 0.6689
Epoch 42/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.1682 - acc: 0.7402 - val\_loss: 1.7364 - val\_acc: 0.6624
Epoch 43/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.1738 - acc: 0.7400 - val\_loss: 1.7098 - val\_acc: 0.6666
Epoch 44/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.2435 - acc: 0.7280 - val\_loss: 1.7602 - val\_acc: 0.6548
Epoch 45/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.2859 - acc: 0.7242 - val\_loss: 1.8110 - val\_acc: 0.6601
Epoch 46/200
35064/35064 [==============================] - 140s 4ms/step - loss: 1.2898 - acc: 0.7204 - val\_loss: 1.7842 - val\_acc: 0.6606
Epoch 47/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.2068 - acc: 0.7388 - val\_loss: 1.7259 - val\_acc: 0.6705
Epoch 48/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.1618 - acc: 0.7477 - val\_loss: 1.6906 - val\_acc: 0.6687
Epoch 49/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.1279 - acc: 0.7531 - val\_loss: 1.7258 - val\_acc: 0.6670
Epoch 50/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.1577 - acc: 0.7481 - val\_loss: 1.7121 - val\_acc: 0.6707
Epoch 51/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.1347 - acc: 0.7541 - val\_loss: 1.6959 - val\_acc: 0.6743
Epoch 52/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.1308 - acc: 0.7568 - val\_loss: 1.6965 - val\_acc: 0.6750
Epoch 53/200
35064/35064 [==============================] - 140s 4ms/step - loss: 1.0699 - acc: 0.7717 - val\_loss: 1.6711 - val\_acc: 0.6791
Epoch 54/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0466 - acc: 0.7728 - val\_loss: 1.6825 - val\_acc: 0.6798
Epoch 55/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0465 - acc: 0.7767 - val\_loss: 1.6843 - val\_acc: 0.6770
Epoch 56/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0345 - acc: 0.7801 - val\_loss: 1.6425 - val\_acc: 0.6823
Epoch 57/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.0118 - acc: 0.7852 - val\_loss: 1.6394 - val\_acc: 0.6840
Epoch 58/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0540 - acc: 0.7778 - val\_loss: 1.6701 - val\_acc: 0.6813
Epoch 59/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0193 - acc: 0.7860 - val\_loss: 1.7112 - val\_acc: 0.6825
Epoch 60/200
35064/35064 [==============================] - 140s 4ms/step - loss: 1.0306 - acc: 0.7858 - val\_loss: 1.7209 - val\_acc: 0.6817
Epoch 61/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.0136 - acc: 0.7899 - val\_loss: 1.6648 - val\_acc: 0.6866
Epoch 62/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0064 - acc: 0.7906 - val\_loss: 1.6700 - val\_acc: 0.6878
Epoch 63/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9770 - acc: 0.7971 - val\_loss: 1.6668 - val\_acc: 0.6905
Epoch 64/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0150 - acc: 0.7883 - val\_loss: 1.7020 - val\_acc: 0.6853
Epoch 65/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0667 - acc: 0.7851 - val\_loss: 1.8428 - val\_acc: 0.6715
Epoch 66/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.1434 - acc: 0.7646 - val\_loss: 1.7021 - val\_acc: 0.6920
Epoch 67/200
35064/35064 [==============================] - 140s 4ms/step - loss: 1.0200 - acc: 0.7908 - val\_loss: 1.7178 - val\_acc: 0.6869
Epoch 68/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0177 - acc: 0.7905 - val\_loss: 1.7014 - val\_acc: 0.6870
Epoch 69/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9700 - acc: 0.8002 - val\_loss: 1.6930 - val\_acc: 0.6899
Epoch 70/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.0761 - acc: 0.7837 - val\_loss: 1.6892 - val\_acc: 0.6888
Epoch 71/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9734 - acc: 0.8001 - val\_loss: 1.7259 - val\_acc: 0.6841
Epoch 72/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0007 - acc: 0.7942 - val\_loss: 1.7262 - val\_acc: 0.6916
Epoch 73/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9395 - acc: 0.8104 - val\_loss: 1.7899 - val\_acc: 0.6753
Epoch 74/200
35064/35064 [==============================] - 139s 4ms/step - loss: 0.9571 - acc: 0.8065 - val\_loss: 1.7007 - val\_acc: 0.6846
Epoch 75/200
35064/35064 [==============================] - 138s 4ms/step - loss: 1.1590 - acc: 0.7678 - val\_loss: 1.8227 - val\_acc: 0.6777
Epoch 76/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0556 - acc: 0.7825 - val\_loss: 1.7691 - val\_acc: 0.6794
Epoch 77/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9701 - acc: 0.8037 - val\_loss: 1.7386 - val\_acc: 0.6845
Epoch 78/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9234 - acc: 0.8158 - val\_loss: 1.8928 - val\_acc: 0.6752
Epoch 79/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0562 - acc: 0.7885 - val\_loss: 1.7734 - val\_acc: 0.6857
Epoch 80/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9272 - acc: 0.8159 - val\_loss: 1.7429 - val\_acc: 0.6928
Epoch 81/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.9445 - acc: 0.8155 - val\_loss: 1.7311 - val\_acc: 0.6890
Epoch 82/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9013 - acc: 0.8248 - val\_loss: 1.7274 - val\_acc: 0.6907
Epoch 83/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.9606 - acc: 0.8139 - val\_loss: 1.7959 - val\_acc: 0.6828
Epoch 84/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9376 - acc: 0.8163 - val\_loss: 1.7526 - val\_acc: 0.6870
Epoch 85/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.8892 - acc: 0.8270 - val\_loss: 1.7603 - val\_acc: 0.6946
Epoch 86/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9019 - acc: 0.8273 - val\_loss: 1.8375 - val\_acc: 0.6890
Epoch 87/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.8853 - acc: 0.8320 - val\_loss: 1.7808 - val\_acc: 0.6901
Epoch 88/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.8700 - acc: 0.8370 - val\_loss: 1.7959 - val\_acc: 0.6883
Epoch 89/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8832 - acc: 0.8360 - val\_loss: 1.7683 - val\_acc: 0.6945
Epoch 90/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8640 - acc: 0.8421 - val\_loss: 1.8149 - val\_acc: 0.6843
Epoch 91/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8651 - acc: 0.8377 - val\_loss: 1.7494 - val\_acc: 0.6939
Epoch 92/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8504 - acc: 0.8413 - val\_loss: 1.7601 - val\_acc: 0.6926
Epoch 93/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8855 - acc: 0.8331 - val\_loss: 1.7994 - val\_acc: 0.6934
Epoch 94/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8137 - acc: 0.8547 - val\_loss: 1.7840 - val\_acc: 0.6956
Epoch 95/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.8227 - acc: 0.8528 - val\_loss: 1.8385 - val\_acc: 0.6885
Epoch 96/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.9335 - acc: 0.8266 - val\_loss: 1.8731 - val\_acc: 0.6841
Epoch 97/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9080 - acc: 0.8361 - val\_loss: 1.8534 - val\_acc: 0.6896
Epoch 98/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8479 - acc: 0.8510 - val\_loss: 1.8753 - val\_acc: 0.6857
Epoch 99/200
35064/35064 [==============================] - 137s 4ms/step - loss: 1.0121 - acc: 0.8218 - val\_loss: 1.9838 - val\_acc: 0.6691
Epoch 100/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.9631 - acc: 0.8190 - val\_loss: 1.8892 - val\_acc: 0.6828
Epoch 101/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8415 - acc: 0.8485 - val\_loss: 1.8871 - val\_acc: 0.6841
Epoch 102/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.8091 - acc: 0.8578 - val\_loss: 1.8736 - val\_acc: 0.6929
Epoch 103/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8261 - acc: 0.8535 - val\_loss: 1.9602 - val\_acc: 0.6797
Epoch 104/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.9307 - acc: 0.8345 - val\_loss: 1.8819 - val\_acc: 0.6932
Epoch 105/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8899 - acc: 0.8416 - val\_loss: 1.9216 - val\_acc: 0.6814
Epoch 106/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.8518 - acc: 0.8487 - val\_loss: 1.8460 - val\_acc: 0.6935
Epoch 107/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8060 - acc: 0.8614 - val\_loss: 1.8508 - val\_acc: 0.6943
Epoch 108/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7777 - acc: 0.8687 - val\_loss: 1.8562 - val\_acc: 0.6953
Epoch 109/200
35064/35064 [==============================] - 141s 4ms/step - loss: 0.7649 - acc: 0.8721 - val\_loss: 1.8714 - val\_acc: 0.6946
Epoch 110/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7506 - acc: 0.8784 - val\_loss: 1.8873 - val\_acc: 0.6947
Epoch 111/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7408 - acc: 0.8824 - val\_loss: 1.8935 - val\_acc: 0.6929
Epoch 112/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7350 - acc: 0.8838 - val\_loss: 1.8857 - val\_acc: 0.6931
Epoch 113/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7789 - acc: 0.8741 - val\_loss: 1.9808 - val\_acc: 0.6878
Epoch 114/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8045 - acc: 0.8634 - val\_loss: 1.9353 - val\_acc: 0.6896
Epoch 115/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7471 - acc: 0.8807 - val\_loss: 2.0022 - val\_acc: 0.6800
Epoch 116/200
35064/35064 [==============================] - 141s 4ms/step - loss: 0.7904 - acc: 0.8713 - val\_loss: 1.9381 - val\_acc: 0.6865
Epoch 117/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7998 - acc: 0.8682 - val\_loss: 1.9644 - val\_acc: 0.6858
Epoch 118/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.8114 - acc: 0.8640 - val\_loss: 1.9292 - val\_acc: 0.6898
Epoch 119/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7744 - acc: 0.8732 - val\_loss: 1.9816 - val\_acc: 0.6858
Epoch 120/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7265 - acc: 0.8852 - val\_loss: 1.9728 - val\_acc: 0.6936
Epoch 121/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7035 - acc: 0.8919 - val\_loss: 1.9780 - val\_acc: 0.6932
Epoch 122/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7649 - acc: 0.8788 - val\_loss: 1.9840 - val\_acc: 0.6954
Epoch 123/200
35064/35064 [==============================] - 141s 4ms/step - loss: 0.6976 - acc: 0.8963 - val\_loss: 1.9710 - val\_acc: 0.6921
Epoch 124/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8087 - acc: 0.8696 - val\_loss: 2.0539 - val\_acc: 0.6858
Epoch 125/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8602 - acc: 0.8618 - val\_loss: 2.0926 - val\_acc: 0.6751
Epoch 126/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.8336 - acc: 0.8661 - val\_loss: 2.0927 - val\_acc: 0.6815
Epoch 127/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8443 - acc: 0.8670 - val\_loss: 2.0437 - val\_acc: 0.6870
Epoch 128/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7485 - acc: 0.8885 - val\_loss: 2.0031 - val\_acc: 0.6940
Epoch 129/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7120 - acc: 0.8992 - val\_loss: 2.0335 - val\_acc: 0.6909
Epoch 130/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.7327 - acc: 0.8893 - val\_loss: 2.0445 - val\_acc: 0.6905
Epoch 131/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7036 - acc: 0.8986 - val\_loss: 1.9983 - val\_acc: 0.6962
Epoch 132/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7914 - acc: 0.8783 - val\_loss: 2.1721 - val\_acc: 0.6737
Epoch 133/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8735 - acc: 0.8561 - val\_loss: 2.0250 - val\_acc: 0.6912
Epoch 134/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7699 - acc: 0.8836 - val\_loss: 2.0388 - val\_acc: 0.6899
Epoch 135/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7266 - acc: 0.8941 - val\_loss: 2.0329 - val\_acc: 0.6871
Epoch 136/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7205 - acc: 0.8953 - val\_loss: 2.0464 - val\_acc: 0.6930
Epoch 137/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.7514 - acc: 0.8877 - val\_loss: 2.1716 - val\_acc: 0.6867
Epoch 138/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7664 - acc: 0.8840 - val\_loss: 2.0589 - val\_acc: 0.6885
Epoch 139/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6993 - acc: 0.9011 - val\_loss: 2.0373 - val\_acc: 0.6926
Epoch 140/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7255 - acc: 0.8961 - val\_loss: 2.0482 - val\_acc: 0.6902
Epoch 141/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7534 - acc: 0.8930 - val\_loss: 2.1280 - val\_acc: 0.6871
Epoch 142/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7593 - acc: 0.8938 - val\_loss: 2.1204 - val\_acc: 0.6919
Epoch 143/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7403 - acc: 0.8974 - val\_loss: 2.1288 - val\_acc: 0.6915
Epoch 144/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.7068 - acc: 0.9022 - val\_loss: 2.1444 - val\_acc: 0.6880
Epoch 145/200
35064/35064 [==============================] - 139s 4ms/step - loss: 0.6979 - acc: 0.9036 - val\_loss: 2.0699 - val\_acc: 0.6935
Epoch 146/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6514 - acc: 0.9148 - val\_loss: 2.0791 - val\_acc: 0.6928
Epoch 147/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6604 - acc: 0.9120 - val\_loss: 2.0973 - val\_acc: 0.6946
Epoch 148/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6417 - acc: 0.9169 - val\_loss: 2.1270 - val\_acc: 0.6966
Epoch 149/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6227 - acc: 0.9235 - val\_loss: 2.1516 - val\_acc: 0.6828
Epoch 150/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6497 - acc: 0.9136 - val\_loss: 2.2967 - val\_acc: 0.6733
Epoch 151/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.8068 - acc: 0.8804 - val\_loss: 2.1529 - val\_acc: 0.6834
Epoch 152/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7095 - acc: 0.9016 - val\_loss: 2.1387 - val\_acc: 0.6880
Epoch 153/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6611 - acc: 0.9141 - val\_loss: 2.1600 - val\_acc: 0.6894
Epoch 154/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6293 - acc: 0.9245 - val\_loss: 2.1423 - val\_acc: 0.6891
Epoch 155/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6304 - acc: 0.9240 - val\_loss: 2.1351 - val\_acc: 0.6922
Epoch 156/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6274 - acc: 0.9235 - val\_loss: 2.1716 - val\_acc: 0.6894
Epoch 157/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.8467 - acc: 0.8710 - val\_loss: 2.2029 - val\_acc: 0.6903
Epoch 158/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.6863 - acc: 0.9103 - val\_loss: 2.2022 - val\_acc: 0.6875
Epoch 159/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6515 - acc: 0.9177 - val\_loss: 2.2214 - val\_acc: 0.6870
Epoch 160/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6741 - acc: 0.9167 - val\_loss: 2.1922 - val\_acc: 0.6887
Epoch 161/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6379 - acc: 0.9240 - val\_loss: 2.1854 - val\_acc: 0.6930
Epoch 162/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6937 - acc: 0.9114 - val\_loss: 2.2353 - val\_acc: 0.6832
Epoch 163/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6491 - acc: 0.9225 - val\_loss: 2.2622 - val\_acc: 0.6853
Epoch 164/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6317 - acc: 0.9266 - val\_loss: 2.2186 - val\_acc: 0.6931
Epoch 165/200
35064/35064 [==============================] - 139s 4ms/step - loss: 0.6059 - acc: 0.9354 - val\_loss: 2.2518 - val\_acc: 0.6825
Epoch 166/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6957 - acc: 0.9041 - val\_loss: 2.2600 - val\_acc: 0.6845
Epoch 167/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.7294 - acc: 0.9010 - val\_loss: 2.5466 - val\_acc: 0.6606
Epoch 168/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7166 - acc: 0.9033 - val\_loss: 2.3014 - val\_acc: 0.6828
Epoch 169/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6330 - acc: 0.9265 - val\_loss: 2.3093 - val\_acc: 0.6885
Epoch 170/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6408 - acc: 0.9242 - val\_loss: 2.2721 - val\_acc: 0.6871
Epoch 171/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6105 - acc: 0.9305 - val\_loss: 2.2752 - val\_acc: 0.6848
Epoch 172/200
35064/35064 [==============================] - 139s 4ms/step - loss: 0.5843 - acc: 0.9387 - val\_loss: 2.3060 - val\_acc: 0.6873
Epoch 173/200
35064/35064 [==============================] - 139s 4ms/step - loss: 0.6447 - acc: 0.9220 - val\_loss: 2.2835 - val\_acc: 0.6861
Epoch 174/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6640 - acc: 0.9148 - val\_loss: 2.2846 - val\_acc: 0.6840
Epoch 175/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.5975 - acc: 0.9324 - val\_loss: 2.3051 - val\_acc: 0.6874
Epoch 176/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.5833 - acc: 0.9379 - val\_loss: 2.3098 - val\_acc: 0.6824
Epoch 177/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.5804 - acc: 0.9400 - val\_loss: 2.2902 - val\_acc: 0.6890
Epoch 178/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.5826 - acc: 0.9387 - val\_loss: 2.3428 - val\_acc: 0.6869
Epoch 179/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.5835 - acc: 0.9405 - val\_loss: 2.3300 - val\_acc: 0.6873
Epoch 180/200
35064/35064 [==============================] - 139s 4ms/step - loss: 0.5708 - acc: 0.9432 - val\_loss: 2.3279 - val\_acc: 0.6858
Epoch 181/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6840 - acc: 0.9184 - val\_loss: 2.3455 - val\_acc: 0.6856
Epoch 182/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6034 - acc: 0.9333 - val\_loss: 2.3340 - val\_acc: 0.6866
Epoch 183/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6026 - acc: 0.9361 - val\_loss: 2.3632 - val\_acc: 0.6866
Epoch 184/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.5731 - acc: 0.9427 - val\_loss: 2.3874 - val\_acc: 0.6848
Epoch 185/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.5992 - acc: 0.9354 - val\_loss: 2.3524 - val\_acc: 0.6912
Epoch 186/200
35064/35064 [==============================] - 139s 4ms/step - loss: 0.6237 - acc: 0.9338 - val\_loss: 2.3736 - val\_acc: 0.6859
Epoch 187/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.5980 - acc: 0.9420 - val\_loss: 2.3688 - val\_acc: 0.6826
Epoch 188/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.7787 - acc: 0.8995 - val\_loss: 2.3733 - val\_acc: 0.6911
Epoch 189/200
35064/35064 [==============================] - 145s 4ms/step - loss: 0.7129 - acc: 0.9112 - val\_loss: 2.4780 - val\_acc: 0.6804
Epoch 190/200
35064/35064 [==============================] - 142s 4ms/step - loss: 0.6972 - acc: 0.9152 - val\_loss: 2.4421 - val\_acc: 0.6824
Epoch 191/200
35064/35064 [==============================] - 139s 4ms/step - loss: 0.6073 - acc: 0.9358 - val\_loss: 2.4171 - val\_acc: 0.6858
Epoch 192/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.5962 - acc: 0.9392 - val\_loss: 2.4374 - val\_acc: 0.6856
Epoch 193/200
35064/35064 [==============================] - 140s 4ms/step - loss: 0.6259 - acc: 0.9324 - val\_loss: 2.4680 - val\_acc: 0.6832
Epoch 194/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.6413 - acc: 0.9289 - val\_loss: 2.4076 - val\_acc: 0.6891
Epoch 195/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.6269 - acc: 0.9343 - val\_loss: 2.3884 - val\_acc: 0.6899
Epoch 196/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.5868 - acc: 0.9421 - val\_loss: 2.4256 - val\_acc: 0.6855
Epoch 197/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.5962 - acc: 0.9408 - val\_loss: 2.4152 - val\_acc: 0.6948
Epoch 198/200
35064/35064 [==============================] - 137s 4ms/step - loss: 0.5632 - acc: 0.9473 - val\_loss: 2.4614 - val\_acc: 0.6853
Epoch 199/200
35064/35064 [==============================] - 138s 4ms/step - loss: 0.5760 - acc: 0.9449 - val\_loss: 2.4659 - val\_acc: 0.6873
Epoch 200/200
35064/35064 [==============================] - 142s 4ms/step - loss: 0.5886 - acc: 0.9413 - val\_loss: 2.4760 - val\_acc: 0.6870
8766/8766 [==============================] - 13s 1ms/step
Test score:  2.475978355170547
Test accuracy:  0.6869723940722452

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{y\PYZus{}probas} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
         
         \PY{n}{y\PYZus{}classes} \PY{o}{=} \PY{n}{y\PYZus{}probas}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{}keras.np\PYZus{}utils.probas\PYZus{}to\PYZus{}classes(y\PYZus{}probas)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{y\PYZus{}classes}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accurancy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kappa: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cohen\PYZus{}kappa\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.6869723933378964
kappa:  0.5640655164987017

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{p}{,} \PY{n}{history}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{sequential}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_36 (InputLayer)           (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_37 (InputLayer)           (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_38 (InputLayer)           (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_23 (Model)                (None, 280)          6040        input\_36[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_24 (Model)                (None, 100)          4440        input\_37[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_25 (Model)                (None, 100)          4640        input\_38[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_7 (Concatenate)     (None, 480)          0           model\_23[1][0]                   
                                                                 model\_24[1][0]                   
                                                                 model\_25[1][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_22 (Dense)                (None, 500)          240500      concatenate\_7[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_23 (Dense)                (None, 500)          250500      dense\_22[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_24 (Dense)                (None, 5)            2505        dense\_23[0][0]                   
==================================================================================================
Total params: 508,625
Trainable params: 508,625
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.6869723933378964
kappa:  0.5640655164987017

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{model\PYZus{}eeg} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{model\PYZus{}pulse} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{model\PYZus{}acc} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                              \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{n}{in\PYZus{}eeg} \PY{o}{=}  \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}pulse} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}acc} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{accelerometer\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{out\PYZus{}eeg} \PY{o}{=} \PY{n}{model\PYZus{}eeg}\PY{p}{(}\PY{n}{in\PYZus{}eeg}\PY{p}{)}
         \PY{n}{out\PYZus{}pulse} \PY{o}{=} \PY{n}{model\PYZus{}pulse}\PY{p}{(}\PY{n}{in\PYZus{}pulse}\PY{p}{)}
         \PY{n}{out\PYZus{}acc} \PY{o}{=} \PY{n}{model\PYZus{}acc}\PY{p}{(}\PY{n}{in\PYZus{}acc}\PY{p}{)}
         
         \PY{n}{concatenated} \PY{o}{=} \PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{out\PYZus{}eeg}\PY{p}{,} \PY{n}{out\PYZus{}pulse}\PY{p}{,} \PY{n}{out\PYZus{}acc}\PY{p}{]}\PY{p}{)}
         \PY{n}{dense1} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{concatenated}\PY{p}{)}
         \PY{n}{drop1} \PY{o}{=} \PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{(}\PY{n}{dense1}\PY{p}{)}
         \PY{n}{dense2} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{drop1}\PY{p}{)}
         \PY{n}{out} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense2}\PY{p}{)}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{p}{[}\PY{n}{in\PYZus{}eeg}\PY{p}{,} \PY{n}{in\PYZus{}pulse}\PY{p}{,} \PY{n}{in\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{out}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}plot\PYZus{}model(model, to\PYZus{}file=\PYZsq{}mergeModel1 with dropout.png\PYZsq{})}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  

    \end{Verbatim}

    Same model except dropout(0.5) between the two dense layers

\includegraphics{mergeModel1 with dropout.png}

Performance: - accurancy: 0.68 - kappa: 0.53

Accurancy stays the same than without dropout, kappa is a bit worst.
However overfitting issue is solved

\includegraphics{mergeModel1dropout_acc.png}

\begin{figure}
\centering
\includegraphics{mergeModel1dropout_loss.png}
\caption{title}
\end{figure}

This suggest than reducing overfitting by limiting epochs number might
be more efficient than drop out

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{}avec dropout}
         
         \PY{n}{tensor\PYZus{}board} \PY{o}{=} \PY{n}{TensorBoard}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./logs/dropout}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{model\PYZus{}eeg} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{model\PYZus{}pulse} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{model\PYZus{}acc} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                              \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{n}{in\PYZus{}eeg} \PY{o}{=}  \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}pulse} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}acc} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{accelerometer\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{out\PYZus{}eeg} \PY{o}{=} \PY{n}{model\PYZus{}eeg}\PY{p}{(}\PY{n}{in\PYZus{}eeg}\PY{p}{)}
         \PY{n}{out\PYZus{}pulse} \PY{o}{=} \PY{n}{model\PYZus{}pulse}\PY{p}{(}\PY{n}{in\PYZus{}pulse}\PY{p}{)}
         \PY{n}{out\PYZus{}acc} \PY{o}{=} \PY{n}{model\PYZus{}acc}\PY{p}{(}\PY{n}{in\PYZus{}acc}\PY{p}{)}
         
         \PY{n}{concatenated} \PY{o}{=} \PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{out\PYZus{}eeg}\PY{p}{,} \PY{n}{out\PYZus{}pulse}\PY{p}{,} \PY{n}{out\PYZus{}acc}\PY{p}{]}\PY{p}{)}
         \PY{n}{dense1} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{concatenated}\PY{p}{)}
         \PY{n}{drop1} \PY{o}{=} \PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{(}\PY{n}{dense1}\PY{p}{)}
         \PY{n}{dense2} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{drop1}\PY{p}{)}
         \PY{n}{out} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense2}\PY{p}{)}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{p}{[}\PY{n}{in\PYZus{}eeg}\PY{p}{,} \PY{n}{in\PYZus{}pulse}\PY{p}{,} \PY{n}{in\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{out}\PY{p}{)}
         
         
         \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{200}
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{epochs}\PY{o}{=} \PY{n}{epochs}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{tensor\PYZus{}board}\PY{p}{]}\PY{p}{)}
         \PY{n}{score} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test score: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test accuracy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 26s 731us/step - loss: 9.3854 - acc: 0.3490 - val\_loss: 5.9244 - val\_acc: 0.4255
Epoch 2/200
35064/35064 [==============================] - 22s 616us/step - loss: 5.4691 - acc: 0.4059 - val\_loss: 3.7780 - val\_acc: 0.4449
Epoch 3/200
35064/35064 [==============================] - 21s 608us/step - loss: 3.8031 - acc: 0.4303 - val\_loss: 3.0635 - val\_acc: 0.4730
Epoch 4/200
35064/35064 [==============================] - 21s 598us/step - loss: 3.2971 - acc: 0.4391 - val\_loss: 2.7829 - val\_acc: 0.5067
Epoch 5/200
35064/35064 [==============================] - 21s 599us/step - loss: 3.0057 - acc: 0.4698 - val\_loss: 2.6921 - val\_acc: 0.5314
Epoch 6/200
35064/35064 [==============================] - 22s 619us/step - loss: 2.7998 - acc: 0.4908 - val\_loss: 2.5369 - val\_acc: 0.5370
Epoch 7/200
35064/35064 [==============================] - 23s 663us/step - loss: 2.6652 - acc: 0.5056 - val\_loss: 2.3981 - val\_acc: 0.5495
Epoch 8/200
35064/35064 [==============================] - 22s 617us/step - loss: 2.5810 - acc: 0.5138 - val\_loss: 2.3144 - val\_acc: 0.5590
Epoch 9/200
35064/35064 [==============================] - 21s 595us/step - loss: 2.4438 - acc: 0.5250 - val\_loss: 2.2548 - val\_acc: 0.5643
Epoch 10/200
35064/35064 [==============================] - 22s 624us/step - loss: 2.3562 - acc: 0.5334 - val\_loss: 2.1446 - val\_acc: 0.5739
Epoch 11/200
35064/35064 [==============================] - 21s 608us/step - loss: 2.2764 - acc: 0.5419 - val\_loss: 2.1189 - val\_acc: 0.5781
Epoch 12/200
35064/35064 [==============================] - 21s 593us/step - loss: 2.2050 - acc: 0.5480 - val\_loss: 2.0522 - val\_acc: 0.5824
Epoch 13/200
35064/35064 [==============================] - 21s 593us/step - loss: 2.1606 - acc: 0.5508 - val\_loss: 1.9915 - val\_acc: 0.5835
Epoch 14/200
35064/35064 [==============================] - 21s 589us/step - loss: 2.0992 - acc: 0.5511 - val\_loss: 1.9670 - val\_acc: 0.5805
Epoch 15/200
35064/35064 [==============================] - 21s 592us/step - loss: 2.0218 - acc: 0.5540 - val\_loss: 1.8838 - val\_acc: 0.5924
Epoch 16/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.9684 - acc: 0.5658 - val\_loss: 1.8884 - val\_acc: 0.5947
Epoch 17/200
35064/35064 [==============================] - 21s 597us/step - loss: 1.9172 - acc: 0.5681 - val\_loss: 1.8393 - val\_acc: 0.5929
Epoch 18/200
35064/35064 [==============================] - 23s 646us/step - loss: 1.9166 - acc: 0.5678 - val\_loss: 1.8357 - val\_acc: 0.5914
Epoch 19/200
35064/35064 [==============================] - 21s 603us/step - loss: 1.8909 - acc: 0.5709 - val\_loss: 1.8032 - val\_acc: 0.5921
Epoch 20/200
35064/35064 [==============================] - 21s 598us/step - loss: 1.8646 - acc: 0.5657 - val\_loss: 1.8345 - val\_acc: 0.5959
Epoch 21/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.8737 - acc: 0.5699 - val\_loss: 1.7796 - val\_acc: 0.5986
Epoch 22/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.8088 - acc: 0.5755 - val\_loss: 1.7535 - val\_acc: 0.5975
Epoch 23/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.7823 - acc: 0.5759 - val\_loss: 1.7104 - val\_acc: 0.6063
Epoch 24/200
35064/35064 [==============================] - 21s 593us/step - loss: 1.7763 - acc: 0.5802 - val\_loss: 1.7104 - val\_acc: 0.6079
Epoch 25/200
35064/35064 [==============================] - 21s 591us/step - loss: 1.7725 - acc: 0.5801 - val\_loss: 1.7292 - val\_acc: 0.6069
Epoch 26/200
35064/35064 [==============================] - 21s 610us/step - loss: 1.7188 - acc: 0.5912 - val\_loss: 1.6627 - val\_acc: 0.6125
Epoch 27/200
35064/35064 [==============================] - 22s 620us/step - loss: 1.7002 - acc: 0.5921 - val\_loss: 1.6303 - val\_acc: 0.6143
Epoch 28/200
35064/35064 [==============================] - 21s 610us/step - loss: 1.6772 - acc: 0.5931 - val\_loss: 1.6627 - val\_acc: 0.6110
Epoch 29/200
35064/35064 [==============================] - 21s 600us/step - loss: 1.6667 - acc: 0.5947 - val\_loss: 1.6526 - val\_acc: 0.6115
Epoch 30/200
35064/35064 [==============================] - 22s 615us/step - loss: 1.6549 - acc: 0.5953 - val\_loss: 1.6900 - val\_acc: 0.6084
Epoch 31/200
35064/35064 [==============================] - 21s 605us/step - loss: 1.6571 - acc: 0.5998 - val\_loss: 1.6831 - val\_acc: 0.6023
Epoch 32/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.6646 - acc: 0.5971 - val\_loss: 1.6566 - val\_acc: 0.6158
Epoch 33/200
35064/35064 [==============================] - 22s 617us/step - loss: 1.6391 - acc: 0.5987 - val\_loss: 1.6375 - val\_acc: 0.6148
Epoch 34/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.6503 - acc: 0.5978 - val\_loss: 1.6240 - val\_acc: 0.6062
Epoch 35/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.6120 - acc: 0.5996 - val\_loss: 1.6219 - val\_acc: 0.6199
Epoch 36/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.6052 - acc: 0.6052 - val\_loss: 1.6218 - val\_acc: 0.6191
Epoch 37/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.5900 - acc: 0.6083 - val\_loss: 1.5971 - val\_acc: 0.6201
Epoch 38/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.5807 - acc: 0.6091 - val\_loss: 1.5938 - val\_acc: 0.6214
Epoch 39/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.6104 - acc: 0.6049 - val\_loss: 1.6239 - val\_acc: 0.6208
Epoch 40/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.5704 - acc: 0.6117 - val\_loss: 1.5590 - val\_acc: 0.6272
Epoch 41/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.5735 - acc: 0.6112 - val\_loss: 1.5834 - val\_acc: 0.6264
Epoch 42/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.5739 - acc: 0.6124 - val\_loss: 1.6004 - val\_acc: 0.6193
Epoch 43/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.5559 - acc: 0.6164 - val\_loss: 1.5636 - val\_acc: 0.6272
Epoch 44/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.5393 - acc: 0.6137 - val\_loss: 1.5645 - val\_acc: 0.6213
Epoch 45/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.5393 - acc: 0.6147 - val\_loss: 1.5633 - val\_acc: 0.6312
Epoch 46/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.5326 - acc: 0.6170 - val\_loss: 1.5475 - val\_acc: 0.6283
Epoch 47/200
35064/35064 [==============================] - 21s 592us/step - loss: 1.5174 - acc: 0.6193 - val\_loss: 1.5513 - val\_acc: 0.6286
Epoch 48/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5205 - acc: 0.6138 - val\_loss: 1.5909 - val\_acc: 0.6121
Epoch 49/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5379 - acc: 0.6104 - val\_loss: 1.5381 - val\_acc: 0.6036
Epoch 50/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.5047 - acc: 0.6200 - val\_loss: 1.5189 - val\_acc: 0.6234
Epoch 51/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4756 - acc: 0.6208 - val\_loss: 1.5247 - val\_acc: 0.6240
Epoch 52/200
35064/35064 [==============================] - 21s 596us/step - loss: 1.5020 - acc: 0.6194 - val\_loss: 1.5177 - val\_acc: 0.6270
Epoch 53/200
35064/35064 [==============================] - 22s 627us/step - loss: 1.4975 - acc: 0.6211 - val\_loss: 1.5253 - val\_acc: 0.6209
Epoch 54/200
35064/35064 [==============================] - 21s 593us/step - loss: 1.4793 - acc: 0.6255 - val\_loss: 1.5071 - val\_acc: 0.6267
Epoch 55/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4397 - acc: 0.6309 - val\_loss: 1.4947 - val\_acc: 0.6355
Epoch 56/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.4864 - acc: 0.6275 - val\_loss: 1.5446 - val\_acc: 0.6149
Epoch 57/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.4547 - acc: 0.6315 - val\_loss: 1.5182 - val\_acc: 0.6306
Epoch 58/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4322 - acc: 0.6337 - val\_loss: 1.5312 - val\_acc: 0.6329
Epoch 59/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4477 - acc: 0.6344 - val\_loss: 1.5164 - val\_acc: 0.6289
Epoch 60/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4506 - acc: 0.6332 - val\_loss: 1.5307 - val\_acc: 0.6313
Epoch 61/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4487 - acc: 0.6366 - val\_loss: 1.5094 - val\_acc: 0.6263
Epoch 62/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4264 - acc: 0.6385 - val\_loss: 1.5134 - val\_acc: 0.6263
Epoch 63/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.4412 - acc: 0.6340 - val\_loss: 1.5133 - val\_acc: 0.6269
Epoch 64/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4579 - acc: 0.6363 - val\_loss: 1.4988 - val\_acc: 0.6332
Epoch 65/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.4299 - acc: 0.6399 - val\_loss: 1.4955 - val\_acc: 0.6434
Epoch 66/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.4202 - acc: 0.6409 - val\_loss: 1.4959 - val\_acc: 0.6482
Epoch 67/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.4147 - acc: 0.6419 - val\_loss: 1.4856 - val\_acc: 0.6433
Epoch 68/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4293 - acc: 0.6395 - val\_loss: 1.4872 - val\_acc: 0.6342
Epoch 69/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.4251 - acc: 0.6406 - val\_loss: 1.4963 - val\_acc: 0.6224
Epoch 70/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.4208 - acc: 0.6379 - val\_loss: 1.4839 - val\_acc: 0.6429
Epoch 71/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4075 - acc: 0.6401 - val\_loss: 1.4811 - val\_acc: 0.6245
Epoch 72/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.4046 - acc: 0.6430 - val\_loss: 1.4561 - val\_acc: 0.6424
Epoch 73/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.4135 - acc: 0.6371 - val\_loss: 1.4927 - val\_acc: 0.6404
Epoch 74/200
35064/35064 [==============================] - 21s 590us/step - loss: 1.4305 - acc: 0.6341 - val\_loss: 1.5096 - val\_acc: 0.6298
Epoch 75/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.4122 - acc: 0.6411 - val\_loss: 1.4978 - val\_acc: 0.6246
Epoch 76/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.4163 - acc: 0.6412 - val\_loss: 1.4589 - val\_acc: 0.6387
Epoch 77/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.3886 - acc: 0.6425 - val\_loss: 1.4536 - val\_acc: 0.6439
Epoch 78/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3803 - acc: 0.6478 - val\_loss: 1.4500 - val\_acc: 0.6379
Epoch 79/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3783 - acc: 0.6440 - val\_loss: 1.4395 - val\_acc: 0.6343
Epoch 80/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.3612 - acc: 0.6497 - val\_loss: 1.4480 - val\_acc: 0.6423
Epoch 81/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3791 - acc: 0.6479 - val\_loss: 1.4539 - val\_acc: 0.6401
Epoch 82/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3848 - acc: 0.6461 - val\_loss: 1.4526 - val\_acc: 0.6375
Epoch 83/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3663 - acc: 0.6475 - val\_loss: 1.4583 - val\_acc: 0.6457
Epoch 84/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.3704 - acc: 0.6492 - val\_loss: 1.4659 - val\_acc: 0.6346
Epoch 85/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3658 - acc: 0.6484 - val\_loss: 1.4504 - val\_acc: 0.6491
Epoch 86/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3444 - acc: 0.6500 - val\_loss: 1.4223 - val\_acc: 0.6384
Epoch 87/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.3492 - acc: 0.6551 - val\_loss: 1.4263 - val\_acc: 0.6343
Epoch 88/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3456 - acc: 0.6502 - val\_loss: 1.3823 - val\_acc: 0.6450
Epoch 89/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3437 - acc: 0.6544 - val\_loss: 1.4218 - val\_acc: 0.6562
Epoch 90/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3389 - acc: 0.6550 - val\_loss: 1.4320 - val\_acc: 0.6295
Epoch 91/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.3726 - acc: 0.6446 - val\_loss: 1.4305 - val\_acc: 0.6401
Epoch 92/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3683 - acc: 0.6477 - val\_loss: 1.4320 - val\_acc: 0.6312
Epoch 93/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3647 - acc: 0.6472 - val\_loss: 1.4221 - val\_acc: 0.6475
Epoch 94/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.3389 - acc: 0.6541 - val\_loss: 1.4196 - val\_acc: 0.6458
Epoch 95/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3386 - acc: 0.6540 - val\_loss: 1.4336 - val\_acc: 0.6376
Epoch 96/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3319 - acc: 0.6550 - val\_loss: 1.4182 - val\_acc: 0.6362
Epoch 97/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3383 - acc: 0.6533 - val\_loss: 1.3917 - val\_acc: 0.6522
Epoch 98/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.3228 - acc: 0.6567 - val\_loss: 1.4093 - val\_acc: 0.6384
Epoch 99/200
35064/35064 [==============================] - 21s 602us/step - loss: 1.3166 - acc: 0.6572 - val\_loss: 1.3930 - val\_acc: 0.6564
Epoch 100/200
35064/35064 [==============================] - 22s 620us/step - loss: 1.3558 - acc: 0.6528 - val\_loss: 1.4159 - val\_acc: 0.6443
Epoch 101/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.3408 - acc: 0.6537 - val\_loss: 1.4436 - val\_acc: 0.6091
Epoch 102/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.3356 - acc: 0.6532 - val\_loss: 1.4284 - val\_acc: 0.6480
Epoch 103/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3454 - acc: 0.6514 - val\_loss: 1.4010 - val\_acc: 0.6449
Epoch 104/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.3309 - acc: 0.6589 - val\_loss: 1.3866 - val\_acc: 0.6558
Epoch 105/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3207 - acc: 0.6603 - val\_loss: 1.4024 - val\_acc: 0.6480
Epoch 106/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.3015 - acc: 0.6634 - val\_loss: 1.3698 - val\_acc: 0.6408
Epoch 107/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3024 - acc: 0.6622 - val\_loss: 1.3935 - val\_acc: 0.6474
Epoch 108/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.2816 - acc: 0.6663 - val\_loss: 1.3536 - val\_acc: 0.6534
Epoch 109/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.3200 - acc: 0.6547 - val\_loss: 1.3613 - val\_acc: 0.6526
Epoch 110/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2874 - acc: 0.6629 - val\_loss: 1.3772 - val\_acc: 0.6435
Epoch 111/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2885 - acc: 0.6660 - val\_loss: 1.3789 - val\_acc: 0.6614
Epoch 112/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2888 - acc: 0.6635 - val\_loss: 1.3768 - val\_acc: 0.6360
Epoch 113/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3129 - acc: 0.6589 - val\_loss: 1.3811 - val\_acc: 0.6360
Epoch 114/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.3094 - acc: 0.6607 - val\_loss: 1.3749 - val\_acc: 0.6405
Epoch 115/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2852 - acc: 0.6639 - val\_loss: 1.3373 - val\_acc: 0.6567
Epoch 116/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2966 - acc: 0.6598 - val\_loss: 1.3333 - val\_acc: 0.6522
Epoch 117/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2747 - acc: 0.6674 - val\_loss: 1.3440 - val\_acc: 0.6533
Epoch 118/200
35064/35064 [==============================] - 21s 592us/step - loss: 1.2728 - acc: 0.6643 - val\_loss: 1.3260 - val\_acc: 0.6521
Epoch 119/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2697 - acc: 0.6660 - val\_loss: 1.3339 - val\_acc: 0.6555
Epoch 120/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2635 - acc: 0.6685 - val\_loss: 1.3190 - val\_acc: 0.6629
Epoch 121/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2557 - acc: 0.6705 - val\_loss: 1.3354 - val\_acc: 0.6563
Epoch 122/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2722 - acc: 0.6695 - val\_loss: 1.3751 - val\_acc: 0.6409
Epoch 123/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2674 - acc: 0.6703 - val\_loss: 1.3843 - val\_acc: 0.6403
Epoch 124/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.2611 - acc: 0.6647 - val\_loss: 1.3425 - val\_acc: 0.6514
Epoch 125/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.2462 - acc: 0.6732 - val\_loss: 1.3366 - val\_acc: 0.6524
Epoch 126/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2646 - acc: 0.6693 - val\_loss: 1.3277 - val\_acc: 0.6559
Epoch 127/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2475 - acc: 0.6700 - val\_loss: 1.3383 - val\_acc: 0.6669
Epoch 128/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2492 - acc: 0.6708 - val\_loss: 1.2999 - val\_acc: 0.6647
Epoch 129/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2432 - acc: 0.6725 - val\_loss: 1.3115 - val\_acc: 0.6590
Epoch 130/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2502 - acc: 0.6719 - val\_loss: 1.2951 - val\_acc: 0.6711
Epoch 131/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2629 - acc: 0.6708 - val\_loss: 1.3761 - val\_acc: 0.6477
Epoch 132/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.2922 - acc: 0.6640 - val\_loss: 1.3331 - val\_acc: 0.6586
Epoch 133/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2544 - acc: 0.6724 - val\_loss: 1.3122 - val\_acc: 0.6683
Epoch 134/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2529 - acc: 0.6751 - val\_loss: 1.3538 - val\_acc: 0.6467
Epoch 135/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2526 - acc: 0.6703 - val\_loss: 1.3255 - val\_acc: 0.6481
Epoch 136/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2426 - acc: 0.6735 - val\_loss: 1.3098 - val\_acc: 0.6668
Epoch 137/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2232 - acc: 0.6753 - val\_loss: 1.3140 - val\_acc: 0.6566
Epoch 138/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2472 - acc: 0.6681 - val\_loss: 1.3050 - val\_acc: 0.6623
Epoch 139/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2487 - acc: 0.6712 - val\_loss: 1.3030 - val\_acc: 0.6711
Epoch 140/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.2570 - acc: 0.6710 - val\_loss: 1.3115 - val\_acc: 0.6574
Epoch 141/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.2508 - acc: 0.6728 - val\_loss: 1.3125 - val\_acc: 0.6517
Epoch 142/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2340 - acc: 0.6753 - val\_loss: 1.2989 - val\_acc: 0.6601
Epoch 143/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2539 - acc: 0.6737 - val\_loss: 1.2997 - val\_acc: 0.6639
Epoch 144/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2420 - acc: 0.6735 - val\_loss: 1.2845 - val\_acc: 0.6643
Epoch 145/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2321 - acc: 0.6731 - val\_loss: 1.3238 - val\_acc: 0.6481
Epoch 146/200
35064/35064 [==============================] - 21s 611us/step - loss: 1.2410 - acc: 0.6730 - val\_loss: 1.3121 - val\_acc: 0.6614
Epoch 147/200
35064/35064 [==============================] - 21s 612us/step - loss: 1.2320 - acc: 0.6761 - val\_loss: 1.3207 - val\_acc: 0.6530
Epoch 148/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2263 - acc: 0.6766 - val\_loss: 1.3161 - val\_acc: 0.6610
Epoch 149/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.2350 - acc: 0.6744 - val\_loss: 1.3185 - val\_acc: 0.6619
Epoch 150/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2180 - acc: 0.6819 - val\_loss: 1.2839 - val\_acc: 0.6702
Epoch 151/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2199 - acc: 0.6790 - val\_loss: 1.2826 - val\_acc: 0.6652
Epoch 152/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2069 - acc: 0.6819 - val\_loss: 1.2880 - val\_acc: 0.6729
Epoch 153/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.2255 - acc: 0.6751 - val\_loss: 1.3331 - val\_acc: 0.6522
Epoch 154/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2468 - acc: 0.6717 - val\_loss: 1.3234 - val\_acc: 0.6622
Epoch 155/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2286 - acc: 0.6747 - val\_loss: 1.2975 - val\_acc: 0.6671
Epoch 156/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.2196 - acc: 0.6797 - val\_loss: 1.2722 - val\_acc: 0.6655
Epoch 157/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2021 - acc: 0.6797 - val\_loss: 1.2778 - val\_acc: 0.6610
Epoch 158/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1996 - acc: 0.6829 - val\_loss: 1.2858 - val\_acc: 0.6663
Epoch 159/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2112 - acc: 0.6787 - val\_loss: 1.2689 - val\_acc: 0.6726
Epoch 160/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1962 - acc: 0.6846 - val\_loss: 1.2780 - val\_acc: 0.6661
Epoch 161/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1936 - acc: 0.6848 - val\_loss: 1.2630 - val\_acc: 0.6638
Epoch 162/200
35064/35064 [==============================] - 21s 589us/step - loss: 1.1848 - acc: 0.6841 - val\_loss: 1.2670 - val\_acc: 0.6629
Epoch 163/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2080 - acc: 0.6805 - val\_loss: 1.2746 - val\_acc: 0.6700
Epoch 164/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2415 - acc: 0.6718 - val\_loss: 1.3194 - val\_acc: 0.6534
Epoch 165/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2324 - acc: 0.6737 - val\_loss: 1.2583 - val\_acc: 0.6645
Epoch 166/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1903 - acc: 0.6848 - val\_loss: 1.2345 - val\_acc: 0.6740
Epoch 167/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1976 - acc: 0.6824 - val\_loss: 1.2680 - val\_acc: 0.6796
Epoch 168/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.2151 - acc: 0.6806 - val\_loss: 1.2859 - val\_acc: 0.6677
Epoch 169/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.2032 - acc: 0.6829 - val\_loss: 1.2751 - val\_acc: 0.6653
Epoch 170/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1867 - acc: 0.6851 - val\_loss: 1.2460 - val\_acc: 0.6809
Epoch 171/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1866 - acc: 0.6877 - val\_loss: 1.2373 - val\_acc: 0.6752
Epoch 172/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1813 - acc: 0.6865 - val\_loss: 1.2611 - val\_acc: 0.6713
Epoch 173/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1608 - acc: 0.6913 - val\_loss: 1.2390 - val\_acc: 0.6759
Epoch 174/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1651 - acc: 0.6865 - val\_loss: 1.2713 - val\_acc: 0.6686
Epoch 175/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1827 - acc: 0.6898 - val\_loss: 1.2623 - val\_acc: 0.6742
Epoch 176/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1655 - acc: 0.6892 - val\_loss: 1.2531 - val\_acc: 0.6664
Epoch 177/200
35064/35064 [==============================] - 21s 588us/step - loss: 1.1674 - acc: 0.6892 - val\_loss: 1.2487 - val\_acc: 0.6685
Epoch 178/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1717 - acc: 0.6866 - val\_loss: 1.2490 - val\_acc: 0.6677
Epoch 179/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1533 - acc: 0.6919 - val\_loss: 1.2591 - val\_acc: 0.6501
Epoch 180/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1669 - acc: 0.6888 - val\_loss: 1.2261 - val\_acc: 0.6838
Epoch 181/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1605 - acc: 0.6887 - val\_loss: 1.2244 - val\_acc: 0.6797
Epoch 182/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1522 - acc: 0.6909 - val\_loss: 1.2490 - val\_acc: 0.6765
Epoch 183/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1498 - acc: 0.6938 - val\_loss: 1.2484 - val\_acc: 0.6697
Epoch 184/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1543 - acc: 0.6930 - val\_loss: 1.2377 - val\_acc: 0.6748
Epoch 185/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1526 - acc: 0.6946 - val\_loss: 1.2372 - val\_acc: 0.6773
Epoch 186/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1778 - acc: 0.6857 - val\_loss: 1.2728 - val\_acc: 0.6541
Epoch 187/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1749 - acc: 0.6872 - val\_loss: 1.2555 - val\_acc: 0.6726
Epoch 188/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1479 - acc: 0.6933 - val\_loss: 1.2359 - val\_acc: 0.6697
Epoch 189/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1420 - acc: 0.6928 - val\_loss: 1.2423 - val\_acc: 0.6773
Epoch 190/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1443 - acc: 0.6949 - val\_loss: 1.2432 - val\_acc: 0.6814
Epoch 191/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1526 - acc: 0.6938 - val\_loss: 1.2465 - val\_acc: 0.6728
Epoch 192/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1440 - acc: 0.6911 - val\_loss: 1.2556 - val\_acc: 0.6587
Epoch 193/200
35064/35064 [==============================] - 22s 615us/step - loss: 1.1521 - acc: 0.6905 - val\_loss: 1.2640 - val\_acc: 0.6753
Epoch 194/200
35064/35064 [==============================] - 21s 608us/step - loss: 1.1593 - acc: 0.6939 - val\_loss: 1.2676 - val\_acc: 0.6701
Epoch 195/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1661 - acc: 0.6905 - val\_loss: 1.2395 - val\_acc: 0.6755
Epoch 196/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1642 - acc: 0.6890 - val\_loss: 1.2413 - val\_acc: 0.6804
Epoch 197/200
35064/35064 [==============================] - 21s 587us/step - loss: 1.1592 - acc: 0.6899 - val\_loss: 1.2338 - val\_acc: 0.6849
Epoch 198/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1357 - acc: 0.6933 - val\_loss: 1.2493 - val\_acc: 0.6645
Epoch 199/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1354 - acc: 0.6950 - val\_loss: 1.2200 - val\_acc: 0.6794
Epoch 200/200
35064/35064 [==============================] - 21s 586us/step - loss: 1.1401 - acc: 0.6966 - val\_loss: 1.2218 - val\_acc: 0.6785
8766/8766 [==============================] - 3s 294us/step
Test score:  1.2217534959737824
Test accuracy:  0.6785306884985168

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{p}{,} \PY{n}{history}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{sequential}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_4 (InputLayer)            (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_5 (InputLayer)            (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_6 (InputLayer)            (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_1 (Model)                 (None, 280)          1525        input\_4[0][0]                    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_2 (Model)                 (None, 100)          1125        input\_5[0][0]                    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_3 (Model)                 (None, 100)          1175        input\_6[0][0]                    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_1 (Concatenate)     (None, 480)          0           model\_1[1][0]                    
                                                                 model\_2[1][0]                    
                                                                 model\_3[1][0]                    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_1 (Dense)                 (None, 500)          240500      concatenate\_1[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_1 (Dropout)             (None, 500)          0           dense\_1[0][0]                    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_2 (Dense)                 (None, 500)          250500      dropout\_1[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_3 (Dense)                 (None, 5)            2505        dense\_2[0][0]                    
==================================================================================================
Total params: 497,330
Trainable params: 497,330
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.6785306867442391
kappa:  0.5332394493407178

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Same model with same dropout but with varying optimizer leraning rate.
here the preformance after 200 epochs - 5e-04: model does not train -
1e-04 (same as above) - accurancy: 0.68 - kappa: 0.54 - 1e-05 -
accurancy: 0.61 - kappa: 0.44

\begin{itemize}
\tightlist
\item
  1e-06

  \begin{itemize}
  \tightlist
  \item
    accurancy: 0.45
  \item
    kappa: 0.21
  \end{itemize}
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{}avec dropout with adam steps in [0.0005, 0.0001, 0.00001, 0.000001]}
         
         \PY{n}{tensor\PYZus{}board} \PY{o}{=} \PY{n}{TensorBoard}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./logs/dropout\PYZus{}steps}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{step} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mf}{0.0005}\PY{p}{,} \PY{l+m+mf}{0.0001}\PY{p}{,} \PY{l+m+mf}{0.00001}\PY{p}{,} \PY{l+m+mf}{0.000001}\PY{p}{]}\PY{p}{:}
             \PY{n}{model\PYZus{}eeg} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{model\PYZus{}pulse} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{model\PYZus{}acc} \PY{o}{=} \PY{n}{submodel}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{kernel2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                                  \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool2}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         
             \PY{n}{in\PYZus{}eeg} \PY{o}{=}  \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
             \PY{n}{in\PYZus{}pulse} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
             \PY{n}{in\PYZus{}acc} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{accelerometer\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         
             \PY{n}{out\PYZus{}eeg} \PY{o}{=} \PY{n}{model\PYZus{}eeg}\PY{p}{(}\PY{n}{in\PYZus{}eeg}\PY{p}{)}
             \PY{n}{out\PYZus{}pulse} \PY{o}{=} \PY{n}{model\PYZus{}pulse}\PY{p}{(}\PY{n}{in\PYZus{}pulse}\PY{p}{)}
             \PY{n}{out\PYZus{}acc} \PY{o}{=} \PY{n}{model\PYZus{}acc}\PY{p}{(}\PY{n}{in\PYZus{}acc}\PY{p}{)}
         
             \PY{n}{concatenated} \PY{o}{=} \PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{out\PYZus{}eeg}\PY{p}{,} \PY{n}{out\PYZus{}pulse}\PY{p}{,} \PY{n}{out\PYZus{}acc}\PY{p}{]}\PY{p}{)}
             \PY{n}{dense1} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{concatenated}\PY{p}{)}
             \PY{n}{drop1} \PY{o}{=} \PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{(}\PY{n}{dense1}\PY{p}{)}
             \PY{n}{dense2} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{drop1}\PY{p}{)}
             \PY{n}{out} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense2}\PY{p}{)}
         
             \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{p}{[}\PY{n}{in\PYZus{}eeg}\PY{p}{,} \PY{n}{in\PYZus{}pulse}\PY{p}{,} \PY{n}{in\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{out}\PY{p}{)}
         
         
             \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adam}\PY{p}{(}\PY{n}{step}\PY{p}{)}
         
             \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                           \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,}
                           \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
             \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{200}
             \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{epochs}\PY{o}{=} \PY{n}{epochs}\PY{p}{,} 
                                 \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{tensor\PYZus{}board}\PY{p}{]}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{results for adam }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{step}\PY{p}{)}
             \PY{n}{modelPerf}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{p}{,} \PY{n}{history}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{sequential}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 38s 1ms/step - loss: 9.7170 - acc: 0.3911 - val\_loss: 9.5523 - val\_acc: 0.4066
Epoch 2/200
35064/35064 [==============================] - 35s 1ms/step - loss: 9.5601 - acc: 0.4060 - val\_loss: 9.5339 - val\_acc: 0.4081
Epoch 3/200
35064/35064 [==============================] - 35s 991us/step - loss: 9.3555 - acc: 0.4190 - val\_loss: 9.4174 - val\_acc: 0.4154
Epoch 4/200
35064/35064 [==============================] - 35s 1ms/step - loss: 9.9101 - acc: 0.3847 - val\_loss: 10.0460 - val\_acc: 0.3760
Epoch 5/200
35064/35064 [==============================] - 34s 983us/step - loss: 10.1273 - acc: 0.3712 - val\_loss: 10.0206 - val\_acc: 0.3777
Epoch 6/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.0962 - acc: 0.3729 - val\_loss: 10.0125 - val\_acc: 0.3781
Epoch 7/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.1131 - acc: 0.3717 - val\_loss: 9.9732 - val\_acc: 0.3807
Epoch 8/200
35064/35064 [==============================] - 36s 1ms/step - loss: 9.3225 - acc: 0.4210 - val\_loss: 8.9437 - val\_acc: 0.4449
Epoch 9/200
35064/35064 [==============================] - 37s 1ms/step - loss: 9.3581 - acc: 0.4193 - val\_loss: 8.8856 - val\_acc: 0.4487
Epoch 10/200
35064/35064 [==============================] - 35s 992us/step - loss: 9.1994 - acc: 0.4291 - val\_loss: 9.9750 - val\_acc: 0.3811
Epoch 11/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.1233 - acc: 0.3717 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 12/200
35064/35064 [==============================] - 34s 977us/step - loss: 10.1942 - acc: 0.3675 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 13/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2070 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 14/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2053 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 15/200
35064/35064 [==============================] - 35s 998us/step - loss: 10.1250 - acc: 0.3717 - val\_loss: 10.0551 - val\_acc: 0.3761
Epoch 16/200
35064/35064 [==============================] - 37s 1ms/step - loss: 10.1744 - acc: 0.3687 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 17/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2057 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 18/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2048 - acc: 0.3669 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 19/200
35064/35064 [==============================] - 37s 1ms/step - loss: 10.2057 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 20/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2062 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 21/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 22/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 23/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2067 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 24/200
35064/35064 [==============================] - 35s 998us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 25/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 26/200
35064/35064 [==============================] - 37s 1ms/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 27/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 28/200
35064/35064 [==============================] - 35s 995us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 29/200
35064/35064 [==============================] - 35s 990us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 30/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 31/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2067 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 32/200
35064/35064 [==============================] - 35s 999us/step - loss: 10.2062 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 33/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 34/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2067 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 35/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 36/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2057 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 37/200
35064/35064 [==============================] - 34s 963us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 38/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2085 - acc: 0.3666 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 39/200
35064/35064 [==============================] - 34s 968us/step - loss: 10.2067 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 40/200
35064/35064 [==============================] - 35s 992us/step - loss: 10.2067 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 41/200
35064/35064 [==============================] - 34s 978us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 42/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2067 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 43/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 44/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2053 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 45/200
35064/35064 [==============================] - 34s 977us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 46/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 47/200
35064/35064 [==============================] - 34s 982us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 48/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 49/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2057 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 50/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 51/200
35064/35064 [==============================] - 33s 949us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 52/200
35064/35064 [==============================] - 35s 990us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 53/200
35064/35064 [==============================] - 34s 959us/step - loss: 10.2067 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 54/200
35064/35064 [==============================] - 35s 999us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 55/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2062 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 56/200
35064/35064 [==============================] - 34s 971us/step - loss: 10.2057 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 57/200
35064/35064 [==============================] - 35s 1ms/step - loss: 10.2090 - acc: 0.3666 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 58/200
35064/35064 [==============================] - 33s 939us/step - loss: 10.2062 - acc: 0.3668 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 59/200
35064/35064 [==============================] - 33s 941us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 60/200
35064/35064 [==============================] - 34s 958us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 61/200
35064/35064 [==============================] - 33s 951us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0835 - val\_acc: 0.3744
Epoch 62/200
35064/35064 [==============================] - 33s 955us/step - loss: 10.1715 - acc: 0.3689 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 63/200
35064/35064 [==============================] - 33s 950us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 64/200
35064/35064 [==============================] - 33s 952us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 65/200
35064/35064 [==============================] - 34s 958us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 66/200
35064/35064 [==============================] - 33s 951us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 67/200
35064/35064 [==============================] - 33s 946us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 68/200
35064/35064 [==============================] - 33s 951us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 69/200
35064/35064 [==============================] - 33s 939us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 70/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 71/200
35064/35064 [==============================] - 34s 965us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 72/200
35064/35064 [==============================] - 34s 970us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 73/200
35064/35064 [==============================] - 33s 945us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 74/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 75/200
35064/35064 [==============================] - 33s 949us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 76/200
35064/35064 [==============================] - 33s 950us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 77/200
35064/35064 [==============================] - 34s 956us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 78/200
35064/35064 [==============================] - 34s 962us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 79/200
35064/35064 [==============================] - 33s 944us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 80/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 81/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 82/200
35064/35064 [==============================] - 33s 945us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 83/200
35064/35064 [==============================] - 34s 962us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 84/200
35064/35064 [==============================] - 33s 948us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 85/200
35064/35064 [==============================] - 33s 949us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 86/200
35064/35064 [==============================] - 33s 947us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 87/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 88/200
35064/35064 [==============================] - 34s 967us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 89/200
35064/35064 [==============================] - 36s 1ms/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 90/200
35064/35064 [==============================] - 38s 1ms/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 91/200
35064/35064 [==============================] - 35s 997us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 92/200
35064/35064 [==============================] - 34s 958us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 93/200
35064/35064 [==============================] - 34s 969us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 94/200
35064/35064 [==============================] - 34s 966us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 95/200
35064/35064 [==============================] - 34s 968us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 96/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 97/200
35064/35064 [==============================] - 34s 961us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 98/200
35064/35064 [==============================] - 34s 959us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 99/200
35064/35064 [==============================] - 34s 964us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 100/200
35064/35064 [==============================] - 34s 980us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 101/200
35064/35064 [==============================] - 34s 964us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 102/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 103/200
35064/35064 [==============================] - 33s 947us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 104/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2067 - acc: 0.3668 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 105/200
35064/35064 [==============================] - 33s 950us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 106/200
35064/35064 [==============================] - 33s 951us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 107/200
35064/35064 [==============================] - 34s 971us/step - loss: 10.2085 - acc: 0.3666 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 108/200
35064/35064 [==============================] - 34s 968us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 109/200
35064/35064 [==============================] - 33s 945us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 110/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 111/200
35064/35064 [==============================] - 34s 964us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 112/200
35064/35064 [==============================] - 34s 964us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 113/200
35064/35064 [==============================] - 34s 963us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 114/200
35064/35064 [==============================] - 34s 956us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 115/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 116/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 117/200
35064/35064 [==============================] - 33s 941us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 118/200
35064/35064 [==============================] - 33s 955us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 119/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 120/200
35064/35064 [==============================] - 33s 949us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 121/200
35064/35064 [==============================] - 33s 948us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 122/200
35064/35064 [==============================] - 33s 950us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 123/200
35064/35064 [==============================] - 33s 953us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 124/200
35064/35064 [==============================] - 33s 935us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 125/200
35064/35064 [==============================] - 34s 957us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 126/200
35064/35064 [==============================] - 33s 936us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 127/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 128/200
35064/35064 [==============================] - 33s 948us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 129/200
35064/35064 [==============================] - 34s 982us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 130/200
35064/35064 [==============================] - 34s 959us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 131/200
35064/35064 [==============================] - 33s 941us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 132/200
35064/35064 [==============================] - 33s 947us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 133/200
35064/35064 [==============================] - 33s 948us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 134/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 135/200
35064/35064 [==============================] - 33s 952us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 136/200
35064/35064 [==============================] - 34s 961us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 137/200
35064/35064 [==============================] - 33s 953us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 138/200
35064/35064 [==============================] - 33s 942us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 139/200
35064/35064 [==============================] - 34s 961us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 140/200
35064/35064 [==============================] - 34s 958us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 141/200
35064/35064 [==============================] - 33s 953us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 142/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 143/200
35064/35064 [==============================] - 33s 935us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 144/200
35064/35064 [==============================] - 33s 940us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 145/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 146/200
35064/35064 [==============================] - 33s 944us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 147/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 148/200
35064/35064 [==============================] - 34s 957us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 149/200
35064/35064 [==============================] - 33s 955us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 150/200
35064/35064 [==============================] - 33s 946us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 151/200
35064/35064 [==============================] - 33s 944us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 152/200
35064/35064 [==============================] - 33s 951us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 153/200
35064/35064 [==============================] - 33s 954us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 154/200
35064/35064 [==============================] - 33s 943us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 155/200
35064/35064 [==============================] - 34s 957us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 156/200
35064/35064 [==============================] - 33s 952us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 157/200
35064/35064 [==============================] - 34s 955us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 158/200
35064/35064 [==============================] - 34s 977us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 159/200
35064/35064 [==============================] - 33s 955us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 160/200
35064/35064 [==============================] - 34s 960us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 161/200
35064/35064 [==============================] - 34s 963us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 162/200
35064/35064 [==============================] - 33s 948us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 163/200
35064/35064 [==============================] - 33s 948us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 164/200
35064/35064 [==============================] - 33s 941us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 165/200
35064/35064 [==============================] - 34s 956us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 166/200
35064/35064 [==============================] - 33s 941us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 167/200
35064/35064 [==============================] - 33s 950us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 168/200
35064/35064 [==============================] - 33s 945us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 169/200
35064/35064 [==============================] - 34s 957us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 170/200
35064/35064 [==============================] - 34s 956us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 171/200
35064/35064 [==============================] - 33s 952us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 172/200
35064/35064 [==============================] - 33s 949us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 173/200
35064/35064 [==============================] - 33s 946us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 174/200
35064/35064 [==============================] - 33s 951us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 175/200
35064/35064 [==============================] - 33s 951us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 176/200
35064/35064 [==============================] - 33s 945us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 177/200
35064/35064 [==============================] - 33s 955us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 178/200
35064/35064 [==============================] - 33s 953us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 179/200
35064/35064 [==============================] - 33s 950us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 180/200
35064/35064 [==============================] - 34s 958us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 181/200
35064/35064 [==============================] - 33s 950us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 182/200
35064/35064 [==============================] - 33s 949us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 183/200
35064/35064 [==============================] - 33s 941us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 184/200
35064/35064 [==============================] - 33s 953us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 185/200
35064/35064 [==============================] - 34s 957us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 186/200
35064/35064 [==============================] - 34s 964us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 187/200
35064/35064 [==============================] - 35s 988us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 188/200
35064/35064 [==============================] - 33s 953us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 189/200
35064/35064 [==============================] - 33s 942us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 190/200
35064/35064 [==============================] - 33s 932us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 191/200
35064/35064 [==============================] - 33s 952us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 192/200
35064/35064 [==============================] - 33s 940us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 193/200
35064/35064 [==============================] - 34s 957us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 194/200
35064/35064 [==============================] - 34s 966us/step - loss: 10.2071 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 195/200
35064/35064 [==============================] - 33s 949us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 196/200
35064/35064 [==============================] - 33s 936us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 197/200
35064/35064 [==============================] - 33s 951us/step - loss: 10.2080 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 198/200
35064/35064 [==============================] - 34s 958us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 199/200
35064/35064 [==============================] - 33s 945us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
Epoch 200/200
35064/35064 [==============================] - 33s 948us/step - loss: 10.2076 - acc: 0.3667 - val\_loss: 10.0853 - val\_acc: 0.3743
results for adam  0.0005
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_22 (InputLayer)           (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_23 (InputLayer)           (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_24 (InputLayer)           (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_13 (Model)                (None, 280)          1525        input\_22[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_14 (Model)                (None, 100)          1125        input\_23[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_15 (Model)                (None, 100)          1175        input\_24[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_4 (Concatenate)     (None, 480)          0           model\_13[1][0]                   
                                                                 model\_14[1][0]                   
                                                                 model\_15[1][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_10 (Dense)                (None, 500)          240500      concatenate\_4[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_4 (Dropout)             (None, 500)          0           dense\_10[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_11 (Dense)                (None, 500)          250500      dropout\_4[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_12 (Dense)                (None, 5)            2505        dense\_11[0][0]                   
==================================================================================================
Total params: 497,330
Trainable params: 497,330
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.37428701802418435
kappa:  0.0

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 45s 1ms/step - loss: 7.2970 - acc: 0.3948 - val\_loss: 4.6579 - val\_acc: 0.4746
Epoch 2/200
35064/35064 [==============================] - 45s 1ms/step - loss: 4.6048 - acc: 0.4332 - val\_loss: 3.3371 - val\_acc: 0.4912
Epoch 3/200
35064/35064 [==============================] - 44s 1ms/step - loss: 3.6250 - acc: 0.4578 - val\_loss: 2.9149 - val\_acc: 0.5170
Epoch 4/200
35064/35064 [==============================] - 44s 1ms/step - loss: 3.1580 - acc: 0.4796 - val\_loss: 2.6833 - val\_acc: 0.5229
Epoch 5/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.9444 - acc: 0.4872 - val\_loss: 2.5693 - val\_acc: 0.5532
Epoch 6/200
35064/35064 [==============================] - 46s 1ms/step - loss: 2.7515 - acc: 0.5091 - val\_loss: 2.4511 - val\_acc: 0.5461
Epoch 7/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.5838 - acc: 0.5257 - val\_loss: 2.3259 - val\_acc: 0.5667
Epoch 8/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.4787 - acc: 0.5312 - val\_loss: 2.2809 - val\_acc: 0.5784
Epoch 9/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.3818 - acc: 0.5400 - val\_loss: 2.1504 - val\_acc: 0.5803
Epoch 10/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.3370 - acc: 0.5415 - val\_loss: 2.1736 - val\_acc: 0.5830
Epoch 11/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.2629 - acc: 0.5511 - val\_loss: 2.1639 - val\_acc: 0.5848
Epoch 12/200
35064/35064 [==============================] - 46s 1ms/step - loss: 2.1898 - acc: 0.5599 - val\_loss: 2.0769 - val\_acc: 0.5849
Epoch 13/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.1303 - acc: 0.5630 - val\_loss: 1.9856 - val\_acc: 0.5867
Epoch 14/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.0539 - acc: 0.5709 - val\_loss: 1.9236 - val\_acc: 0.5956
Epoch 15/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.0847 - acc: 0.5674 - val\_loss: 2.0775 - val\_acc: 0.5877
Epoch 16/200
35064/35064 [==============================] - 45s 1ms/step - loss: 2.0236 - acc: 0.5746 - val\_loss: 1.9085 - val\_acc: 0.6031
Epoch 17/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.9439 - acc: 0.5766 - val\_loss: 1.8542 - val\_acc: 0.6064
Epoch 18/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.9135 - acc: 0.5820 - val\_loss: 1.8145 - val\_acc: 0.6076
Epoch 19/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.8907 - acc: 0.5814 - val\_loss: 1.8220 - val\_acc: 0.6056
Epoch 20/200
35064/35064 [==============================] - 47s 1ms/step - loss: 1.8878 - acc: 0.5830 - val\_loss: 1.8333 - val\_acc: 0.6063
Epoch 21/200
35064/35064 [==============================] - 48s 1ms/step - loss: 1.8087 - acc: 0.5899 - val\_loss: 1.7557 - val\_acc: 0.6123
Epoch 22/200
35064/35064 [==============================] - 47s 1ms/step - loss: 1.7994 - acc: 0.5877 - val\_loss: 1.7535 - val\_acc: 0.6142
Epoch 23/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.8016 - acc: 0.5944 - val\_loss: 1.7731 - val\_acc: 0.6115
Epoch 24/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.7483 - acc: 0.5967 - val\_loss: 1.7397 - val\_acc: 0.6190
Epoch 25/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.7262 - acc: 0.5990 - val\_loss: 1.7095 - val\_acc: 0.6263
Epoch 26/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.6964 - acc: 0.6025 - val\_loss: 1.7004 - val\_acc: 0.6193
Epoch 27/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.6808 - acc: 0.6018 - val\_loss: 1.7103 - val\_acc: 0.6238
Epoch 28/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.6613 - acc: 0.6040 - val\_loss: 1.6731 - val\_acc: 0.6279
Epoch 29/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.6526 - acc: 0.6039 - val\_loss: 1.6420 - val\_acc: 0.6303
Epoch 30/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.6254 - acc: 0.6064 - val\_loss: 1.6585 - val\_acc: 0.6251
Epoch 31/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.6175 - acc: 0.6086 - val\_loss: 1.6273 - val\_acc: 0.6278
Epoch 32/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5891 - acc: 0.6122 - val\_loss: 1.6321 - val\_acc: 0.6275
Epoch 33/200
35064/35064 [==============================] - 46s 1ms/step - loss: 1.5661 - acc: 0.6131 - val\_loss: 1.6037 - val\_acc: 0.6238
Epoch 34/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5670 - acc: 0.6146 - val\_loss: 1.6019 - val\_acc: 0.6225
Epoch 35/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5785 - acc: 0.6120 - val\_loss: 1.6079 - val\_acc: 0.6261
Epoch 36/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5476 - acc: 0.6166 - val\_loss: 1.5935 - val\_acc: 0.6331
Epoch 37/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5363 - acc: 0.6169 - val\_loss: 1.5647 - val\_acc: 0.6291
Epoch 38/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5309 - acc: 0.6176 - val\_loss: 1.5949 - val\_acc: 0.6176
Epoch 39/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5588 - acc: 0.6116 - val\_loss: 1.6054 - val\_acc: 0.6263
Epoch 40/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5353 - acc: 0.6179 - val\_loss: 1.5521 - val\_acc: 0.6299
Epoch 41/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5161 - acc: 0.6175 - val\_loss: 1.5491 - val\_acc: 0.6254
Epoch 42/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5018 - acc: 0.6216 - val\_loss: 1.5088 - val\_acc: 0.6321
Epoch 43/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5137 - acc: 0.6241 - val\_loss: 1.5496 - val\_acc: 0.6210
Epoch 44/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5086 - acc: 0.6192 - val\_loss: 1.5485 - val\_acc: 0.6271
Epoch 45/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.5132 - acc: 0.6187 - val\_loss: 1.5401 - val\_acc: 0.6321
Epoch 46/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4809 - acc: 0.6278 - val\_loss: 1.5273 - val\_acc: 0.6331
Epoch 47/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4551 - acc: 0.6311 - val\_loss: 1.5093 - val\_acc: 0.6361
Epoch 48/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4586 - acc: 0.6273 - val\_loss: 1.5110 - val\_acc: 0.6311
Epoch 49/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4608 - acc: 0.6241 - val\_loss: 1.5351 - val\_acc: 0.6331
Epoch 50/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4483 - acc: 0.6302 - val\_loss: 1.4836 - val\_acc: 0.6384
Epoch 51/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4170 - acc: 0.6354 - val\_loss: 1.4994 - val\_acc: 0.6360
Epoch 52/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4226 - acc: 0.6362 - val\_loss: 1.4842 - val\_acc: 0.6419
Epoch 53/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4189 - acc: 0.6352 - val\_loss: 1.5015 - val\_acc: 0.6354
Epoch 54/200
35064/35064 [==============================] - 46s 1ms/step - loss: 1.4395 - acc: 0.6358 - val\_loss: 1.4913 - val\_acc: 0.6378
Epoch 55/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3955 - acc: 0.6411 - val\_loss: 1.4971 - val\_acc: 0.6417
Epoch 56/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3892 - acc: 0.6405 - val\_loss: 1.4497 - val\_acc: 0.6391
Epoch 57/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3979 - acc: 0.6421 - val\_loss: 1.4353 - val\_acc: 0.6470
Epoch 58/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3935 - acc: 0.6406 - val\_loss: 1.4394 - val\_acc: 0.6394
Epoch 59/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3744 - acc: 0.6475 - val\_loss: 1.4477 - val\_acc: 0.6396
Epoch 60/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.4151 - acc: 0.6322 - val\_loss: 1.4476 - val\_acc: 0.6358
Epoch 61/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3866 - acc: 0.6421 - val\_loss: 1.4322 - val\_acc: 0.6449
Epoch 62/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3714 - acc: 0.6442 - val\_loss: 1.4604 - val\_acc: 0.6339
Epoch 63/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.3850 - acc: 0.6428 - val\_loss: 1.4752 - val\_acc: 0.6385
Epoch 64/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3946 - acc: 0.6396 - val\_loss: 1.4145 - val\_acc: 0.6353
Epoch 65/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3731 - acc: 0.6434 - val\_loss: 1.4244 - val\_acc: 0.6392
Epoch 66/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3707 - acc: 0.6410 - val\_loss: 1.4545 - val\_acc: 0.6305
Epoch 67/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3529 - acc: 0.6469 - val\_loss: 1.4217 - val\_acc: 0.6460
Epoch 68/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3805 - acc: 0.6412 - val\_loss: 1.4310 - val\_acc: 0.6395
Epoch 69/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3889 - acc: 0.6420 - val\_loss: 1.4388 - val\_acc: 0.6402
Epoch 70/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3483 - acc: 0.6475 - val\_loss: 1.4231 - val\_acc: 0.6428
Epoch 71/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3431 - acc: 0.6483 - val\_loss: 1.3989 - val\_acc: 0.6386
Epoch 72/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3352 - acc: 0.6499 - val\_loss: 1.3987 - val\_acc: 0.6429
Epoch 73/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3260 - acc: 0.6519 - val\_loss: 1.4055 - val\_acc: 0.6439
Epoch 74/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3414 - acc: 0.6511 - val\_loss: 1.4114 - val\_acc: 0.6496
Epoch 75/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3359 - acc: 0.6517 - val\_loss: 1.4116 - val\_acc: 0.6413
Epoch 76/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3191 - acc: 0.6534 - val\_loss: 1.4416 - val\_acc: 0.6232
Epoch 77/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3486 - acc: 0.6480 - val\_loss: 1.4074 - val\_acc: 0.6461
Epoch 78/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3232 - acc: 0.6530 - val\_loss: 1.3954 - val\_acc: 0.6470
Epoch 79/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3204 - acc: 0.6546 - val\_loss: 1.4156 - val\_acc: 0.6258
Epoch 80/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3052 - acc: 0.6559 - val\_loss: 1.4260 - val\_acc: 0.6449
Epoch 81/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3386 - acc: 0.6504 - val\_loss: 1.3901 - val\_acc: 0.6482
Epoch 82/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3275 - acc: 0.6531 - val\_loss: 1.4054 - val\_acc: 0.6556
Epoch 83/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3183 - acc: 0.6565 - val\_loss: 1.4013 - val\_acc: 0.6547
Epoch 84/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3128 - acc: 0.6568 - val\_loss: 1.4071 - val\_acc: 0.6389
Epoch 85/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3040 - acc: 0.6596 - val\_loss: 1.3984 - val\_acc: 0.6571
Epoch 86/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2862 - acc: 0.6628 - val\_loss: 1.3892 - val\_acc: 0.6499
Epoch 87/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.3016 - acc: 0.6592 - val\_loss: 1.3738 - val\_acc: 0.6489
Epoch 88/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2953 - acc: 0.6603 - val\_loss: 1.4008 - val\_acc: 0.6577
Epoch 89/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2949 - acc: 0.6583 - val\_loss: 1.3747 - val\_acc: 0.6518
Epoch 90/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2837 - acc: 0.6609 - val\_loss: 1.3693 - val\_acc: 0.6393
Epoch 91/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2904 - acc: 0.6595 - val\_loss: 1.3394 - val\_acc: 0.6579
Epoch 92/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2708 - acc: 0.6636 - val\_loss: 1.3450 - val\_acc: 0.6578
Epoch 93/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2801 - acc: 0.6601 - val\_loss: 1.3541 - val\_acc: 0.6454
Epoch 94/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2915 - acc: 0.6619 - val\_loss: 1.3828 - val\_acc: 0.6561
Epoch 95/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2898 - acc: 0.6634 - val\_loss: 1.3621 - val\_acc: 0.6532
Epoch 96/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2880 - acc: 0.6582 - val\_loss: 1.3736 - val\_acc: 0.6567
Epoch 97/200
35064/35064 [==============================] - 46s 1ms/step - loss: 1.2923 - acc: 0.6622 - val\_loss: 1.3704 - val\_acc: 0.6606
Epoch 98/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2838 - acc: 0.6628 - val\_loss: 1.3840 - val\_acc: 0.6514
Epoch 99/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2954 - acc: 0.6609 - val\_loss: 1.3779 - val\_acc: 0.6538
Epoch 100/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2855 - acc: 0.6619 - val\_loss: 1.3710 - val\_acc: 0.6470
Epoch 101/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2825 - acc: 0.6635 - val\_loss: 1.3939 - val\_acc: 0.6594
Epoch 102/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2829 - acc: 0.6660 - val\_loss: 1.4135 - val\_acc: 0.6569
Epoch 103/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2756 - acc: 0.6624 - val\_loss: 1.3609 - val\_acc: 0.6656
Epoch 104/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2559 - acc: 0.6660 - val\_loss: 1.3673 - val\_acc: 0.6628
Epoch 105/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.2765 - acc: 0.6615 - val\_loss: 1.3852 - val\_acc: 0.6445
Epoch 106/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.2579 - acc: 0.6660 - val\_loss: 1.3789 - val\_acc: 0.6431
Epoch 107/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2494 - acc: 0.6680 - val\_loss: 1.3658 - val\_acc: 0.6581
Epoch 108/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.2848 - acc: 0.6651 - val\_loss: 1.3521 - val\_acc: 0.6619
Epoch 109/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2489 - acc: 0.6691 - val\_loss: 1.3613 - val\_acc: 0.6588
Epoch 110/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2534 - acc: 0.6678 - val\_loss: 1.3684 - val\_acc: 0.6577
Epoch 111/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2494 - acc: 0.6741 - val\_loss: 1.3604 - val\_acc: 0.6585
Epoch 112/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2518 - acc: 0.6719 - val\_loss: 1.3819 - val\_acc: 0.6516
Epoch 113/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2530 - acc: 0.6694 - val\_loss: 1.3458 - val\_acc: 0.6678
Epoch 114/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2646 - acc: 0.6706 - val\_loss: 1.3967 - val\_acc: 0.6550
Epoch 115/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2761 - acc: 0.6699 - val\_loss: 1.3973 - val\_acc: 0.6404
Epoch 116/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2816 - acc: 0.6697 - val\_loss: 1.3882 - val\_acc: 0.6580
Epoch 117/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.2695 - acc: 0.6695 - val\_loss: 1.3692 - val\_acc: 0.6578
Epoch 118/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2461 - acc: 0.6729 - val\_loss: 1.3338 - val\_acc: 0.6644
Epoch 119/200
35064/35064 [==============================] - 46s 1ms/step - loss: 1.2319 - acc: 0.6760 - val\_loss: 1.3367 - val\_acc: 0.6624
Epoch 120/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2298 - acc: 0.6739 - val\_loss: 1.3467 - val\_acc: 0.6653
Epoch 121/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2261 - acc: 0.6755 - val\_loss: 1.3347 - val\_acc: 0.6656
Epoch 122/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2480 - acc: 0.6741 - val\_loss: 1.3453 - val\_acc: 0.6515
Epoch 123/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2387 - acc: 0.6745 - val\_loss: 1.3177 - val\_acc: 0.6662
Epoch 124/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2189 - acc: 0.6794 - val\_loss: 1.3097 - val\_acc: 0.6674
Epoch 125/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2165 - acc: 0.6785 - val\_loss: 1.3117 - val\_acc: 0.6756
Epoch 126/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2241 - acc: 0.6760 - val\_loss: 1.3630 - val\_acc: 0.6525
Epoch 127/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2517 - acc: 0.6716 - val\_loss: 1.3276 - val\_acc: 0.6709
Epoch 128/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2289 - acc: 0.6756 - val\_loss: 1.3220 - val\_acc: 0.6661
Epoch 129/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2335 - acc: 0.6756 - val\_loss: 1.3310 - val\_acc: 0.6581
Epoch 130/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2298 - acc: 0.6781 - val\_loss: 1.3498 - val\_acc: 0.6396
Epoch 131/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2498 - acc: 0.6733 - val\_loss: 1.3367 - val\_acc: 0.6601
Epoch 132/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.2166 - acc: 0.6787 - val\_loss: 1.3410 - val\_acc: 0.6571
Epoch 133/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.2229 - acc: 0.6806 - val\_loss: 1.3368 - val\_acc: 0.6645
Epoch 134/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2144 - acc: 0.6819 - val\_loss: 1.3294 - val\_acc: 0.6660
Epoch 135/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2000 - acc: 0.6828 - val\_loss: 1.3294 - val\_acc: 0.6610
Epoch 136/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2034 - acc: 0.6814 - val\_loss: 1.3274 - val\_acc: 0.6660
Epoch 137/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2042 - acc: 0.6818 - val\_loss: 1.3471 - val\_acc: 0.6575
Epoch 138/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.2273 - acc: 0.6765 - val\_loss: 1.3152 - val\_acc: 0.6664
Epoch 139/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2724 - acc: 0.6676 - val\_loss: 1.3740 - val\_acc: 0.6598
Epoch 140/200
35064/35064 [==============================] - 46s 1ms/step - loss: 1.2595 - acc: 0.6734 - val\_loss: 1.3508 - val\_acc: 0.6574
Epoch 141/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2531 - acc: 0.6740 - val\_loss: 1.3484 - val\_acc: 0.6635
Epoch 142/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2322 - acc: 0.6792 - val\_loss: 1.3727 - val\_acc: 0.6466
Epoch 143/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2208 - acc: 0.6802 - val\_loss: 1.3483 - val\_acc: 0.6593
Epoch 144/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2357 - acc: 0.6749 - val\_loss: 1.3459 - val\_acc: 0.6691
Epoch 145/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2152 - acc: 0.6764 - val\_loss: 1.3461 - val\_acc: 0.6637
Epoch 146/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2145 - acc: 0.6778 - val\_loss: 1.3710 - val\_acc: 0.6491
Epoch 147/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2219 - acc: 0.6806 - val\_loss: 1.3430 - val\_acc: 0.6602
Epoch 148/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2219 - acc: 0.6781 - val\_loss: 1.3556 - val\_acc: 0.6572
Epoch 149/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2030 - acc: 0.6818 - val\_loss: 1.2989 - val\_acc: 0.6662
Epoch 150/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2067 - acc: 0.6828 - val\_loss: 1.3270 - val\_acc: 0.6670
Epoch 151/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1999 - acc: 0.6825 - val\_loss: 1.3262 - val\_acc: 0.6650
Epoch 152/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1915 - acc: 0.6854 - val\_loss: 1.3428 - val\_acc: 0.6595
Epoch 153/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2108 - acc: 0.6834 - val\_loss: 1.3408 - val\_acc: 0.6687
Epoch 154/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1973 - acc: 0.6877 - val\_loss: 1.3460 - val\_acc: 0.6669
Epoch 155/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1864 - acc: 0.6906 - val\_loss: 1.3220 - val\_acc: 0.6700
Epoch 156/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1843 - acc: 0.6904 - val\_loss: 1.3369 - val\_acc: 0.6723
Epoch 157/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1787 - acc: 0.6876 - val\_loss: 1.3322 - val\_acc: 0.6650
Epoch 158/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2001 - acc: 0.6867 - val\_loss: 1.3289 - val\_acc: 0.6669
Epoch 159/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1874 - acc: 0.6859 - val\_loss: 1.3062 - val\_acc: 0.6601
Epoch 160/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1946 - acc: 0.6860 - val\_loss: 1.3265 - val\_acc: 0.6660
Epoch 161/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1951 - acc: 0.6878 - val\_loss: 1.3281 - val\_acc: 0.6638
Epoch 162/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1840 - acc: 0.6921 - val\_loss: 1.3214 - val\_acc: 0.6725
Epoch 163/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1814 - acc: 0.6916 - val\_loss: 1.3298 - val\_acc: 0.6670
Epoch 164/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1629 - acc: 0.6953 - val\_loss: 1.3233 - val\_acc: 0.6562
Epoch 165/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1667 - acc: 0.6947 - val\_loss: 1.3329 - val\_acc: 0.6574
Epoch 166/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1680 - acc: 0.6951 - val\_loss: 1.3373 - val\_acc: 0.6602
Epoch 167/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1904 - acc: 0.6907 - val\_loss: 1.3620 - val\_acc: 0.6725
Epoch 168/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1895 - acc: 0.6923 - val\_loss: 1.3066 - val\_acc: 0.6777
Epoch 169/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1700 - acc: 0.6946 - val\_loss: 1.3090 - val\_acc: 0.6687
Epoch 170/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1884 - acc: 0.6883 - val\_loss: 1.3521 - val\_acc: 0.6607
Epoch 171/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2025 - acc: 0.6902 - val\_loss: 1.3488 - val\_acc: 0.6742
Epoch 172/200
35064/35064 [==============================] - 44s 1ms/step - loss: 1.1876 - acc: 0.6902 - val\_loss: 1.3095 - val\_acc: 0.6788
Epoch 173/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1812 - acc: 0.6918 - val\_loss: 1.3021 - val\_acc: 0.6675
Epoch 174/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1782 - acc: 0.6930 - val\_loss: 1.3182 - val\_acc: 0.6708
Epoch 175/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1956 - acc: 0.6895 - val\_loss: 1.3041 - val\_acc: 0.6689
Epoch 176/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1831 - acc: 0.6941 - val\_loss: 1.3039 - val\_acc: 0.6672
Epoch 177/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1712 - acc: 0.6945 - val\_loss: 1.2883 - val\_acc: 0.6793
Epoch 178/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1693 - acc: 0.6980 - val\_loss: 1.2934 - val\_acc: 0.6759
Epoch 179/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1612 - acc: 0.6993 - val\_loss: 1.2959 - val\_acc: 0.6822
Epoch 180/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1648 - acc: 0.6937 - val\_loss: 1.3116 - val\_acc: 0.6697
Epoch 181/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1820 - acc: 0.6913 - val\_loss: 1.3124 - val\_acc: 0.6785
Epoch 182/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1671 - acc: 0.6977 - val\_loss: 1.3090 - val\_acc: 0.6735
Epoch 183/200
35064/35064 [==============================] - 46s 1ms/step - loss: 1.1698 - acc: 0.6956 - val\_loss: 1.2942 - val\_acc: 0.6695
Epoch 184/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1565 - acc: 0.7006 - val\_loss: 1.2847 - val\_acc: 0.6817
Epoch 185/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1625 - acc: 0.6969 - val\_loss: 1.3279 - val\_acc: 0.6677
Epoch 186/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1642 - acc: 0.6970 - val\_loss: 1.3194 - val\_acc: 0.6793
Epoch 187/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1648 - acc: 0.6969 - val\_loss: 1.2994 - val\_acc: 0.6696
Epoch 188/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1680 - acc: 0.6967 - val\_loss: 1.2984 - val\_acc: 0.6692
Epoch 189/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1790 - acc: 0.6926 - val\_loss: 1.3152 - val\_acc: 0.6727
Epoch 190/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1901 - acc: 0.6949 - val\_loss: 1.3165 - val\_acc: 0.6674
Epoch 191/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1915 - acc: 0.6897 - val\_loss: 1.3547 - val\_acc: 0.6638
Epoch 192/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2072 - acc: 0.6937 - val\_loss: 1.3261 - val\_acc: 0.6712
Epoch 193/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1907 - acc: 0.6958 - val\_loss: 1.3597 - val\_acc: 0.6577
Epoch 194/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.2027 - acc: 0.6937 - val\_loss: 1.3372 - val\_acc: 0.6753
Epoch 195/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1702 - acc: 0.6997 - val\_loss: 1.3255 - val\_acc: 0.6726
Epoch 196/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1927 - acc: 0.6964 - val\_loss: 1.3404 - val\_acc: 0.6743
Epoch 197/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1864 - acc: 0.6943 - val\_loss: 1.3247 - val\_acc: 0.6707
Epoch 198/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1727 - acc: 0.6978 - val\_loss: 1.3036 - val\_acc: 0.6686
Epoch 199/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1588 - acc: 0.6984 - val\_loss: 1.2892 - val\_acc: 0.6845
Epoch 200/200
35064/35064 [==============================] - 45s 1ms/step - loss: 1.1437 - acc: 0.7031 - val\_loss: 1.2823 - val\_acc: 0.6836
results for adam  0.0001
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_28 (InputLayer)           (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_29 (InputLayer)           (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_30 (InputLayer)           (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_17 (Model)                (None, 280)          1525        input\_28[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_18 (Model)                (None, 100)          1125        input\_29[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_19 (Model)                (None, 100)          1175        input\_30[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_5 (Concatenate)     (None, 480)          0           model\_17[1][0]                   
                                                                 model\_18[1][0]                   
                                                                 model\_19[1][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_13 (Dense)                (None, 500)          240500      concatenate\_5[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_5 (Dropout)             (None, 500)          0           dense\_13[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_14 (Dense)                (None, 500)          250500      dropout\_5[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_15 (Dense)                (None, 5)            2505        dense\_14[0][0]                   
==================================================================================================
Total params: 497,330
Trainable params: 497,330
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_7.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_8.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.6835500798539813
kappa:  0.5423040535524692

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 58s 2ms/step - loss: 8.9034 - acc: 0.3177 - val\_loss: 7.0852 - val\_acc: 0.3718
Epoch 2/200
35064/35064 [==============================] - 57s 2ms/step - loss: 8.0149 - acc: 0.3558 - val\_loss: 6.4518 - val\_acc: 0.3948
Epoch 3/200
35064/35064 [==============================] - 59s 2ms/step - loss: 7.5437 - acc: 0.3676 - val\_loss: 6.0250 - val\_acc: 0.4074
Epoch 4/200
35064/35064 [==============================] - 58s 2ms/step - loss: 7.1576 - acc: 0.3718 - val\_loss: 5.7299 - val\_acc: 0.4127
Epoch 5/200
35064/35064 [==============================] - 58s 2ms/step - loss: 6.7183 - acc: 0.3844 - val\_loss: 5.6416 - val\_acc: 0.4122
Epoch 6/200
35064/35064 [==============================] - 58s 2ms/step - loss: 6.3497 - acc: 0.3924 - val\_loss: 4.8962 - val\_acc: 0.4516
Epoch 7/200
35064/35064 [==============================] - 58s 2ms/step - loss: 5.9767 - acc: 0.3990 - val\_loss: 4.8141 - val\_acc: 0.4597
Epoch 8/200
35064/35064 [==============================] - 58s 2ms/step - loss: 5.7237 - acc: 0.4052 - val\_loss: 4.6152 - val\_acc: 0.4602
Epoch 9/200
35064/35064 [==============================] - 58s 2ms/step - loss: 5.4754 - acc: 0.4159 - val\_loss: 4.4245 - val\_acc: 0.4679
Epoch 10/200
35064/35064 [==============================] - 58s 2ms/step - loss: 5.2467 - acc: 0.4135 - val\_loss: 4.2642 - val\_acc: 0.4673
Epoch 11/200
35064/35064 [==============================] - 58s 2ms/step - loss: 5.0562 - acc: 0.4186 - val\_loss: 4.1464 - val\_acc: 0.4747
Epoch 12/200
35064/35064 [==============================] - 58s 2ms/step - loss: 4.8658 - acc: 0.4235 - val\_loss: 4.0297 - val\_acc: 0.4746
Epoch 13/200
35064/35064 [==============================] - 57s 2ms/step - loss: 4.7038 - acc: 0.4251 - val\_loss: 3.9358 - val\_acc: 0.4701
Epoch 14/200
35064/35064 [==============================] - 58s 2ms/step - loss: 4.5726 - acc: 0.4252 - val\_loss: 3.9001 - val\_acc: 0.4722
Epoch 15/200
35064/35064 [==============================] - 58s 2ms/step - loss: 4.4283 - acc: 0.4299 - val\_loss: 3.7677 - val\_acc: 0.4833
Epoch 16/200
35064/35064 [==============================] - 58s 2ms/step - loss: 4.2993 - acc: 0.4336 - val\_loss: 3.6924 - val\_acc: 0.4754
Epoch 17/200
35064/35064 [==============================] - 58s 2ms/step - loss: 4.1965 - acc: 0.4355 - val\_loss: 3.6207 - val\_acc: 0.4884
Epoch 18/200
35064/35064 [==============================] - 58s 2ms/step - loss: 4.0953 - acc: 0.4371 - val\_loss: 3.5501 - val\_acc: 0.4845
Epoch 19/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.9888 - acc: 0.4391 - val\_loss: 3.4794 - val\_acc: 0.4813
Epoch 20/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.9178 - acc: 0.4397 - val\_loss: 3.3857 - val\_acc: 0.4895
Epoch 21/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.8166 - acc: 0.4448 - val\_loss: 3.3598 - val\_acc: 0.4896
Epoch 22/200
35064/35064 [==============================] - 57s 2ms/step - loss: 3.7256 - acc: 0.4450 - val\_loss: 3.2900 - val\_acc: 0.4841
Epoch 23/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.6658 - acc: 0.4484 - val\_loss: 3.2435 - val\_acc: 0.4898
Epoch 24/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.5605 - acc: 0.4535 - val\_loss: 3.1856 - val\_acc: 0.4926
Epoch 25/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.5270 - acc: 0.4507 - val\_loss: 3.1519 - val\_acc: 0.4904
Epoch 26/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.4636 - acc: 0.4550 - val\_loss: 3.1233 - val\_acc: 0.4861
Epoch 27/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.3766 - acc: 0.4649 - val\_loss: 3.1005 - val\_acc: 0.4930
Epoch 28/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.3365 - acc: 0.4608 - val\_loss: 3.0347 - val\_acc: 0.4971
Epoch 29/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.3136 - acc: 0.4641 - val\_loss: 3.0074 - val\_acc: 0.4978
Epoch 30/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.2589 - acc: 0.4700 - val\_loss: 2.9493 - val\_acc: 0.4893
Epoch 31/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.2329 - acc: 0.4672 - val\_loss: 2.9539 - val\_acc: 0.4930
Epoch 32/200
35064/35064 [==============================] - 57s 2ms/step - loss: 3.1766 - acc: 0.4712 - val\_loss: 2.9294 - val\_acc: 0.4878
Epoch 33/200
35064/35064 [==============================] - 57s 2ms/step - loss: 3.1649 - acc: 0.4733 - val\_loss: 2.9118 - val\_acc: 0.5011
Epoch 34/200
35064/35064 [==============================] - 57s 2ms/step - loss: 3.1522 - acc: 0.4758 - val\_loss: 2.8822 - val\_acc: 0.4958
Epoch 35/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.0911 - acc: 0.4754 - val\_loss: 2.8420 - val\_acc: 0.5011
Epoch 36/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.0556 - acc: 0.4719 - val\_loss: 2.8052 - val\_acc: 0.5024
Epoch 37/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.0238 - acc: 0.4763 - val\_loss: 2.8068 - val\_acc: 0.5010
Epoch 38/200
35064/35064 [==============================] - 58s 2ms/step - loss: 3.0320 - acc: 0.4764 - val\_loss: 2.7743 - val\_acc: 0.5066
Epoch 39/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.9828 - acc: 0.4793 - val\_loss: 2.7485 - val\_acc: 0.5080
Epoch 40/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.9697 - acc: 0.4837 - val\_loss: 2.7306 - val\_acc: 0.5123
Epoch 41/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.9641 - acc: 0.4827 - val\_loss: 2.7203 - val\_acc: 0.5062
Epoch 42/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.9207 - acc: 0.4883 - val\_loss: 2.6938 - val\_acc: 0.5120
Epoch 43/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.8974 - acc: 0.4894 - val\_loss: 2.6517 - val\_acc: 0.5209
Epoch 44/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.8867 - acc: 0.4926 - val\_loss: 2.6667 - val\_acc: 0.5116
Epoch 45/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.8470 - acc: 0.4961 - val\_loss: 2.6552 - val\_acc: 0.5135
Epoch 46/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.8608 - acc: 0.4930 - val\_loss: 2.6452 - val\_acc: 0.5157
Epoch 47/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.8389 - acc: 0.4947 - val\_loss: 2.6444 - val\_acc: 0.5179
Epoch 48/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.7890 - acc: 0.4982 - val\_loss: 2.6117 - val\_acc: 0.5233
Epoch 49/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.7944 - acc: 0.4978 - val\_loss: 2.6091 - val\_acc: 0.5120
Epoch 50/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.7625 - acc: 0.4997 - val\_loss: 2.5965 - val\_acc: 0.5165
Epoch 51/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.7232 - acc: 0.5052 - val\_loss: 2.5624 - val\_acc: 0.5297
Epoch 52/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.7484 - acc: 0.4997 - val\_loss: 2.5590 - val\_acc: 0.5279
Epoch 53/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.6949 - acc: 0.5085 - val\_loss: 2.5652 - val\_acc: 0.5249
Epoch 54/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.7081 - acc: 0.5057 - val\_loss: 2.5341 - val\_acc: 0.5340
Epoch 55/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.6985 - acc: 0.5043 - val\_loss: 2.5222 - val\_acc: 0.5343
Epoch 56/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.6914 - acc: 0.5061 - val\_loss: 2.5203 - val\_acc: 0.5291
Epoch 57/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.6732 - acc: 0.5131 - val\_loss: 2.4860 - val\_acc: 0.5345
Epoch 58/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.6313 - acc: 0.5149 - val\_loss: 2.4984 - val\_acc: 0.5289
Epoch 59/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.6406 - acc: 0.5152 - val\_loss: 2.4829 - val\_acc: 0.5341
Epoch 60/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.6132 - acc: 0.5161 - val\_loss: 2.4832 - val\_acc: 0.5357
Epoch 61/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.5916 - acc: 0.5188 - val\_loss: 2.4768 - val\_acc: 0.5341
Epoch 62/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.5783 - acc: 0.5189 - val\_loss: 2.4614 - val\_acc: 0.5362
Epoch 63/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.5640 - acc: 0.5229 - val\_loss: 2.4422 - val\_acc: 0.5371
Epoch 64/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.5336 - acc: 0.5232 - val\_loss: 2.4186 - val\_acc: 0.5400
Epoch 65/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.5417 - acc: 0.5227 - val\_loss: 2.3946 - val\_acc: 0.5460
Epoch 66/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.5454 - acc: 0.5218 - val\_loss: 2.3783 - val\_acc: 0.5436
Epoch 67/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.5035 - acc: 0.5253 - val\_loss: 2.3772 - val\_acc: 0.5394
Epoch 68/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.4737 - acc: 0.5294 - val\_loss: 2.3649 - val\_acc: 0.5413
Epoch 69/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.5034 - acc: 0.5255 - val\_loss: 2.3616 - val\_acc: 0.5470
Epoch 70/200
35064/35064 [==============================] - 59s 2ms/step - loss: 2.4704 - acc: 0.5289 - val\_loss: 2.3564 - val\_acc: 0.5405
Epoch 71/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.4610 - acc: 0.5282 - val\_loss: 2.3422 - val\_acc: 0.5489
Epoch 72/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.4726 - acc: 0.5267 - val\_loss: 2.3314 - val\_acc: 0.5461
Epoch 73/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.4591 - acc: 0.5318 - val\_loss: 2.3291 - val\_acc: 0.5511
Epoch 74/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.4170 - acc: 0.5328 - val\_loss: 2.3382 - val\_acc: 0.5479
Epoch 75/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.4096 - acc: 0.5327 - val\_loss: 2.3102 - val\_acc: 0.5552
Epoch 76/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.4374 - acc: 0.5337 - val\_loss: 2.3180 - val\_acc: 0.5552
Epoch 77/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.4131 - acc: 0.5345 - val\_loss: 2.2951 - val\_acc: 0.5586
Epoch 78/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.4149 - acc: 0.5381 - val\_loss: 2.2888 - val\_acc: 0.5627
Epoch 79/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.3789 - acc: 0.5381 - val\_loss: 2.2691 - val\_acc: 0.5601
Epoch 80/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.3473 - acc: 0.5428 - val\_loss: 2.2615 - val\_acc: 0.5586
Epoch 81/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.3445 - acc: 0.5449 - val\_loss: 2.2507 - val\_acc: 0.5614
Epoch 82/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.3573 - acc: 0.5405 - val\_loss: 2.2246 - val\_acc: 0.5673
Epoch 83/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.3196 - acc: 0.5481 - val\_loss: 2.2251 - val\_acc: 0.5646
Epoch 84/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.3318 - acc: 0.5420 - val\_loss: 2.2290 - val\_acc: 0.5661
Epoch 85/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2900 - acc: 0.5479 - val\_loss: 2.2239 - val\_acc: 0.5679
Epoch 86/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.3232 - acc: 0.5472 - val\_loss: 2.2330 - val\_acc: 0.5646
Epoch 87/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2681 - acc: 0.5497 - val\_loss: 2.2270 - val\_acc: 0.5666
Epoch 88/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2718 - acc: 0.5493 - val\_loss: 2.1998 - val\_acc: 0.5722
Epoch 89/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2436 - acc: 0.5493 - val\_loss: 2.1866 - val\_acc: 0.5718
Epoch 90/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2732 - acc: 0.5482 - val\_loss: 2.1805 - val\_acc: 0.5723
Epoch 91/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2405 - acc: 0.5512 - val\_loss: 2.1704 - val\_acc: 0.5765
Epoch 92/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2551 - acc: 0.5504 - val\_loss: 2.1676 - val\_acc: 0.5739
Epoch 93/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2438 - acc: 0.5528 - val\_loss: 2.1440 - val\_acc: 0.5742
Epoch 94/200
35064/35064 [==============================] - 59s 2ms/step - loss: 2.2129 - acc: 0.5551 - val\_loss: 2.1592 - val\_acc: 0.5712
Epoch 95/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2186 - acc: 0.5532 - val\_loss: 2.1391 - val\_acc: 0.5745
Epoch 96/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2178 - acc: 0.5574 - val\_loss: 2.1262 - val\_acc: 0.5757
Epoch 97/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2178 - acc: 0.5542 - val\_loss: 2.1305 - val\_acc: 0.5751
Epoch 98/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2213 - acc: 0.5542 - val\_loss: 2.1160 - val\_acc: 0.5779
Epoch 99/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1775 - acc: 0.5568 - val\_loss: 2.1110 - val\_acc: 0.5739
Epoch 100/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.2004 - acc: 0.5550 - val\_loss: 2.1147 - val\_acc: 0.5745
Epoch 101/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.1880 - acc: 0.5538 - val\_loss: 2.1039 - val\_acc: 0.5754
Epoch 102/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1705 - acc: 0.5566 - val\_loss: 2.1243 - val\_acc: 0.5761
Epoch 103/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1783 - acc: 0.5597 - val\_loss: 2.1153 - val\_acc: 0.5764
Epoch 104/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1490 - acc: 0.5613 - val\_loss: 2.1056 - val\_acc: 0.5778
Epoch 105/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1431 - acc: 0.5588 - val\_loss: 2.0981 - val\_acc: 0.5765
Epoch 106/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1475 - acc: 0.5615 - val\_loss: 2.0853 - val\_acc: 0.5780
Epoch 107/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1455 - acc: 0.5635 - val\_loss: 2.0914 - val\_acc: 0.5795
Epoch 108/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1452 - acc: 0.5617 - val\_loss: 2.0760 - val\_acc: 0.5801
Epoch 109/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.1349 - acc: 0.5631 - val\_loss: 2.0819 - val\_acc: 0.5809
Epoch 110/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1001 - acc: 0.5652 - val\_loss: 2.0838 - val\_acc: 0.5803
Epoch 111/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1258 - acc: 0.5593 - val\_loss: 2.0736 - val\_acc: 0.5817
Epoch 112/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1107 - acc: 0.5653 - val\_loss: 2.0784 - val\_acc: 0.5813
Epoch 113/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1039 - acc: 0.5651 - val\_loss: 2.0712 - val\_acc: 0.5788
Epoch 114/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0932 - acc: 0.5653 - val\_loss: 2.0515 - val\_acc: 0.5843
Epoch 115/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.0740 - acc: 0.5687 - val\_loss: 2.0742 - val\_acc: 0.5815
Epoch 116/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0718 - acc: 0.5663 - val\_loss: 2.0660 - val\_acc: 0.5840
Epoch 117/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1017 - acc: 0.5658 - val\_loss: 2.0481 - val\_acc: 0.5852
Epoch 118/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.0887 - acc: 0.5659 - val\_loss: 2.0587 - val\_acc: 0.5860
Epoch 119/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0769 - acc: 0.5689 - val\_loss: 2.0409 - val\_acc: 0.5861
Epoch 120/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0469 - acc: 0.5688 - val\_loss: 2.0414 - val\_acc: 0.5883
Epoch 121/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0491 - acc: 0.5678 - val\_loss: 2.0193 - val\_acc: 0.5914
Epoch 122/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0501 - acc: 0.5707 - val\_loss: 2.0322 - val\_acc: 0.5872
Epoch 123/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0597 - acc: 0.5695 - val\_loss: 2.0161 - val\_acc: 0.5901
Epoch 124/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0529 - acc: 0.5737 - val\_loss: 2.0040 - val\_acc: 0.5885
Epoch 125/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.0268 - acc: 0.5733 - val\_loss: 1.9923 - val\_acc: 0.5901
Epoch 126/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0197 - acc: 0.5715 - val\_loss: 2.0010 - val\_acc: 0.5898
Epoch 127/200
35064/35064 [==============================] - 57s 2ms/step - loss: 2.0187 - acc: 0.5731 - val\_loss: 1.9967 - val\_acc: 0.5885
Epoch 128/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0190 - acc: 0.5731 - val\_loss: 1.9952 - val\_acc: 0.5881
Epoch 129/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0174 - acc: 0.5720 - val\_loss: 2.0076 - val\_acc: 0.5882
Epoch 130/200
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0066 - acc: 0.5747 - val\_loss: 1.9941 - val\_acc: 0.5894
Epoch 131/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9973 - acc: 0.5753 - val\_loss: 1.9772 - val\_acc: 0.5894
Epoch 132/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.9834 - acc: 0.5735 - val\_loss: 1.9763 - val\_acc: 0.5929
Epoch 133/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9744 - acc: 0.5783 - val\_loss: 1.9701 - val\_acc: 0.5893
Epoch 134/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9995 - acc: 0.5739 - val\_loss: 2.0093 - val\_acc: 0.5917
Epoch 135/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9807 - acc: 0.5761 - val\_loss: 1.9774 - val\_acc: 0.5921
Epoch 136/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9902 - acc: 0.5734 - val\_loss: 1.9578 - val\_acc: 0.5910
Epoch 137/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9705 - acc: 0.5792 - val\_loss: 1.9516 - val\_acc: 0.5916
Epoch 138/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9554 - acc: 0.5786 - val\_loss: 1.9421 - val\_acc: 0.5918
Epoch 139/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9478 - acc: 0.5799 - val\_loss: 1.9501 - val\_acc: 0.5918
Epoch 140/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9670 - acc: 0.5783 - val\_loss: 1.9200 - val\_acc: 0.5942
Epoch 141/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9229 - acc: 0.5785 - val\_loss: 1.9113 - val\_acc: 0.5962
Epoch 142/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9328 - acc: 0.5802 - val\_loss: 1.9265 - val\_acc: 0.5954
Epoch 143/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9258 - acc: 0.5799 - val\_loss: 1.9152 - val\_acc: 0.5971
Epoch 144/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.9353 - acc: 0.5791 - val\_loss: 1.9142 - val\_acc: 0.5956
Epoch 145/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9104 - acc: 0.5815 - val\_loss: 1.9129 - val\_acc: 0.5963
Epoch 146/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.9104 - acc: 0.5820 - val\_loss: 1.9109 - val\_acc: 0.5971
Epoch 147/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.9053 - acc: 0.5819 - val\_loss: 1.9015 - val\_acc: 0.5974
Epoch 148/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8794 - acc: 0.5817 - val\_loss: 1.9024 - val\_acc: 0.5979
Epoch 149/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8785 - acc: 0.5860 - val\_loss: 1.9043 - val\_acc: 0.5992
Epoch 150/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8909 - acc: 0.5839 - val\_loss: 1.9053 - val\_acc: 0.5967
Epoch 151/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8939 - acc: 0.5859 - val\_loss: 1.9029 - val\_acc: 0.5966
Epoch 152/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8634 - acc: 0.5853 - val\_loss: 1.8953 - val\_acc: 0.5948
Epoch 153/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.8638 - acc: 0.5864 - val\_loss: 1.9017 - val\_acc: 0.5959
Epoch 154/200
35064/35064 [==============================] - 59s 2ms/step - loss: 1.8854 - acc: 0.5862 - val\_loss: 1.9044 - val\_acc: 0.5955
Epoch 155/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8676 - acc: 0.5842 - val\_loss: 1.9032 - val\_acc: 0.5971
Epoch 156/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8650 - acc: 0.5858 - val\_loss: 1.8990 - val\_acc: 0.5999
Epoch 157/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8455 - acc: 0.5874 - val\_loss: 1.9098 - val\_acc: 0.5990
Epoch 158/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8643 - acc: 0.5863 - val\_loss: 1.8953 - val\_acc: 0.6000
Epoch 159/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8720 - acc: 0.5863 - val\_loss: 1.9006 - val\_acc: 0.5999
Epoch 160/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8630 - acc: 0.5875 - val\_loss: 1.8895 - val\_acc: 0.6019
Epoch 161/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8141 - acc: 0.5932 - val\_loss: 1.8847 - val\_acc: 0.6010
Epoch 162/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8397 - acc: 0.5894 - val\_loss: 1.8784 - val\_acc: 0.6008
Epoch 163/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8328 - acc: 0.5927 - val\_loss: 1.8885 - val\_acc: 0.6008
Epoch 164/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8313 - acc: 0.5878 - val\_loss: 1.8798 - val\_acc: 0.6028
Epoch 165/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8298 - acc: 0.5903 - val\_loss: 1.8688 - val\_acc: 0.6026
Epoch 166/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8233 - acc: 0.5897 - val\_loss: 1.8667 - val\_acc: 0.6008
Epoch 167/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.8296 - acc: 0.5896 - val\_loss: 1.8784 - val\_acc: 0.6002
Epoch 168/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8164 - acc: 0.5913 - val\_loss: 1.8720 - val\_acc: 0.6010
Epoch 169/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8087 - acc: 0.5915 - val\_loss: 1.8551 - val\_acc: 0.6021
Epoch 170/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8218 - acc: 0.5938 - val\_loss: 1.8584 - val\_acc: 0.6034
Epoch 171/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8161 - acc: 0.5909 - val\_loss: 1.8570 - val\_acc: 0.6060
Epoch 172/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.8033 - acc: 0.5898 - val\_loss: 1.8418 - val\_acc: 0.6043
Epoch 173/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7997 - acc: 0.5904 - val\_loss: 1.8404 - val\_acc: 0.6036
Epoch 174/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7812 - acc: 0.5923 - val\_loss: 1.8240 - val\_acc: 0.6040
Epoch 175/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7898 - acc: 0.5955 - val\_loss: 1.8261 - val\_acc: 0.6023
Epoch 176/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7749 - acc: 0.5906 - val\_loss: 1.8314 - val\_acc: 0.6028
Epoch 177/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7722 - acc: 0.5921 - val\_loss: 1.8249 - val\_acc: 0.6034
Epoch 178/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7830 - acc: 0.5929 - val\_loss: 1.8196 - val\_acc: 0.6038
Epoch 179/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7667 - acc: 0.5949 - val\_loss: 1.8125 - val\_acc: 0.6043
Epoch 180/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7625 - acc: 0.5944 - val\_loss: 1.7969 - val\_acc: 0.6065
Epoch 181/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7657 - acc: 0.5960 - val\_loss: 1.7936 - val\_acc: 0.6063
Epoch 182/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7583 - acc: 0.5929 - val\_loss: 1.8123 - val\_acc: 0.6069
Epoch 183/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7666 - acc: 0.5921 - val\_loss: 1.8197 - val\_acc: 0.6035
Epoch 184/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7580 - acc: 0.5973 - val\_loss: 1.8087 - val\_acc: 0.6051
Epoch 185/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7711 - acc: 0.5929 - val\_loss: 1.8167 - val\_acc: 0.6071
Epoch 186/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7725 - acc: 0.5955 - val\_loss: 1.8004 - val\_acc: 0.6096
Epoch 187/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7471 - acc: 0.5972 - val\_loss: 1.8001 - val\_acc: 0.6081
Epoch 188/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7399 - acc: 0.5967 - val\_loss: 1.8007 - val\_acc: 0.6056
Epoch 189/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7336 - acc: 0.5955 - val\_loss: 1.7903 - val\_acc: 0.6072
Epoch 190/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7339 - acc: 0.5966 - val\_loss: 1.8087 - val\_acc: 0.6064
Epoch 191/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7299 - acc: 0.5963 - val\_loss: 1.8204 - val\_acc: 0.6051
Epoch 192/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7172 - acc: 0.6013 - val\_loss: 1.8141 - val\_acc: 0.6060
Epoch 193/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7398 - acc: 0.5977 - val\_loss: 1.8127 - val\_acc: 0.6061
Epoch 194/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7238 - acc: 0.5979 - val\_loss: 1.8047 - val\_acc: 0.6071
Epoch 195/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7135 - acc: 0.5967 - val\_loss: 1.7982 - val\_acc: 0.6088
Epoch 196/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7114 - acc: 0.6003 - val\_loss: 1.7918 - val\_acc: 0.6091
Epoch 197/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7241 - acc: 0.5974 - val\_loss: 1.8011 - val\_acc: 0.6076
Epoch 198/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7120 - acc: 0.5986 - val\_loss: 1.7896 - val\_acc: 0.6102
Epoch 199/200
35064/35064 [==============================] - 57s 2ms/step - loss: 1.7118 - acc: 0.5993 - val\_loss: 1.7987 - val\_acc: 0.6113
Epoch 200/200
35064/35064 [==============================] - 58s 2ms/step - loss: 1.7043 - acc: 0.5994 - val\_loss: 1.7879 - val\_acc: 0.6112
results for adam  1e-05
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_34 (InputLayer)           (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_35 (InputLayer)           (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_36 (InputLayer)           (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_21 (Model)                (None, 280)          1525        input\_34[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_22 (Model)                (None, 100)          1125        input\_35[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_23 (Model)                (None, 100)          1175        input\_36[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_6 (Concatenate)     (None, 480)          0           model\_21[1][0]                   
                                                                 model\_22[1][0]                   
                                                                 model\_23[1][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_16 (Dense)                (None, 500)          240500      concatenate\_6[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_6 (Dropout)             (None, 500)          0           dense\_16[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_17 (Dense)                (None, 500)          250500      dropout\_6[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_18 (Dense)                (None, 5)            2505        dense\_17[0][0]                   
==================================================================================================
Total params: 497,330
Trainable params: 497,330
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_12.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_13.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.6112251882272416
kappa:  0.43700519041360064

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  after removing the cwd from sys.path.
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 74s 2ms/step - loss: 13.2296 - acc: 0.1352 - val\_loss: 12.5849 - val\_acc: 0.1799
Epoch 2/200
35064/35064 [==============================] - 72s 2ms/step - loss: 12.4455 - acc: 0.1721 - val\_loss: 11.0168 - val\_acc: 0.2121
Epoch 3/200
35064/35064 [==============================] - 73s 2ms/step - loss: 11.0787 - acc: 0.2384 - val\_loss: 8.8596 - val\_acc: 0.3657
Epoch 4/200
35064/35064 [==============================] - 72s 2ms/step - loss: 10.2754 - acc: 0.2976 - val\_loss: 9.5929 - val\_acc: 0.3714
Epoch 5/200
35064/35064 [==============================] - 74s 2ms/step - loss: 10.1607 - acc: 0.3152 - val\_loss: 9.7669 - val\_acc: 0.3719
Epoch 6/200
35064/35064 [==============================] - 73s 2ms/step - loss: 10.0645 - acc: 0.3274 - val\_loss: 9.8389 - val\_acc: 0.3726
Epoch 7/200
35064/35064 [==============================] - 73s 2ms/step - loss: 10.0290 - acc: 0.3322 - val\_loss: 9.8945 - val\_acc: 0.3734
Epoch 8/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.9737 - acc: 0.3386 - val\_loss: 9.8813 - val\_acc: 0.3736
Epoch 9/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.9760 - acc: 0.3394 - val\_loss: 9.8882 - val\_acc: 0.3744
Epoch 10/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.9421 - acc: 0.3373 - val\_loss: 9.8251 - val\_acc: 0.3742
Epoch 11/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.8372 - acc: 0.3394 - val\_loss: 9.7406 - val\_acc: 0.3745
Epoch 12/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.7323 - acc: 0.3399 - val\_loss: 9.5135 - val\_acc: 0.3757
Epoch 13/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.6350 - acc: 0.3398 - val\_loss: 8.8404 - val\_acc: 0.3796
Epoch 14/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.3193 - acc: 0.3478 - val\_loss: 7.8321 - val\_acc: 0.3994
Epoch 15/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.1923 - acc: 0.3519 - val\_loss: 7.6560 - val\_acc: 0.4061
Epoch 16/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.1326 - acc: 0.3534 - val\_loss: 7.7232 - val\_acc: 0.4089
Epoch 17/200
35064/35064 [==============================] - 73s 2ms/step - loss: 9.0224 - acc: 0.3575 - val\_loss: 7.8322 - val\_acc: 0.4107
Epoch 18/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.9659 - acc: 0.3659 - val\_loss: 7.9527 - val\_acc: 0.4097
Epoch 19/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.9298 - acc: 0.3689 - val\_loss: 8.0521 - val\_acc: 0.4095
Epoch 20/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.8700 - acc: 0.3682 - val\_loss: 8.0344 - val\_acc: 0.4108
Epoch 21/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.8100 - acc: 0.3703 - val\_loss: 8.1334 - val\_acc: 0.4071
Epoch 22/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.8446 - acc: 0.3718 - val\_loss: 8.1473 - val\_acc: 0.4117
Epoch 23/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.8112 - acc: 0.3726 - val\_loss: 8.0741 - val\_acc: 0.4118
Epoch 24/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.7642 - acc: 0.3762 - val\_loss: 8.0737 - val\_acc: 0.4117
Epoch 25/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.7519 - acc: 0.3739 - val\_loss: 8.0117 - val\_acc: 0.4133
Epoch 26/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.7161 - acc: 0.3769 - val\_loss: 8.0701 - val\_acc: 0.4086
Epoch 27/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.7691 - acc: 0.3749 - val\_loss: 8.0029 - val\_acc: 0.4119
Epoch 28/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.6624 - acc: 0.3797 - val\_loss: 7.9449 - val\_acc: 0.4128
Epoch 29/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.6584 - acc: 0.3799 - val\_loss: 7.9656 - val\_acc: 0.4126
Epoch 30/200
35064/35064 [==============================] - 74s 2ms/step - loss: 8.6271 - acc: 0.3808 - val\_loss: 7.8686 - val\_acc: 0.4151
Epoch 31/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.6601 - acc: 0.3798 - val\_loss: 7.7613 - val\_acc: 0.4170
Epoch 32/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.6334 - acc: 0.3805 - val\_loss: 7.8168 - val\_acc: 0.4162
Epoch 33/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.5618 - acc: 0.3835 - val\_loss: 7.7898 - val\_acc: 0.4165
Epoch 34/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.5761 - acc: 0.3817 - val\_loss: 7.7067 - val\_acc: 0.4189
Epoch 35/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.5850 - acc: 0.3820 - val\_loss: 7.7210 - val\_acc: 0.4191
Epoch 36/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.5518 - acc: 0.3846 - val\_loss: 7.7089 - val\_acc: 0.4180
Epoch 37/200
35064/35064 [==============================] - 74s 2ms/step - loss: 8.5041 - acc: 0.3855 - val\_loss: 7.7121 - val\_acc: 0.4179
Epoch 38/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.4702 - acc: 0.3886 - val\_loss: 7.6222 - val\_acc: 0.4207
Epoch 39/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.4148 - acc: 0.3880 - val\_loss: 7.6195 - val\_acc: 0.4211
Epoch 40/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.5020 - acc: 0.3838 - val\_loss: 7.6067 - val\_acc: 0.4212
Epoch 41/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.3455 - acc: 0.3941 - val\_loss: 7.5800 - val\_acc: 0.4206
Epoch 42/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.3918 - acc: 0.3868 - val\_loss: 7.5049 - val\_acc: 0.4225
Epoch 43/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.3447 - acc: 0.3907 - val\_loss: 7.4576 - val\_acc: 0.4233
Epoch 44/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.3918 - acc: 0.3865 - val\_loss: 7.4413 - val\_acc: 0.4223
Epoch 45/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.3538 - acc: 0.3895 - val\_loss: 7.4221 - val\_acc: 0.4215
Epoch 46/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.3219 - acc: 0.3897 - val\_loss: 7.4167 - val\_acc: 0.4215
Epoch 47/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.3119 - acc: 0.3901 - val\_loss: 7.3900 - val\_acc: 0.4230
Epoch 48/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.3319 - acc: 0.3881 - val\_loss: 7.3728 - val\_acc: 0.4224
Epoch 49/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.2664 - acc: 0.3927 - val\_loss: 7.3666 - val\_acc: 0.4188
Epoch 50/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.2266 - acc: 0.3941 - val\_loss: 7.3529 - val\_acc: 0.4221
Epoch 51/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.2384 - acc: 0.3922 - val\_loss: 7.3916 - val\_acc: 0.4191
Epoch 52/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.2302 - acc: 0.3928 - val\_loss: 7.4403 - val\_acc: 0.4189
Epoch 53/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.2562 - acc: 0.3900 - val\_loss: 7.3613 - val\_acc: 0.4221
Epoch 54/200
35064/35064 [==============================] - 74s 2ms/step - loss: 8.2477 - acc: 0.3917 - val\_loss: 7.3398 - val\_acc: 0.4233
Epoch 55/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.1667 - acc: 0.3975 - val\_loss: 7.2875 - val\_acc: 0.4249
Epoch 56/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.1405 - acc: 0.3950 - val\_loss: 7.3073 - val\_acc: 0.4228
Epoch 57/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.1751 - acc: 0.3934 - val\_loss: 7.3152 - val\_acc: 0.4214
Epoch 58/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.0890 - acc: 0.3992 - val\_loss: 7.2327 - val\_acc: 0.4236
Epoch 59/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.1517 - acc: 0.3936 - val\_loss: 7.1894 - val\_acc: 0.4241
Epoch 60/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.0712 - acc: 0.3974 - val\_loss: 7.2434 - val\_acc: 0.4204
Epoch 61/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.0408 - acc: 0.4005 - val\_loss: 7.1676 - val\_acc: 0.4239
Epoch 62/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.0265 - acc: 0.3992 - val\_loss: 7.1511 - val\_acc: 0.4221
Epoch 63/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.0378 - acc: 0.3960 - val\_loss: 7.1471 - val\_acc: 0.4228
Epoch 64/200
35064/35064 [==============================] - 73s 2ms/step - loss: 8.0231 - acc: 0.3963 - val\_loss: 7.0447 - val\_acc: 0.4254
Epoch 65/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.9546 - acc: 0.4023 - val\_loss: 7.0656 - val\_acc: 0.4223
Epoch 66/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.9291 - acc: 0.4023 - val\_loss: 7.2091 - val\_acc: 0.4160
Epoch 67/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.9284 - acc: 0.4040 - val\_loss: 7.2051 - val\_acc: 0.4172
Epoch 68/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.8612 - acc: 0.4033 - val\_loss: 7.1741 - val\_acc: 0.4179
Epoch 69/200
35064/35064 [==============================] - 74s 2ms/step - loss: 7.9516 - acc: 0.4003 - val\_loss: 7.1447 - val\_acc: 0.4182
Epoch 70/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.8561 - acc: 0.4032 - val\_loss: 7.0666 - val\_acc: 0.4213
Epoch 71/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.8109 - acc: 0.4034 - val\_loss: 7.0661 - val\_acc: 0.4213
Epoch 72/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.8397 - acc: 0.4053 - val\_loss: 7.0762 - val\_acc: 0.4206
Epoch 73/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.7573 - acc: 0.4094 - val\_loss: 7.0555 - val\_acc: 0.4206
Epoch 74/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.7515 - acc: 0.4085 - val\_loss: 7.0275 - val\_acc: 0.4201
Epoch 75/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.7695 - acc: 0.4053 - val\_loss: 6.9691 - val\_acc: 0.4206
Epoch 76/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.7153 - acc: 0.4104 - val\_loss: 6.9961 - val\_acc: 0.4217
Epoch 77/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.6460 - acc: 0.4137 - val\_loss: 7.0302 - val\_acc: 0.4192
Epoch 78/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.6700 - acc: 0.4097 - val\_loss: 6.9756 - val\_acc: 0.4212
Epoch 79/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.6310 - acc: 0.4122 - val\_loss: 6.9818 - val\_acc: 0.4180
Epoch 80/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.6759 - acc: 0.4067 - val\_loss: 6.9437 - val\_acc: 0.4190
Epoch 81/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.7020 - acc: 0.4065 - val\_loss: 6.9210 - val\_acc: 0.4193
Epoch 82/200
35064/35064 [==============================] - 74s 2ms/step - loss: 7.5809 - acc: 0.4124 - val\_loss: 6.9338 - val\_acc: 0.4167
Epoch 83/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.5271 - acc: 0.4155 - val\_loss: 6.8254 - val\_acc: 0.4238
Epoch 84/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.5823 - acc: 0.4115 - val\_loss: 6.8169 - val\_acc: 0.4231
Epoch 85/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.4953 - acc: 0.4158 - val\_loss: 6.8431 - val\_acc: 0.4200
Epoch 86/200
35064/35064 [==============================] - 74s 2ms/step - loss: 7.5338 - acc: 0.4119 - val\_loss: 6.7274 - val\_acc: 0.4247
Epoch 87/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.5285 - acc: 0.4095 - val\_loss: 6.7456 - val\_acc: 0.4224
Epoch 88/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.4372 - acc: 0.4155 - val\_loss: 6.6853 - val\_acc: 0.4240
Epoch 89/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.4424 - acc: 0.4137 - val\_loss: 6.7633 - val\_acc: 0.4187
Epoch 90/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.4324 - acc: 0.4144 - val\_loss: 6.6851 - val\_acc: 0.4221
Epoch 91/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.3362 - acc: 0.4173 - val\_loss: 6.6400 - val\_acc: 0.4246
Epoch 92/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.3153 - acc: 0.4178 - val\_loss: 6.6868 - val\_acc: 0.4212
Epoch 93/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.3105 - acc: 0.4180 - val\_loss: 6.7688 - val\_acc: 0.4184
Epoch 94/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.3547 - acc: 0.4141 - val\_loss: 6.6405 - val\_acc: 0.4229
Epoch 95/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.2778 - acc: 0.4184 - val\_loss: 6.5618 - val\_acc: 0.4258
Epoch 96/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.2021 - acc: 0.4239 - val\_loss: 6.5746 - val\_acc: 0.4257
Epoch 97/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.2131 - acc: 0.4205 - val\_loss: 6.4716 - val\_acc: 0.4302
Epoch 98/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.2115 - acc: 0.4211 - val\_loss: 6.4713 - val\_acc: 0.4314
Epoch 99/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.1904 - acc: 0.4231 - val\_loss: 6.4836 - val\_acc: 0.4285
Epoch 100/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.1679 - acc: 0.4208 - val\_loss: 6.5972 - val\_acc: 0.4236
Epoch 101/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.1659 - acc: 0.4220 - val\_loss: 6.5620 - val\_acc: 0.4247
Epoch 102/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.1046 - acc: 0.4228 - val\_loss: 6.4932 - val\_acc: 0.4266
Epoch 103/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.1681 - acc: 0.4199 - val\_loss: 6.5784 - val\_acc: 0.4235
Epoch 104/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.0372 - acc: 0.4274 - val\_loss: 6.4987 - val\_acc: 0.4263
Epoch 105/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.0817 - acc: 0.4239 - val\_loss: 6.4766 - val\_acc: 0.4270
Epoch 106/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.0556 - acc: 0.4243 - val\_loss: 6.4960 - val\_acc: 0.4248
Epoch 107/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.0333 - acc: 0.4253 - val\_loss: 6.4999 - val\_acc: 0.4249
Epoch 108/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.0057 - acc: 0.4260 - val\_loss: 6.4511 - val\_acc: 0.4247
Epoch 109/200
35064/35064 [==============================] - 74s 2ms/step - loss: 7.0519 - acc: 0.4205 - val\_loss: 6.4111 - val\_acc: 0.4274
Epoch 110/200
35064/35064 [==============================] - 73s 2ms/step - loss: 7.0031 - acc: 0.4277 - val\_loss: 6.3798 - val\_acc: 0.4289
Epoch 111/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.9878 - acc: 0.4245 - val\_loss: 6.3823 - val\_acc: 0.4278
Epoch 112/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.9700 - acc: 0.4278 - val\_loss: 6.3218 - val\_acc: 0.4320
Epoch 113/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.9345 - acc: 0.4275 - val\_loss: 6.2951 - val\_acc: 0.4329
Epoch 114/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.9053 - acc: 0.4298 - val\_loss: 6.2418 - val\_acc: 0.4358
Epoch 115/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.8435 - acc: 0.4333 - val\_loss: 6.3017 - val\_acc: 0.4301
Epoch 116/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.8614 - acc: 0.4287 - val\_loss: 6.2944 - val\_acc: 0.4306
Epoch 117/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.8419 - acc: 0.4290 - val\_loss: 6.2735 - val\_acc: 0.4311
Epoch 118/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.8161 - acc: 0.4308 - val\_loss: 6.2689 - val\_acc: 0.4309
Epoch 119/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.8078 - acc: 0.4296 - val\_loss: 6.2723 - val\_acc: 0.4316
Epoch 120/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.7715 - acc: 0.4319 - val\_loss: 6.2427 - val\_acc: 0.4318
Epoch 121/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.7687 - acc: 0.4320 - val\_loss: 6.1922 - val\_acc: 0.4347
Epoch 122/200
35064/35064 [==============================] - 74s 2ms/step - loss: 6.7708 - acc: 0.4283 - val\_loss: 6.2226 - val\_acc: 0.4313
Epoch 123/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.7666 - acc: 0.4302 - val\_loss: 6.1811 - val\_acc: 0.4328
Epoch 124/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.7185 - acc: 0.4304 - val\_loss: 6.2602 - val\_acc: 0.4292
Epoch 125/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.7037 - acc: 0.4290 - val\_loss: 6.1456 - val\_acc: 0.4328
Epoch 126/200
35064/35064 [==============================] - 72s 2ms/step - loss: 6.6569 - acc: 0.4335 - val\_loss: 6.1424 - val\_acc: 0.4321
Epoch 127/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.6840 - acc: 0.4321 - val\_loss: 6.0877 - val\_acc: 0.4351
Epoch 128/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.6493 - acc: 0.4308 - val\_loss: 6.0428 - val\_acc: 0.4366
Epoch 129/200
35064/35064 [==============================] - 72s 2ms/step - loss: 6.5788 - acc: 0.4343 - val\_loss: 6.0586 - val\_acc: 0.4360
Epoch 130/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.5795 - acc: 0.4351 - val\_loss: 5.9481 - val\_acc: 0.4399
Epoch 131/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.6112 - acc: 0.4290 - val\_loss: 5.9804 - val\_acc: 0.4382
Epoch 132/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.5433 - acc: 0.4346 - val\_loss: 6.0443 - val\_acc: 0.4361
Epoch 133/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.5570 - acc: 0.4339 - val\_loss: 5.9737 - val\_acc: 0.4385
Epoch 134/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.5298 - acc: 0.4330 - val\_loss: 5.9547 - val\_acc: 0.4383
Epoch 135/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.4763 - acc: 0.4365 - val\_loss: 5.9657 - val\_acc: 0.4367
Epoch 136/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.5035 - acc: 0.4343 - val\_loss: 5.9733 - val\_acc: 0.4379
Epoch 137/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.4611 - acc: 0.4336 - val\_loss: 5.9963 - val\_acc: 0.4367
Epoch 138/200
35064/35064 [==============================] - 72s 2ms/step - loss: 6.3923 - acc: 0.4408 - val\_loss: 5.9386 - val\_acc: 0.4369
Epoch 139/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.4355 - acc: 0.4348 - val\_loss: 5.8969 - val\_acc: 0.4382
Epoch 140/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.3936 - acc: 0.4378 - val\_loss: 5.8946 - val\_acc: 0.4369
Epoch 141/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.3481 - acc: 0.4405 - val\_loss: 5.8723 - val\_acc: 0.4378
Epoch 142/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.3835 - acc: 0.4376 - val\_loss: 5.8768 - val\_acc: 0.4385
Epoch 143/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.3731 - acc: 0.4327 - val\_loss: 5.8136 - val\_acc: 0.4398
Epoch 144/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.3361 - acc: 0.4345 - val\_loss: 5.8061 - val\_acc: 0.4389
Epoch 145/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.2719 - acc: 0.4384 - val\_loss: 5.8402 - val\_acc: 0.4361
Epoch 146/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.2715 - acc: 0.4369 - val\_loss: 5.7594 - val\_acc: 0.4391
Epoch 147/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.2878 - acc: 0.4376 - val\_loss: 5.7429 - val\_acc: 0.4395
Epoch 148/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.2266 - acc: 0.4381 - val\_loss: 5.7051 - val\_acc: 0.4400
Epoch 149/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.1977 - acc: 0.4418 - val\_loss: 5.7496 - val\_acc: 0.4368
Epoch 150/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.2208 - acc: 0.4390 - val\_loss: 5.6653 - val\_acc: 0.4427
Epoch 151/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.1482 - acc: 0.4390 - val\_loss: 5.7342 - val\_acc: 0.4386
Epoch 152/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.1298 - acc: 0.4409 - val\_loss: 5.6518 - val\_acc: 0.4435
Epoch 153/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.0956 - acc: 0.4428 - val\_loss: 5.6564 - val\_acc: 0.4401
Epoch 154/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.1196 - acc: 0.4403 - val\_loss: 5.6064 - val\_acc: 0.4436
Epoch 155/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.0647 - acc: 0.4442 - val\_loss: 5.6082 - val\_acc: 0.4428
Epoch 156/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.0864 - acc: 0.4384 - val\_loss: 5.5752 - val\_acc: 0.4447
Epoch 157/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.0265 - acc: 0.4409 - val\_loss: 5.6043 - val\_acc: 0.4406
Epoch 158/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.0089 - acc: 0.4447 - val\_loss: 5.5137 - val\_acc: 0.4459
Epoch 159/200
35064/35064 [==============================] - 72s 2ms/step - loss: 5.9969 - acc: 0.4421 - val\_loss: 5.4684 - val\_acc: 0.4466
Epoch 160/200
35064/35064 [==============================] - 73s 2ms/step - loss: 6.0074 - acc: 0.4439 - val\_loss: 5.4479 - val\_acc: 0.4472
Epoch 161/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.9763 - acc: 0.4388 - val\_loss: 5.4098 - val\_acc: 0.4491
Epoch 162/200
35064/35064 [==============================] - 74s 2ms/step - loss: 5.9244 - acc: 0.4426 - val\_loss: 5.4296 - val\_acc: 0.4463
Epoch 163/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.9470 - acc: 0.4436 - val\_loss: 5.4373 - val\_acc: 0.4426
Epoch 164/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.9109 - acc: 0.4414 - val\_loss: 5.3649 - val\_acc: 0.4481
Epoch 165/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.9013 - acc: 0.4416 - val\_loss: 5.3689 - val\_acc: 0.4463
Epoch 166/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.8589 - acc: 0.4416 - val\_loss: 5.3643 - val\_acc: 0.4446
Epoch 167/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.8145 - acc: 0.4468 - val\_loss: 5.3701 - val\_acc: 0.4427
Epoch 168/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.8171 - acc: 0.4458 - val\_loss: 5.3531 - val\_acc: 0.4427
Epoch 169/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.8160 - acc: 0.4437 - val\_loss: 5.3901 - val\_acc: 0.4399
Epoch 170/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.8096 - acc: 0.4418 - val\_loss: 5.2852 - val\_acc: 0.4452
Epoch 171/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.7554 - acc: 0.4420 - val\_loss: 5.2948 - val\_acc: 0.4444
Epoch 172/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.7207 - acc: 0.4415 - val\_loss: 5.2855 - val\_acc: 0.4438
Epoch 173/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.7235 - acc: 0.4463 - val\_loss: 5.2406 - val\_acc: 0.4465
Epoch 174/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.7153 - acc: 0.4436 - val\_loss: 5.2501 - val\_acc: 0.4444
Epoch 175/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.6512 - acc: 0.4465 - val\_loss: 5.2237 - val\_acc: 0.4452
Epoch 176/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.6174 - acc: 0.4485 - val\_loss: 5.2205 - val\_acc: 0.4436
Epoch 177/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.6028 - acc: 0.4455 - val\_loss: 5.1452 - val\_acc: 0.4484
Epoch 178/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.6454 - acc: 0.4405 - val\_loss: 5.1806 - val\_acc: 0.4425
Epoch 179/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.6373 - acc: 0.4420 - val\_loss: 5.1237 - val\_acc: 0.4452
Epoch 180/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.5992 - acc: 0.4453 - val\_loss: 5.1113 - val\_acc: 0.4450
Epoch 181/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.5571 - acc: 0.4445 - val\_loss: 5.0725 - val\_acc: 0.4478
Epoch 182/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.5428 - acc: 0.4435 - val\_loss: 5.0376 - val\_acc: 0.4487
Epoch 183/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.5746 - acc: 0.4443 - val\_loss: 5.0414 - val\_acc: 0.4472
Epoch 184/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.5087 - acc: 0.4451 - val\_loss: 5.0928 - val\_acc: 0.4427
Epoch 185/200
35064/35064 [==============================] - 73s 2ms/step - loss: 5.4651 - acc: 0.4443 - val\_loss: 5.0594 - val\_acc: 0.4443
Epoch 186/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.5056 - acc: 0.4406 - val\_loss: 5.0051 - val\_acc: 0.4472
Epoch 187/200
35064/35064 [==============================] - 76s 2ms/step - loss: 5.4418 - acc: 0.4429 - val\_loss: 4.9621 - val\_acc: 0.4492
Epoch 188/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.4262 - acc: 0.4461 - val\_loss: 4.9678 - val\_acc: 0.4467
Epoch 189/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.4243 - acc: 0.4466 - val\_loss: 4.9785 - val\_acc: 0.4471
Epoch 190/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.4174 - acc: 0.4458 - val\_loss: 4.9527 - val\_acc: 0.4473
Epoch 191/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.4235 - acc: 0.4418 - val\_loss: 4.9274 - val\_acc: 0.4478
Epoch 192/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.4168 - acc: 0.4412 - val\_loss: 4.9183 - val\_acc: 0.4482
Epoch 193/200
35064/35064 [==============================] - 74s 2ms/step - loss: 5.3655 - acc: 0.4448 - val\_loss: 4.8932 - val\_acc: 0.4483
Epoch 194/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.3227 - acc: 0.4478 - val\_loss: 4.9196 - val\_acc: 0.4476
Epoch 195/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.3236 - acc: 0.4423 - val\_loss: 4.8498 - val\_acc: 0.4493
Epoch 196/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.3595 - acc: 0.4419 - val\_loss: 4.8179 - val\_acc: 0.4505
Epoch 197/200
35064/35064 [==============================] - 76s 2ms/step - loss: 5.2855 - acc: 0.4458 - val\_loss: 4.8181 - val\_acc: 0.4486
Epoch 198/200
35064/35064 [==============================] - 76s 2ms/step - loss: 5.2472 - acc: 0.4452 - val\_loss: 4.8069 - val\_acc: 0.4488
Epoch 199/200
35064/35064 [==============================] - 75s 2ms/step - loss: 5.1955 - acc: 0.4486 - val\_loss: 4.7521 - val\_acc: 0.4511
Epoch 200/200
35064/35064 [==============================] - 76s 2ms/step - loss: 5.2224 - acc: 0.4440 - val\_loss: 4.7305 - val\_acc: 0.4524
results for adam  1e-06
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_40 (InputLayer)           (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_41 (InputLayer)           (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_42 (InputLayer)           (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_25 (Model)                (None, 280)          1525        input\_40[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_26 (Model)                (None, 100)          1125        input\_41[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_27 (Model)                (None, 100)          1175        input\_42[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_7 (Concatenate)     (None, 480)          0           model\_25[1][0]                   
                                                                 model\_26[1][0]                   
                                                                 model\_27[1][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_19 (Dense)                (None, 500)          240500      concatenate\_7[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_7 (Dropout)             (None, 500)          0           dense\_19[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_20 (Dense)                (None, 500)          250500      dropout\_7[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_21 (Dense)                (None, 5)            2505        dense\_20[0][0]                   
==================================================================================================
Total params: 497,330
Trainable params: 497,330
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_17.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_18.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.45242984257357977
kappa:  0.20611071058287544

    \end{Verbatim}

    Different architecture - sub convolution model for eeg, pulse and
accelerometer have only one convolution layer and a maxPool layer - eeg:
nb\_filter1=20, kernel1=25, maxPool1=20 - pulse \& accelerometers:
nb\_filter1=20, kernel1=10, maxPool1=8

\begin{figure}
\centering
\includegraphics{attachment:submodel_eeg2.png}
\caption{submodel\_eeg2.png}
\end{figure}

\begin{itemize}
\tightlist
\item
  submodel out put are not flatten but added 
\item
  output is feed in a 1D convolution layer (nb\_filter = 20,
  kernel\_size=10) and a maxPool layer (10)
\item
  a flatten layer
\item
  two dense layer with 500 neurons
\item
  output softmax layer is regularized with l2(0.01)
\end{itemize}

\begin{figure}
\centering
\includegraphics{attachment:mergeModel2.png}
\caption{mergeModel2.png}
\end{figure}

 Model performance: - accurancy: 0.65 - kappa: 0.51 Dispite l2
regularisation there is overfitting
\includegraphics{attachment:mergeModeladd_l2_acc.png}

\begin{figure}
\centering
\includegraphics{attachment:mergeModeladd_l2_loss.png}
\caption{mergeModeladd\_l2\_loss.png}
\end{figure}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} three 1D conv subnet are conv1}
         \PY{c+c1}{\PYZsh{}maxPool args are choosen so once flatten their output have the same shapes}
         \PY{c+c1}{\PYZsh{}then outputs are added }
         \PY{c+c1}{\PYZsh{}using L2 reg}
         
         \PY{n}{tensor\PYZus{}board} \PY{o}{=} \PY{n}{TensorBoard}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./logs/add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{20}
         \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{20}
         
         \PY{n}{model\PYZus{}eeg} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
         \PY{n}{model\PYZus{}pulse} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,}\PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
         \PY{n}{model\PYZus{}acc} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}print(\PYZdq{}eeg\PYZdq{})}
         \PY{n}{model\PYZus{}eeg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}print(\PYZdq{}pulse\PYZdq{})}
         \PY{n}{model\PYZus{}pulse}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}print(\PYZdq{}acc\PYZdq{})}
         \PY{n}{model\PYZus{}acc}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{in\PYZus{}eeg} \PY{o}{=}  \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}pulse} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}acc} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{accelerometer\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{out\PYZus{}eeg} \PY{o}{=} \PY{n}{model\PYZus{}eeg}\PY{p}{(}\PY{n}{in\PYZus{}eeg}\PY{p}{)}
         \PY{n}{out\PYZus{}pulse} \PY{o}{=} \PY{n}{model\PYZus{}pulse}\PY{p}{(}\PY{n}{in\PYZus{}pulse}\PY{p}{)}
         \PY{n}{out\PYZus{}acc} \PY{o}{=} \PY{n}{model\PYZus{}acc}\PY{p}{(}\PY{n}{in\PYZus{}acc}\PY{p}{)}
         
         \PY{n}{added} \PY{o}{=} \PY{n}{add}\PY{p}{(}\PY{p}{[}\PY{n}{out\PYZus{}eeg}\PY{p}{,} \PY{n}{out\PYZus{}pulse}\PY{p}{,} \PY{n}{out\PYZus{}acc}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{conv3} \PY{o}{=} \PY{n}{Conv1D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{n}{nb\PYZus{}filter2}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{added}\PY{p}{)}
         \PY{n}{pool3} \PY{o}{=} \PY{n}{MaxPooling1D}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{(}\PY{n}{conv3}\PY{p}{)}
         
         \PY{n}{flat} \PY{o}{=} \PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{(}\PY{n}{pool3}\PY{p}{)}
         
         \PY{n}{dense1} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{flat}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}drop1 = Dropout(0.5)(dense1)}
         
         \PY{c+c1}{\PYZsh{}dense2 = Dense(500, activation=\PYZsq{}relu\PYZsq{})(dense1)}
         \PY{n}{dense2} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kernel\PYZus{}regularizer}\PY{o}{=}\PY{n}{regularizers}\PY{o}{.}\PY{n}{l2}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n}{dense1}\PY{p}{)}
         \PY{n}{out} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense2}\PY{p}{)}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{p}{[}\PY{n}{in\PYZus{}eeg}\PY{p}{,} \PY{n}{in\PYZus{}pulse}\PY{p}{,} \PY{n}{in\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{out}\PY{p}{)}
         
         
         \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=20)`
  app.launch\_new\_instance()
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  app.launch\_new\_instance()
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`
  app.launch\_new\_instance()
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:33: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=20)`

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
input\_169 (InputLayer)       (None, 3750, 4)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_175 (Conv1D)          (None, 3726, 20)          2020      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_169 (MaxPoolin (None, 186, 20)           0         
=================================================================
Total params: 2,020
Trainable params: 2,020
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
input\_170 (InputLayer)       (None, 1500, 2)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_176 (Conv1D)          (None, 1491, 20)          420       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_170 (MaxPoolin (None, 186, 20)           0         
=================================================================
Total params: 420
Trainable params: 420
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
input\_171 (InputLayer)       (None, 1500, 3)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_177 (Conv1D)          (None, 1491, 20)          620       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_171 (MaxPoolin (None, 186, 20)           0         
=================================================================
Total params: 620
Trainable params: 620
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_172 (InputLayer)          (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_173 (InputLayer)          (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_174 (InputLayer)          (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_106 (Model)               (None, 186, 20)      2020        input\_172[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_107 (Model)               (None, 186, 20)      420         input\_173[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_108 (Model)               (None, 186, 20)      620         input\_174[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_14 (Add)                    (None, 186, 20)      0           model\_106[1][0]                  
                                                                 model\_107[1][0]                  
                                                                 model\_108[1][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_178 (Conv1D)             (None, 177, 20)      4020        add\_14[0][0]                     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_172 (MaxPooling1D (None, 17, 20)       0           conv1d\_178[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_71 (Flatten)            (None, 340)          0           max\_pooling1d\_172[0][0]          
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_71 (Dense)                (None, 500)          170500      flatten\_71[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_72 (Dense)                (None, 500)          250500      dense\_71[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_73 (Dense)                (None, 5)            2505        dense\_72[0][0]                   
==================================================================================================
Total params: 430,585
Trainable params: 430,585
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{200}
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{epochs}\PY{o}{=} \PY{n}{epochs}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{tensor\PYZus{}board}\PY{p}{]}\PY{p}{)}
         \PY{n}{score} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test score: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test accuracy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/200
35064/35064 [==============================] - 123s 4ms/step - loss: 4.9882 - acc: 0.4257 - val\_loss: 3.9223 - val\_acc: 0.4464
Epoch 2/200
35064/35064 [==============================] - 116s 3ms/step - loss: 3.6079 - acc: 0.4540 - val\_loss: 3.7078 - val\_acc: 0.4752
Epoch 3/200
35064/35064 [==============================] - 116s 3ms/step - loss: 3.2156 - acc: 0.4640 - val\_loss: 3.1095 - val\_acc: 0.4757
Epoch 4/200
35064/35064 [==============================] - 115s 3ms/step - loss: 2.9232 - acc: 0.4886 - val\_loss: 2.9182 - val\_acc: 0.4993
Epoch 5/200
35064/35064 [==============================] - 113s 3ms/step - loss: 2.7711 - acc: 0.5007 - val\_loss: 3.0523 - val\_acc: 0.4659
Epoch 6/200
35064/35064 [==============================] - 113s 3ms/step - loss: 2.6123 - acc: 0.5058 - val\_loss: 2.5816 - val\_acc: 0.4966
Epoch 7/200
35064/35064 [==============================] - 113s 3ms/step - loss: 2.3859 - acc: 0.5267 - val\_loss: 2.6768 - val\_acc: 0.5155
Epoch 8/200
35064/35064 [==============================] - 114s 3ms/step - loss: 2.6180 - acc: 0.4943 - val\_loss: 2.5353 - val\_acc: 0.5001
Epoch 9/200
35064/35064 [==============================] - 113s 3ms/step - loss: 2.4636 - acc: 0.4931 - val\_loss: 2.5379 - val\_acc: 0.5003
Epoch 10/200
35064/35064 [==============================] - 113s 3ms/step - loss: 2.3650 - acc: 0.5090 - val\_loss: 2.5745 - val\_acc: 0.4854
Epoch 11/200
35064/35064 [==============================] - 112s 3ms/step - loss: 2.2168 - acc: 0.5308 - val\_loss: 2.4354 - val\_acc: 0.5244
Epoch 12/200
35064/35064 [==============================] - 112s 3ms/step - loss: 2.2624 - acc: 0.5374 - val\_loss: 2.3080 - val\_acc: 0.5322
Epoch 13/200
35064/35064 [==============================] - 113s 3ms/step - loss: 2.1791 - acc: 0.5327 - val\_loss: 2.4642 - val\_acc: 0.5128
Epoch 14/200
35064/35064 [==============================] - 113s 3ms/step - loss: 2.0637 - acc: 0.5496 - val\_loss: 2.2160 - val\_acc: 0.5440
Epoch 15/200
35064/35064 [==============================] - 112s 3ms/step - loss: 2.1196 - acc: 0.5503 - val\_loss: 2.5003 - val\_acc: 0.5094
Epoch 16/200
35064/35064 [==============================] - 113s 3ms/step - loss: 2.0923 - acc: 0.5576 - val\_loss: 2.1977 - val\_acc: 0.5499
Epoch 17/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.9477 - acc: 0.5730 - val\_loss: 2.1131 - val\_acc: 0.5463
Epoch 18/200
35064/35064 [==============================] - 112s 3ms/step - loss: 2.0966 - acc: 0.5578 - val\_loss: 2.1477 - val\_acc: 0.5637
Epoch 19/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.9462 - acc: 0.5804 - val\_loss: 2.1871 - val\_acc: 0.5598
Epoch 20/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.8580 - acc: 0.5880 - val\_loss: 2.0392 - val\_acc: 0.5813
Epoch 21/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.8148 - acc: 0.5894 - val\_loss: 1.9390 - val\_acc: 0.5873
Epoch 22/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.7950 - acc: 0.5909 - val\_loss: 1.9628 - val\_acc: 0.5853
Epoch 23/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.7900 - acc: 0.5933 - val\_loss: 1.9150 - val\_acc: 0.5874
Epoch 24/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.7264 - acc: 0.6071 - val\_loss: 1.8333 - val\_acc: 0.5894
Epoch 25/200
35064/35064 [==============================] - 111s 3ms/step - loss: 1.6894 - acc: 0.6101 - val\_loss: 1.8270 - val\_acc: 0.6008
Epoch 26/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.7402 - acc: 0.6002 - val\_loss: 1.9096 - val\_acc: 0.5902
Epoch 27/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.6652 - acc: 0.6142 - val\_loss: 1.8584 - val\_acc: 0.5958
Epoch 28/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.6914 - acc: 0.6138 - val\_loss: 1.8361 - val\_acc: 0.6027
Epoch 29/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.5773 - acc: 0.6275 - val\_loss: 1.7837 - val\_acc: 0.6078
Epoch 30/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.6481 - acc: 0.6196 - val\_loss: 1.8365 - val\_acc: 0.6004
Epoch 31/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.5682 - acc: 0.6261 - val\_loss: 1.7527 - val\_acc: 0.6097
Epoch 32/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.5208 - acc: 0.6347 - val\_loss: 1.7534 - val\_acc: 0.6109
Epoch 33/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.5586 - acc: 0.6248 - val\_loss: 1.8023 - val\_acc: 0.6007
Epoch 34/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.5795 - acc: 0.6279 - val\_loss: 1.7666 - val\_acc: 0.6135
Epoch 35/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.5110 - acc: 0.6329 - val\_loss: 1.7045 - val\_acc: 0.6238
Epoch 36/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.5166 - acc: 0.6337 - val\_loss: 1.7429 - val\_acc: 0.6157
Epoch 37/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.5704 - acc: 0.6235 - val\_loss: 1.8525 - val\_acc: 0.6170
Epoch 38/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.5336 - acc: 0.6378 - val\_loss: 1.8204 - val\_acc: 0.6085
Epoch 39/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.4886 - acc: 0.6388 - val\_loss: 1.7840 - val\_acc: 0.6198
Epoch 40/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.4232 - acc: 0.6477 - val\_loss: 1.7243 - val\_acc: 0.6206
Epoch 41/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.3935 - acc: 0.6525 - val\_loss: 1.7203 - val\_acc: 0.6256
Epoch 42/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.4439 - acc: 0.6484 - val\_loss: 1.7356 - val\_acc: 0.6205
Epoch 43/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.4168 - acc: 0.6506 - val\_loss: 1.7505 - val\_acc: 0.6211
Epoch 44/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.3796 - acc: 0.6598 - val\_loss: 1.7273 - val\_acc: 0.6232
Epoch 45/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.3223 - acc: 0.6628 - val\_loss: 1.6834 - val\_acc: 0.6313
Epoch 46/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2946 - acc: 0.6688 - val\_loss: 1.7187 - val\_acc: 0.6251
Epoch 47/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.3553 - acc: 0.6592 - val\_loss: 1.7102 - val\_acc: 0.6275
Epoch 48/200
35064/35064 [==============================] - 111s 3ms/step - loss: 1.4817 - acc: 0.6405 - val\_loss: 1.8110 - val\_acc: 0.6146
Epoch 49/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.4301 - acc: 0.6524 - val\_loss: 1.8381 - val\_acc: 0.6063
Epoch 50/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.3839 - acc: 0.6589 - val\_loss: 1.7931 - val\_acc: 0.6149
Epoch 51/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.3479 - acc: 0.6654 - val\_loss: 1.7049 - val\_acc: 0.6298
Epoch 52/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.3606 - acc: 0.6646 - val\_loss: 1.6703 - val\_acc: 0.6373
Epoch 53/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.2958 - acc: 0.6777 - val\_loss: 1.7464 - val\_acc: 0.6304
Epoch 54/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2748 - acc: 0.6772 - val\_loss: 1.7129 - val\_acc: 0.6332
Epoch 55/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2856 - acc: 0.6765 - val\_loss: 1.6817 - val\_acc: 0.6367
Epoch 56/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2513 - acc: 0.6826 - val\_loss: 1.6738 - val\_acc: 0.6407
Epoch 57/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2280 - acc: 0.6862 - val\_loss: 1.6609 - val\_acc: 0.6378
Epoch 58/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2646 - acc: 0.6819 - val\_loss: 1.7013 - val\_acc: 0.6338
Epoch 59/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.3323 - acc: 0.6743 - val\_loss: 1.7067 - val\_acc: 0.6413
Epoch 60/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.3415 - acc: 0.6736 - val\_loss: 1.7067 - val\_acc: 0.6397
Epoch 61/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.2797 - acc: 0.6842 - val\_loss: 1.6709 - val\_acc: 0.6459
Epoch 62/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2553 - acc: 0.6867 - val\_loss: 1.6951 - val\_acc: 0.6426
Epoch 63/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.2369 - acc: 0.6874 - val\_loss: 1.6592 - val\_acc: 0.6476
Epoch 64/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2395 - acc: 0.6881 - val\_loss: 1.6697 - val\_acc: 0.6453
Epoch 65/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2301 - acc: 0.6917 - val\_loss: 1.7838 - val\_acc: 0.6295
Epoch 66/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2955 - acc: 0.6803 - val\_loss: 1.7127 - val\_acc: 0.6410
Epoch 67/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2144 - acc: 0.6978 - val\_loss: 1.6847 - val\_acc: 0.6392
Epoch 68/200
35064/35064 [==============================] - 112s 3ms/step - loss: 1.2417 - acc: 0.6901 - val\_loss: 1.6770 - val\_acc: 0.6484
Epoch 69/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.1929 - acc: 0.7025 - val\_loss: 1.6541 - val\_acc: 0.6509
Epoch 70/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.1904 - acc: 0.7014 - val\_loss: 1.6838 - val\_acc: 0.6472
Epoch 71/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.2424 - acc: 0.6936 - val\_loss: 1.8151 - val\_acc: 0.6322
Epoch 72/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2640 - acc: 0.6829 - val\_loss: 1.7524 - val\_acc: 0.6302
Epoch 73/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2134 - acc: 0.6953 - val\_loss: 1.6519 - val\_acc: 0.6491
Epoch 74/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1741 - acc: 0.7064 - val\_loss: 1.6891 - val\_acc: 0.6472
Epoch 75/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.1906 - acc: 0.7053 - val\_loss: 1.7020 - val\_acc: 0.6462
Epoch 76/200
35064/35064 [==============================] - 115s 3ms/step - loss: 1.2206 - acc: 0.6999 - val\_loss: 1.6858 - val\_acc: 0.6520
Epoch 77/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1824 - acc: 0.7086 - val\_loss: 1.6159 - val\_acc: 0.6634
Epoch 78/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.1181 - acc: 0.7166 - val\_loss: 1.6934 - val\_acc: 0.6542
Epoch 79/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1169 - acc: 0.7183 - val\_loss: 1.6084 - val\_acc: 0.6605
Epoch 80/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.1126 - acc: 0.7192 - val\_loss: 1.6692 - val\_acc: 0.6506
Epoch 81/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.1627 - acc: 0.7082 - val\_loss: 1.6491 - val\_acc: 0.6526
Epoch 82/200
35064/35064 [==============================] - 116s 3ms/step - loss: 1.1573 - acc: 0.7105 - val\_loss: 1.6252 - val\_acc: 0.6586
Epoch 83/200
35064/35064 [==============================] - 117s 3ms/step - loss: 1.1086 - acc: 0.7225 - val\_loss: 1.6105 - val\_acc: 0.6572
Epoch 84/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1128 - acc: 0.7211 - val\_loss: 1.7421 - val\_acc: 0.6288
Epoch 85/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.2459 - acc: 0.7007 - val\_loss: 1.6849 - val\_acc: 0.6529
Epoch 86/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1801 - acc: 0.7153 - val\_loss: 1.6153 - val\_acc: 0.6566
Epoch 87/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1290 - acc: 0.7200 - val\_loss: 1.6032 - val\_acc: 0.6598
Epoch 88/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.0752 - acc: 0.7288 - val\_loss: 1.6209 - val\_acc: 0.6596
Epoch 89/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0629 - acc: 0.7323 - val\_loss: 1.6527 - val\_acc: 0.6531
Epoch 90/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0273 - acc: 0.7412 - val\_loss: 1.5839 - val\_acc: 0.6660
Epoch 91/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0339 - acc: 0.7380 - val\_loss: 1.6347 - val\_acc: 0.6601
Epoch 92/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1154 - acc: 0.7208 - val\_loss: 1.6468 - val\_acc: 0.6591
Epoch 93/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0597 - acc: 0.7322 - val\_loss: 1.6899 - val\_acc: 0.6554
Epoch 94/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.1511 - acc: 0.7171 - val\_loss: 1.6646 - val\_acc: 0.6618
Epoch 95/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1062 - acc: 0.7251 - val\_loss: 1.6398 - val\_acc: 0.6601
Epoch 96/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0489 - acc: 0.7366 - val\_loss: 1.6331 - val\_acc: 0.6635
Epoch 97/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0204 - acc: 0.7450 - val\_loss: 1.6558 - val\_acc: 0.6620
Epoch 98/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.0234 - acc: 0.7458 - val\_loss: 1.6328 - val\_acc: 0.6647
Epoch 99/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1411 - acc: 0.7342 - val\_loss: 1.8116 - val\_acc: 0.6452
Epoch 100/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1186 - acc: 0.7340 - val\_loss: 1.6901 - val\_acc: 0.6623
Epoch 101/200
35064/35064 [==============================] - 115s 3ms/step - loss: 1.0471 - acc: 0.7442 - val\_loss: 1.6552 - val\_acc: 0.6640
Epoch 102/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0049 - acc: 0.7523 - val\_loss: 1.6876 - val\_acc: 0.6569
Epoch 103/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.0136 - acc: 0.7478 - val\_loss: 1.6264 - val\_acc: 0.6647
Epoch 104/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.9928 - acc: 0.7539 - val\_loss: 1.6410 - val\_acc: 0.6632
Epoch 105/200
35064/35064 [==============================] - 115s 3ms/step - loss: 1.1684 - acc: 0.7252 - val\_loss: 1.7560 - val\_acc: 0.6514
Epoch 106/200
35064/35064 [==============================] - 113s 3ms/step - loss: 1.0598 - acc: 0.7390 - val\_loss: 1.6396 - val\_acc: 0.6615
Epoch 107/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9905 - acc: 0.7539 - val\_loss: 1.6920 - val\_acc: 0.6608
Epoch 108/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0761 - acc: 0.7395 - val\_loss: 1.7573 - val\_acc: 0.6513
Epoch 109/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0233 - acc: 0.7531 - val\_loss: 1.6666 - val\_acc: 0.6597
Epoch 110/200
35064/35064 [==============================] - 115s 3ms/step - loss: 0.9830 - acc: 0.7597 - val\_loss: 1.7036 - val\_acc: 0.6624
Epoch 111/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0041 - acc: 0.7598 - val\_loss: 1.7050 - val\_acc: 0.6614
Epoch 112/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.9928 - acc: 0.7608 - val\_loss: 1.7098 - val\_acc: 0.6582
Epoch 113/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.9965 - acc: 0.7608 - val\_loss: 1.7322 - val\_acc: 0.6636
Epoch 114/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9591 - acc: 0.7679 - val\_loss: 1.6724 - val\_acc: 0.6640
Epoch 115/200
35064/35064 [==============================] - 115s 3ms/step - loss: 0.9590 - acc: 0.7702 - val\_loss: 1.8669 - val\_acc: 0.6468
Epoch 116/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.1415 - acc: 0.7307 - val\_loss: 1.7408 - val\_acc: 0.6581
Epoch 117/200
35064/35064 [==============================] - 114s 3ms/step - loss: 1.0118 - acc: 0.7517 - val\_loss: 1.7245 - val\_acc: 0.6628
Epoch 118/200
35064/35064 [==============================] - 115s 3ms/step - loss: 1.0065 - acc: 0.7584 - val\_loss: 1.7121 - val\_acc: 0.6599
Epoch 119/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9715 - acc: 0.7639 - val\_loss: 1.7043 - val\_acc: 0.6631
Epoch 120/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9414 - acc: 0.7748 - val\_loss: 1.7150 - val\_acc: 0.6611
Epoch 121/200
35064/35064 [==============================] - 115s 3ms/step - loss: 0.9016 - acc: 0.7810 - val\_loss: 1.7178 - val\_acc: 0.6619
Epoch 122/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9204 - acc: 0.7750 - val\_loss: 1.6917 - val\_acc: 0.6616
Epoch 123/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8805 - acc: 0.7885 - val\_loss: 1.6879 - val\_acc: 0.6671
Epoch 124/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8743 - acc: 0.7880 - val\_loss: 1.6881 - val\_acc: 0.6677
Epoch 125/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9100 - acc: 0.7821 - val\_loss: 1.7613 - val\_acc: 0.6597
Epoch 126/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9530 - acc: 0.7728 - val\_loss: 1.7512 - val\_acc: 0.6629
Epoch 127/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9135 - acc: 0.7805 - val\_loss: 1.7686 - val\_acc: 0.6559
Epoch 128/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8998 - acc: 0.7810 - val\_loss: 1.8063 - val\_acc: 0.6571
Epoch 129/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8841 - acc: 0.7880 - val\_loss: 1.7833 - val\_acc: 0.6608
Epoch 130/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8495 - acc: 0.7984 - val\_loss: 1.7485 - val\_acc: 0.6648
Epoch 131/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9131 - acc: 0.7779 - val\_loss: 1.8115 - val\_acc: 0.6529
Epoch 132/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.9335 - acc: 0.7788 - val\_loss: 1.8323 - val\_acc: 0.6531
Epoch 133/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.9050 - acc: 0.7882 - val\_loss: 1.7402 - val\_acc: 0.6651
Epoch 134/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8488 - acc: 0.8040 - val\_loss: 1.7423 - val\_acc: 0.6681
Epoch 135/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8778 - acc: 0.7940 - val\_loss: 1.7639 - val\_acc: 0.6645
Epoch 136/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8937 - acc: 0.7882 - val\_loss: 1.7475 - val\_acc: 0.6671
Epoch 137/200
35064/35064 [==============================] - 115s 3ms/step - loss: 0.8865 - acc: 0.7900 - val\_loss: 1.8362 - val\_acc: 0.6542
Epoch 138/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8472 - acc: 0.8022 - val\_loss: 1.7675 - val\_acc: 0.6637
Epoch 139/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8246 - acc: 0.8089 - val\_loss: 1.7597 - val\_acc: 0.6644
Epoch 140/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8524 - acc: 0.8023 - val\_loss: 1.7879 - val\_acc: 0.6628
Epoch 141/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8294 - acc: 0.8067 - val\_loss: 1.7513 - val\_acc: 0.6622
Epoch 142/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8603 - acc: 0.7996 - val\_loss: 1.7865 - val\_acc: 0.6650
Epoch 143/200
35064/35064 [==============================] - 115s 3ms/step - loss: 0.9074 - acc: 0.7904 - val\_loss: 1.7590 - val\_acc: 0.6632
Epoch 144/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8387 - acc: 0.8087 - val\_loss: 1.8205 - val\_acc: 0.6653
Epoch 145/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7913 - acc: 0.8220 - val\_loss: 1.7693 - val\_acc: 0.6679
Epoch 146/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8244 - acc: 0.8137 - val\_loss: 1.8802 - val\_acc: 0.6493
Epoch 147/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.9173 - acc: 0.7962 - val\_loss: 1.8224 - val\_acc: 0.6604
Epoch 148/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8156 - acc: 0.8163 - val\_loss: 1.8229 - val\_acc: 0.6631
Epoch 149/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8020 - acc: 0.8210 - val\_loss: 1.8267 - val\_acc: 0.6646
Epoch 150/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.8629 - acc: 0.8076 - val\_loss: 1.8944 - val\_acc: 0.6577
Epoch 151/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8838 - acc: 0.8068 - val\_loss: 1.8188 - val\_acc: 0.6583
Epoch 152/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7894 - acc: 0.8268 - val\_loss: 1.8033 - val\_acc: 0.6606
Epoch 153/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.7892 - acc: 0.8288 - val\_loss: 1.8039 - val\_acc: 0.6615
Epoch 154/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.7979 - acc: 0.8273 - val\_loss: 1.8027 - val\_acc: 0.6585
Epoch 155/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7732 - acc: 0.8324 - val\_loss: 1.8152 - val\_acc: 0.6599
Epoch 156/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7507 - acc: 0.8387 - val\_loss: 1.8134 - val\_acc: 0.6604
Epoch 157/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7542 - acc: 0.8366 - val\_loss: 1.8587 - val\_acc: 0.6554
Epoch 158/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7988 - acc: 0.8260 - val\_loss: 1.8397 - val\_acc: 0.6610
Epoch 159/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8311 - acc: 0.8221 - val\_loss: 1.8372 - val\_acc: 0.6571
Epoch 160/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8017 - acc: 0.8267 - val\_loss: 1.8812 - val\_acc: 0.6494
Epoch 161/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7863 - acc: 0.8285 - val\_loss: 1.8890 - val\_acc: 0.6556
Epoch 162/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7629 - acc: 0.8380 - val\_loss: 1.8484 - val\_acc: 0.6577
Epoch 163/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7556 - acc: 0.8380 - val\_loss: 1.8384 - val\_acc: 0.6587
Epoch 164/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.7689 - acc: 0.8387 - val\_loss: 1.8611 - val\_acc: 0.6580
Epoch 165/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7718 - acc: 0.8319 - val\_loss: 1.8369 - val\_acc: 0.6599
Epoch 166/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7540 - acc: 0.8394 - val\_loss: 1.8573 - val\_acc: 0.6616
Epoch 167/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7359 - acc: 0.8463 - val\_loss: 1.9026 - val\_acc: 0.6597
Epoch 168/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7799 - acc: 0.8387 - val\_loss: 1.8774 - val\_acc: 0.6599
Epoch 169/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7557 - acc: 0.8423 - val\_loss: 1.8996 - val\_acc: 0.6534
Epoch 170/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7912 - acc: 0.8317 - val\_loss: 1.9439 - val\_acc: 0.6550
Epoch 171/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.8718 - acc: 0.8189 - val\_loss: 1.9523 - val\_acc: 0.6529
Epoch 172/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.8099 - acc: 0.8348 - val\_loss: 1.9301 - val\_acc: 0.6556
Epoch 173/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7772 - acc: 0.8359 - val\_loss: 1.9268 - val\_acc: 0.6530
Epoch 174/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.7491 - acc: 0.8480 - val\_loss: 1.8650 - val\_acc: 0.6575
Epoch 175/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7426 - acc: 0.8484 - val\_loss: 1.9080 - val\_acc: 0.6505
Epoch 176/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.7265 - acc: 0.8547 - val\_loss: 1.8827 - val\_acc: 0.6574
Epoch 177/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.7033 - acc: 0.8593 - val\_loss: 1.9143 - val\_acc: 0.6432
Epoch 178/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7029 - acc: 0.8597 - val\_loss: 1.9166 - val\_acc: 0.6530
Epoch 179/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7163 - acc: 0.8564 - val\_loss: 1.9202 - val\_acc: 0.6589
Epoch 180/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.6931 - acc: 0.8644 - val\_loss: 1.9393 - val\_acc: 0.6547
Epoch 181/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.6875 - acc: 0.8661 - val\_loss: 1.9356 - val\_acc: 0.6611
Epoch 182/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.6900 - acc: 0.8639 - val\_loss: 1.9039 - val\_acc: 0.6607
Epoch 183/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6654 - acc: 0.8748 - val\_loss: 1.9631 - val\_acc: 0.6465
Epoch 184/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7416 - acc: 0.8532 - val\_loss: 1.9474 - val\_acc: 0.6618
Epoch 185/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6902 - acc: 0.8666 - val\_loss: 1.9406 - val\_acc: 0.6630
Epoch 186/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6730 - acc: 0.8729 - val\_loss: 1.9511 - val\_acc: 0.6628
Epoch 187/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.6504 - acc: 0.8787 - val\_loss: 1.9871 - val\_acc: 0.6582
Epoch 188/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6534 - acc: 0.8803 - val\_loss: 2.0333 - val\_acc: 0.6526
Epoch 189/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.6902 - acc: 0.8736 - val\_loss: 2.0064 - val\_acc: 0.6531
Epoch 190/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.6306 - acc: 0.8869 - val\_loss: 2.0059 - val\_acc: 0.6527
Epoch 191/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.5970 - acc: 0.8973 - val\_loss: 2.0090 - val\_acc: 0.6530
Epoch 192/200
35064/35064 [==============================] - 114s 3ms/step - loss: 0.6485 - acc: 0.8842 - val\_loss: 2.0216 - val\_acc: 0.6580
Epoch 193/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6231 - acc: 0.8888 - val\_loss: 2.0435 - val\_acc: 0.6577
Epoch 194/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6530 - acc: 0.8866 - val\_loss: 2.0125 - val\_acc: 0.6590
Epoch 195/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6436 - acc: 0.8852 - val\_loss: 2.0886 - val\_acc: 0.6502
Epoch 196/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6286 - acc: 0.8881 - val\_loss: 2.0798 - val\_acc: 0.6513
Epoch 197/200
35064/35064 [==============================] - 112s 3ms/step - loss: 0.6380 - acc: 0.8839 - val\_loss: 2.0888 - val\_acc: 0.6512
Epoch 198/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.6243 - acc: 0.8892 - val\_loss: 2.0996 - val\_acc: 0.6515
Epoch 199/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.5995 - acc: 0.9001 - val\_loss: 2.1374 - val\_acc: 0.6452
Epoch 200/200
35064/35064 [==============================] - 113s 3ms/step - loss: 0.7140 - acc: 0.8649 - val\_loss: 2.1223 - val\_acc: 0.6465
8766/8766 [==============================] - 10s 1ms/step
Test score:  2.1222595506516218
Test accuracy:  0.6464750163500205

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{p}{,} \PY{n}{history}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{sequential}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_172 (InputLayer)          (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_173 (InputLayer)          (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_174 (InputLayer)          (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_106 (Model)               (None, 186, 20)      2020        input\_172[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_107 (Model)               (None, 186, 20)      420         input\_173[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_108 (Model)               (None, 186, 20)      620         input\_174[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_14 (Add)                    (None, 186, 20)      0           model\_106[1][0]                  
                                                                 model\_107[1][0]                  
                                                                 model\_108[1][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_178 (Conv1D)             (None, 177, 20)      4020        add\_14[0][0]                     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_172 (MaxPooling1D (None, 17, 20)       0           conv1d\_178[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_71 (Flatten)            (None, 340)          0           max\_pooling1d\_172[0][0]          
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_71 (Dense)                (None, 500)          170500      flatten\_71[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_72 (Dense)                (None, 500)          250500      dense\_71[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_73 (Dense)                (None, 5)            2505        dense\_72[0][0]                   
==================================================================================================
Total params: 430,585
Trainable params: 430,585
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.6464750171115674
kappa:  0.50809032129652

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Different architecture, rather than being added the submodel output are
concatenate on filters channels

\begin{itemize}
\tightlist
\item
  No change on sub convolution models for eeg, pulse and accelerometer
  have only one convolution layer and a maxPool layer

  \begin{itemize}
  \tightlist
  \item
    eeg: nb\_filter1=20, kernel1=25, maxPool1=20
  \item
    pulse \& accelerometers: nb\_filter1=20, kernel1=10, maxPool1=8
  \end{itemize}
\end{itemize}

\begin{itemize}
\tightlist
\item
  submodels output are not added but concatenate on filters axis (output
  will have 5 + 5 + 5 channels) 
\item
  output is feed in a 1D convolution layer (nb\_filter = 20,
  kernel\_size=10) and a maxPool layer (10)
\item
  a flatten layer
\item
  two dense layer with 500 neurons
\item
  no regularization 
\end{itemize}

\begin{figure}
\centering
\includegraphics{attachment:model_empile.png}
\caption{model\_empile.png}
\end{figure}

Model performance on 100 epochs - accurancy: 0.65 - kappa: 0.51

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} three 1D conv subnet are conv1}
         \PY{c+c1}{\PYZsh{}maxPool args are choosen so once flatten their output have the same shapes}
         \PY{c+c1}{\PYZsh{}then outputs are concatenate: there are n filters map per subnetwork \PYZhy{}\PYZgt{} concat has 3*n features map}
         
         \PY{c+c1}{\PYZsh{}with 5 * 5 * 5 filters on 100 epochs (light over fit after 80)}
         \PY{c+c1}{\PYZsh{}accurancy:  0.6536618754277892}
         \PY{c+c1}{\PYZsh{}kappa:  0.5117412469913332}
         
         \PY{n}{tensor\PYZus{}board} \PY{o}{=} \PY{n}{TensorBoard}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./logs/empile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{5}
         \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{5}
         
         \PY{n}{model\PYZus{}eeg} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
         \PY{n}{model\PYZus{}pulse} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,}\PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
         \PY{n}{model\PYZus{}acc} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}print(\PYZdq{}eeg\PYZdq{})}
         \PY{n}{model\PYZus{}eeg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}print(\PYZdq{}pulse\PYZdq{})}
         \PY{n}{model\PYZus{}pulse}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}print(\PYZdq{}acc\PYZdq{})}
         \PY{n}{model\PYZus{}acc}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{in\PYZus{}eeg} \PY{o}{=}  \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}pulse} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}acc} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{accelerometer\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{out\PYZus{}eeg} \PY{o}{=} \PY{n}{model\PYZus{}eeg}\PY{p}{(}\PY{n}{in\PYZus{}eeg}\PY{p}{)}
         \PY{n}{out\PYZus{}pulse} \PY{o}{=} \PY{n}{model\PYZus{}pulse}\PY{p}{(}\PY{n}{in\PYZus{}pulse}\PY{p}{)}
         \PY{n}{out\PYZus{}acc} \PY{o}{=} \PY{n}{model\PYZus{}acc}\PY{p}{(}\PY{n}{in\PYZus{}acc}\PY{p}{)}
         
         \PY{n}{merged} \PY{o}{=} \PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{out\PYZus{}eeg}\PY{p}{,} \PY{n}{out\PYZus{}pulse}\PY{p}{,} \PY{n}{out\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{conv3} \PY{o}{=} \PY{n}{Conv1D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{n}{nb\PYZus{}filter2}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{merged}\PY{p}{)}
         \PY{n}{pool3} \PY{o}{=} \PY{n}{MaxPooling1D}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{(}\PY{n}{conv3}\PY{p}{)}
         
         \PY{n}{flat} \PY{o}{=} \PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{(}\PY{n}{pool3}\PY{p}{)}
         \PY{n}{dense1} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{flat}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}drop1 = Dropout(0.5)(dense1)}
         \PY{n}{dense2} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense1}\PY{p}{)}
         \PY{n}{out} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense2}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{p}{[}\PY{n}{in\PYZus{}eeg}\PY{p}{,} \PY{n}{in\PYZus{}pulse}\PY{p}{,} \PY{n}{in\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{out}\PY{p}{)}
         
         
         \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}plot\PYZus{}model(model, to\PYZus{}file=\PYZsq{}model\PYZus{}empile.png\PYZsq{}, show\PYZus{}shapes=True, show\PYZus{}layer\PYZus{}names=True)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=25, activation="relu", strides=1, padding="valid", filters=5)`
  app.launch\_new\_instance()
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  app.launch\_new\_instance()
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`
  app.launch\_new\_instance()
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:33: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=10, activation="relu", strides=1, padding="valid", filters=5)`

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
input\_199 (InputLayer)       (None, 3750, 4)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_194 (Conv1D)          (None, 3726, 5)           505       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_188 (MaxPoolin (None, 186, 5)            0         
=================================================================
Total params: 505
Trainable params: 505
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
input\_200 (InputLayer)       (None, 1500, 2)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_195 (Conv1D)          (None, 1491, 5)           105       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_189 (MaxPoolin (None, 186, 5)            0         
=================================================================
Total params: 105
Trainable params: 105
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
input\_201 (InputLayer)       (None, 1500, 3)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_196 (Conv1D)          (None, 1491, 5)           155       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_190 (MaxPoolin (None, 186, 5)            0         
=================================================================
Total params: 155
Trainable params: 155
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_202 (InputLayer)          (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_203 (InputLayer)          (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_204 (InputLayer)          (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_125 (Model)               (None, 186, 5)       505         input\_202[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_126 (Model)               (None, 186, 5)       105         input\_203[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_127 (Model)               (None, 186, 5)       155         input\_204[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_17 (Concatenate)    (None, 186, 15)      0           model\_125[1][0]                  
                                                                 model\_126[1][0]                  
                                                                 model\_127[1][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_197 (Conv1D)             (None, 177, 5)       755         concatenate\_17[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_191 (MaxPooling1D (None, 17, 5)        0           conv1d\_197[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_75 (Flatten)            (None, 85)           0           max\_pooling1d\_191[0][0]          
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_83 (Dense)                (None, 500)          43000       flatten\_75[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_84 (Dense)                (None, 500)          250500      dense\_83[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_85 (Dense)                (None, 5)            2505        dense\_84[0][0]                   
==================================================================================================
Total params: 297,525
Trainable params: 297,525
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{100}
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{epochs}\PY{o}{=} \PY{n}{epochs}\PY{p}{,} 
                             \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{tensor\PYZus{}board}\PY{p}{]}\PY{p}{)}
         \PY{n}{score} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test score: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test accuracy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/100
35064/35064 [==============================] - 59s 2ms/step - loss: 2.6970 - acc: 0.4271 - val\_loss: 2.7810 - val\_acc: 0.4403
Epoch 2/100
35064/35064 [==============================] - 59s 2ms/step - loss: 2.6393 - acc: 0.4342 - val\_loss: 2.6297 - val\_acc: 0.4305
Epoch 3/100
35064/35064 [==============================] - 59s 2ms/step - loss: 2.5038 - acc: 0.4474 - val\_loss: 2.4424 - val\_acc: 0.4584
Epoch 4/100
35064/35064 [==============================] - 59s 2ms/step - loss: 2.2955 - acc: 0.4678 - val\_loss: 2.3898 - val\_acc: 0.4602
Epoch 5/100
35064/35064 [==============================] - 59s 2ms/step - loss: 2.1564 - acc: 0.4908 - val\_loss: 2.2408 - val\_acc: 0.4897
Epoch 6/100
35064/35064 [==============================] - 58s 2ms/step - loss: 2.1080 - acc: 0.4949 - val\_loss: 2.1631 - val\_acc: 0.4944
Epoch 7/100
35064/35064 [==============================] - 59s 2ms/step - loss: 2.0455 - acc: 0.5050 - val\_loss: 2.0995 - val\_acc: 0.5032
Epoch 8/100
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0264 - acc: 0.4993 - val\_loss: 2.2232 - val\_acc: 0.4666
Epoch 9/100
35064/35064 [==============================] - 59s 2ms/step - loss: 2.0323 - acc: 0.5002 - val\_loss: 2.1863 - val\_acc: 0.4942
Epoch 10/100
35064/35064 [==============================] - 59s 2ms/step - loss: 2.1283 - acc: 0.4834 - val\_loss: 2.0571 - val\_acc: 0.4985
Epoch 11/100
35064/35064 [==============================] - 58s 2ms/step - loss: 2.0327 - acc: 0.4983 - val\_loss: 2.0631 - val\_acc: 0.4992
Epoch 12/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.9203 - acc: 0.5077 - val\_loss: 1.8940 - val\_acc: 0.5087
Epoch 13/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.8924 - acc: 0.5101 - val\_loss: 1.9657 - val\_acc: 0.4993
Epoch 14/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.8121 - acc: 0.5177 - val\_loss: 1.8634 - val\_acc: 0.5090
Epoch 15/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.7645 - acc: 0.5194 - val\_loss: 1.8149 - val\_acc: 0.5144
Epoch 16/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.7836 - acc: 0.5104 - val\_loss: 1.7789 - val\_acc: 0.5123
Epoch 17/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.6891 - acc: 0.5188 - val\_loss: 1.7411 - val\_acc: 0.5135
Epoch 18/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.6509 - acc: 0.5259 - val\_loss: 1.7205 - val\_acc: 0.5165
Epoch 19/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.6204 - acc: 0.5287 - val\_loss: 1.7205 - val\_acc: 0.5106
Epoch 20/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.6823 - acc: 0.5147 - val\_loss: 1.7193 - val\_acc: 0.5051
Epoch 21/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.6462 - acc: 0.5213 - val\_loss: 1.7336 - val\_acc: 0.5170
Epoch 22/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.6073 - acc: 0.5314 - val\_loss: 1.6314 - val\_acc: 0.5269
Epoch 23/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.5973 - acc: 0.5238 - val\_loss: 1.6308 - val\_acc: 0.5221
Epoch 24/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.5404 - acc: 0.5380 - val\_loss: 1.6044 - val\_acc: 0.5306
Epoch 25/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.5084 - acc: 0.5461 - val\_loss: 1.5830 - val\_acc: 0.5356
Epoch 26/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.5221 - acc: 0.5459 - val\_loss: 1.5750 - val\_acc: 0.5404
Epoch 27/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.5288 - acc: 0.5424 - val\_loss: 1.5604 - val\_acc: 0.5444
Epoch 28/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.4951 - acc: 0.5525 - val\_loss: 1.5434 - val\_acc: 0.5557
Epoch 29/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.4904 - acc: 0.5580 - val\_loss: 1.5377 - val\_acc: 0.5536
Epoch 30/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.4696 - acc: 0.5624 - val\_loss: 1.5235 - val\_acc: 0.5528
Epoch 31/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.4464 - acc: 0.5646 - val\_loss: 1.5441 - val\_acc: 0.5672
Epoch 32/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.4391 - acc: 0.5745 - val\_loss: 1.5159 - val\_acc: 0.5703
Epoch 33/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.4493 - acc: 0.5721 - val\_loss: 1.5228 - val\_acc: 0.5662
Epoch 34/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.4850 - acc: 0.5663 - val\_loss: 1.4857 - val\_acc: 0.5649
Epoch 35/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.4124 - acc: 0.5765 - val\_loss: 1.4864 - val\_acc: 0.5695
Epoch 36/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.3934 - acc: 0.5779 - val\_loss: 1.4598 - val\_acc: 0.5752
Epoch 37/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.3837 - acc: 0.5777 - val\_loss: 1.4718 - val\_acc: 0.5679
Epoch 38/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.3533 - acc: 0.5833 - val\_loss: 1.4287 - val\_acc: 0.5786
Epoch 39/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.3376 - acc: 0.5860 - val\_loss: 1.4057 - val\_acc: 0.5812
Epoch 40/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.3316 - acc: 0.5890 - val\_loss: 1.4088 - val\_acc: 0.5786
Epoch 41/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.3376 - acc: 0.5866 - val\_loss: 1.4088 - val\_acc: 0.5801
Epoch 42/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.3114 - acc: 0.5914 - val\_loss: 1.3969 - val\_acc: 0.5772
Epoch 43/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.3357 - acc: 0.5882 - val\_loss: 1.3899 - val\_acc: 0.5762
Epoch 44/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.3115 - acc: 0.5924 - val\_loss: 1.3898 - val\_acc: 0.5784
Epoch 45/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.3104 - acc: 0.5927 - val\_loss: 1.3892 - val\_acc: 0.5848
Epoch 46/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.3144 - acc: 0.5928 - val\_loss: 1.3751 - val\_acc: 0.5816
Epoch 47/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2659 - acc: 0.6024 - val\_loss: 1.3494 - val\_acc: 0.5880
Epoch 48/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2574 - acc: 0.6076 - val\_loss: 1.3501 - val\_acc: 0.5919
Epoch 49/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2507 - acc: 0.6112 - val\_loss: 1.3569 - val\_acc: 0.5942
Epoch 50/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.2634 - acc: 0.6143 - val\_loss: 1.3209 - val\_acc: 0.5963
Epoch 51/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2693 - acc: 0.6108 - val\_loss: 1.3647 - val\_acc: 0.5959
Epoch 52/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2784 - acc: 0.6139 - val\_loss: 1.3483 - val\_acc: 0.6054
Epoch 53/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.2380 - acc: 0.6267 - val\_loss: 1.3686 - val\_acc: 0.5959
Epoch 54/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.2719 - acc: 0.6189 - val\_loss: 1.3136 - val\_acc: 0.6170
Epoch 55/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.2449 - acc: 0.6274 - val\_loss: 1.2977 - val\_acc: 0.6143
Epoch 56/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.2327 - acc: 0.6263 - val\_loss: 1.2752 - val\_acc: 0.6201
Epoch 57/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2269 - acc: 0.6320 - val\_loss: 1.3151 - val\_acc: 0.6116
Epoch 58/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.2235 - acc: 0.6297 - val\_loss: 1.2591 - val\_acc: 0.6198
Epoch 59/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2107 - acc: 0.6349 - val\_loss: 1.2542 - val\_acc: 0.6213
Epoch 60/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2178 - acc: 0.6308 - val\_loss: 1.2633 - val\_acc: 0.6226
Epoch 61/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.2015 - acc: 0.6346 - val\_loss: 1.2614 - val\_acc: 0.6242
Epoch 62/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.1846 - acc: 0.6397 - val\_loss: 1.2434 - val\_acc: 0.6242
Epoch 63/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1772 - acc: 0.6417 - val\_loss: 1.2407 - val\_acc: 0.6314
Epoch 64/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1691 - acc: 0.6450 - val\_loss: 1.2285 - val\_acc: 0.6337
Epoch 65/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1527 - acc: 0.6493 - val\_loss: 1.2137 - val\_acc: 0.6327
Epoch 66/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.1480 - acc: 0.6488 - val\_loss: 1.2001 - val\_acc: 0.6359
Epoch 67/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1420 - acc: 0.6503 - val\_loss: 1.1836 - val\_acc: 0.6360
Epoch 68/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.1284 - acc: 0.6558 - val\_loss: 1.1881 - val\_acc: 0.6415
Epoch 69/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1256 - acc: 0.6573 - val\_loss: 1.1806 - val\_acc: 0.6431
Epoch 70/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1214 - acc: 0.6590 - val\_loss: 1.2054 - val\_acc: 0.6411
Epoch 71/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.1367 - acc: 0.6577 - val\_loss: 1.1807 - val\_acc: 0.6446
Epoch 72/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1034 - acc: 0.6628 - val\_loss: 1.1971 - val\_acc: 0.6427
Epoch 73/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.1047 - acc: 0.6630 - val\_loss: 1.1752 - val\_acc: 0.6436
Epoch 74/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1095 - acc: 0.6642 - val\_loss: 1.1818 - val\_acc: 0.6448
Epoch 75/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.1175 - acc: 0.6602 - val\_loss: 1.2163 - val\_acc: 0.6403
Epoch 76/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1196 - acc: 0.6613 - val\_loss: 1.1859 - val\_acc: 0.6444
Epoch 77/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1133 - acc: 0.6634 - val\_loss: 1.1784 - val\_acc: 0.6454
Epoch 78/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0926 - acc: 0.6722 - val\_loss: 1.1610 - val\_acc: 0.6484
Epoch 79/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.0735 - acc: 0.6763 - val\_loss: 1.1661 - val\_acc: 0.6445
Epoch 80/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.1001 - acc: 0.6727 - val\_loss: 1.1754 - val\_acc: 0.6446
Epoch 81/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.0798 - acc: 0.6753 - val\_loss: 1.1715 - val\_acc: 0.6431
Epoch 82/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.1046 - acc: 0.6724 - val\_loss: 1.1709 - val\_acc: 0.6394
Epoch 83/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0637 - acc: 0.6780 - val\_loss: 1.1539 - val\_acc: 0.6512
Epoch 84/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0622 - acc: 0.6801 - val\_loss: 1.1515 - val\_acc: 0.6500
Epoch 85/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0657 - acc: 0.6826 - val\_loss: 1.1527 - val\_acc: 0.6535
Epoch 86/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.0762 - acc: 0.6751 - val\_loss: 1.1558 - val\_acc: 0.6439
Epoch 87/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.0475 - acc: 0.6840 - val\_loss: 1.1389 - val\_acc: 0.6529
Epoch 88/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0523 - acc: 0.6857 - val\_loss: 1.1541 - val\_acc: 0.6481
Epoch 89/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0352 - acc: 0.6936 - val\_loss: 1.1574 - val\_acc: 0.6542
Epoch 90/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.0225 - acc: 0.6980 - val\_loss: 1.1651 - val\_acc: 0.6500
Epoch 91/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0432 - acc: 0.6889 - val\_loss: 1.1603 - val\_acc: 0.6504
Epoch 92/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0307 - acc: 0.6900 - val\_loss: 1.1665 - val\_acc: 0.6490
Epoch 93/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0205 - acc: 0.6957 - val\_loss: 1.2399 - val\_acc: 0.6418
Epoch 94/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.0279 - acc: 0.6943 - val\_loss: 1.1623 - val\_acc: 0.6498
Epoch 95/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0204 - acc: 0.6948 - val\_loss: 1.1570 - val\_acc: 0.6550
Epoch 96/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0206 - acc: 0.6960 - val\_loss: 1.1521 - val\_acc: 0.6566
Epoch 97/100
35064/35064 [==============================] - 59s 2ms/step - loss: 1.0284 - acc: 0.6941 - val\_loss: 1.1601 - val\_acc: 0.6522
Epoch 98/100
35064/35064 [==============================] - 60s 2ms/step - loss: 1.0254 - acc: 0.6939 - val\_loss: 1.1795 - val\_acc: 0.6514
Epoch 99/100
35064/35064 [==============================] - 59s 2ms/step - loss: 0.9983 - acc: 0.7044 - val\_loss: 1.1605 - val\_acc: 0.6513
Epoch 100/100
35064/35064 [==============================] - 60s 2ms/step - loss: 0.9850 - acc: 0.7069 - val\_loss: 1.1740 - val\_acc: 0.6537
8766/8766 [==============================] - 8s 881us/step
Test score:  1.173977042261716
Test accuracy:  0.653661878011609

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{p}{,} \PY{n}{history}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{sequential}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_202 (InputLayer)          (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_203 (InputLayer)          (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_204 (InputLayer)          (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_125 (Model)               (None, 186, 5)       505         input\_202[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_126 (Model)               (None, 186, 5)       105         input\_203[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_127 (Model)               (None, 186, 5)       155         input\_204[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_17 (Concatenate)    (None, 186, 15)      0           model\_125[1][0]                  
                                                                 model\_126[1][0]                  
                                                                 model\_127[1][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_197 (Conv1D)             (None, 177, 5)       755         concatenate\_17[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_191 (MaxPooling1D (None, 17, 5)        0           conv1d\_197[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_75 (Flatten)            (None, 85)           0           max\_pooling1d\_191[0][0]          
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_83 (Dense)                (None, 500)          43000       flatten\_75[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_84 (Dense)                (None, 500)          250500      dense\_83[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_85 (Dense)                (None, 5)            2505        dense\_84[0][0]                   
==================================================================================================
Total params: 297,525
Trainable params: 297,525
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.6536618754277892
kappa:  0.5117412469913332

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Channels are stacked in order to apply 2D
convolution}\label{channels-are-stacked-in-order-to-apply-2d-convolution}

submodels output are not concatenate on filters channels but stacked and
the result is fed to a 2D convolution model - sub convolution models for
eeg, pulse and accelerometer have only one convolution layer and a
maxPool layer. Kernel size are 10 times greater than in previous - eeg:
nb\_filter1=10, kernel1=250, maxPool1=20 - pulse \& accelerometers:
nb\_filter1=10, kernel1=100, maxPool1=8 - submodels output are stacked
to a input for a 2D convolution layer(186, 5, 1) - output is fed in a 2D
convolution layer (nb\_filter = 50, kernel\_size=(40, 30)) and a maxPool
layer (20, 1) - a flatten layer - two dense layers with 300 neurons - no
regularization

\begin{figure}
\centering
\includegraphics{attachment:model_stacked.png}
\caption{model\_stacked.png}
\end{figure}

 Model performance for 100 epochs - accurancy: 0.73 - kappa: 0.62

\includegraphics{attachment:ModelStacked_acc.png}
\includegraphics{attachment:ModelStacked_loss.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{} 2D input(samples, rows, cols, channels) with channel last }
         \PY{c+c1}{\PYZsh{}1D output (batch\PYZus{}size, new\PYZus{}steps, filters) (None, 186, 5) / for inputs shape=eeg\PYZus{}train.shape[1:3]}
         
         \PY{n}{tensor\PYZus{}board} \PY{o}{=} \PY{n}{TensorBoard}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./logs/stacked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{10}
         \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{50}
         
         \PY{n}{model\PYZus{}eeg} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{250}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
         \PY{n}{model\PYZus{}pulse} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,}\PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
         \PY{n}{model\PYZus{}acc} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
         
         \PY{n}{in\PYZus{}eeg} \PY{o}{=}  \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}pulse} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{in\PYZus{}acc} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{accelerometer\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{out\PYZus{}eeg} \PY{o}{=} \PY{n}{model\PYZus{}eeg}\PY{p}{(}\PY{n}{in\PYZus{}eeg}\PY{p}{)}
         \PY{n}{out\PYZus{}pulse} \PY{o}{=} \PY{n}{model\PYZus{}pulse}\PY{p}{(}\PY{n}{in\PYZus{}pulse}\PY{p}{)}
         \PY{n}{out\PYZus{}acc} \PY{o}{=} \PY{n}{model\PYZus{}acc}\PY{p}{(}\PY{n}{in\PYZus{}acc}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}conv1D output (batch\PYZus{}size, new\PYZus{}steps, filters)}
         
         \PY{n}{merged} \PY{o}{=} \PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{out\PYZus{}eeg}\PY{p}{,} \PY{n}{out\PYZus{}pulse}\PY{p}{,} \PY{n}{out\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{}batch\PYZus{}size = merged.shape[0]}
         
         \PY{n}{steps} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{merged}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}print(\PYZdq{}steps\PYZdq{}, steps)}
         \PY{n}{filters} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{merged}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{stacked\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{steps}\PY{p}{,} \PY{n}{filters}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{stacked\PYZus{}shape}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} shape (None, 186, 15 = nb\PYZus{}filter1*3) | (batch\PYZus{}size, new\PYZus{}steps, filters)}
         
         \PY{n}{stacked} \PY{o}{=} \PY{n}{Reshape}\PY{p}{(}\PY{p}{(}\PY{n}{steps}\PY{p}{,} \PY{n}{filters}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n}{merged}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}conv2D inputs (samples, rows, cols, channels)}
         
         \PY{n}{conv3} \PY{o}{=} \PY{n}{Conv2D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{n}{nb\PYZus{}filter2}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{40}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                        \PY{n}{data\PYZus{}format}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{channels\PYZus{}last}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{(}\PY{n}{stacked}\PY{p}{)}
         
         
         \PY{n}{pool3} \PY{o}{=} \PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n}{conv3}\PY{p}{)}
         
         \PY{n}{flat} \PY{o}{=} \PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{(}\PY{n}{pool3}\PY{p}{)}
         
         \PY{n}{dense1} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{flat}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}drop1 = Dropout(0.5)(dense1)}
         
         \PY{n}{dense2} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense1}\PY{p}{)}
         \PY{n}{out} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense2}\PY{p}{)}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{p}{[}\PY{n}{in\PYZus{}eeg}\PY{p}{,} \PY{n}{in\PYZus{}pulse}\PY{p}{,} \PY{n}{in\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{out}\PY{p}{)}
         
         
         \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}plot\PYZus{}model(model, to\PYZus{}file=\PYZsq{}model\PYZus{}stacked.png\PYZsq{}, show\PYZus{}shapes=True, show\PYZus{}layer\PYZus{}names=True)}
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=250, activation="relu", strides=1, padding="valid", filters=10)`
  app.launch\_new\_instance()
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=100, activation="relu", strides=1, padding="valid", filters=10)`
  app.launch\_new\_instance()
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel\_size=100, activation="relu", strides=1, padding="valid", filters=10)`
  app.launch\_new\_instance()

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(None, 175, 30, 1)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}envs\textbackslash{}tf-gpu\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel\_size=(40, 30), activation="relu", strides=(1, 1), padding="valid", data\_format="channels\_last", filters=50)`

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_70 (InputLayer)           (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_71 (InputLayer)           (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_72 (InputLayer)           (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_45 (Model)                (None, 175, 10)      10010       input\_70[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_46 (Model)                (None, 175, 10)      2010        input\_71[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_47 (Model)                (None, 175, 10)      3010        input\_72[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_11 (Concatenate)    (None, 175, 30)      0           model\_45[1][0]                   
                                                                 model\_46[1][0]                   
                                                                 model\_47[1][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
reshape\_1 (Reshape)             (None, 175, 30, 1)   0           concatenate\_11[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_1 (Conv2D)               (None, 136, 1, 50)   60050       reshape\_1[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_1 (MaxPooling2D)  (None, 6, 1, 50)     0           conv2d\_1[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_30 (Flatten)            (None, 300)          0           max\_pooling2d\_1[0][0]            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_34 (Dense)                (None, 300)          90300       flatten\_30[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_35 (Dense)                (None, 300)          90300       dense\_34[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_36 (Dense)                (None, 5)            1505        dense\_35[0][0]                   
==================================================================================================
Total params: 257,185
Trainable params: 257,185
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}131}]:} \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{100}
          \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{epochs}\PY{o}{=} \PY{n}{epochs}\PY{p}{,} 
                              \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{tensor\PYZus{}board}\PY{p}{]}\PY{p}{)}
          \PY{n}{score} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test score: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test accuracy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 35064 samples, validate on 8766 samples
Epoch 1/100
35064/35064 [==============================] - 64s 2ms/step - loss: 2.6952 - acc: 0.3854 - val\_loss: 2.3020 - val\_acc: 0.4168
Epoch 2/100
35064/35064 [==============================] - 62s 2ms/step - loss: 2.3729 - acc: 0.4332 - val\_loss: 2.1876 - val\_acc: 0.4797
Epoch 3/100
35064/35064 [==============================] - 62s 2ms/step - loss: 2.0505 - acc: 0.5011 - val\_loss: 2.0814 - val\_acc: 0.4943
Epoch 4/100
35064/35064 [==============================] - 62s 2ms/step - loss: 2.0111 - acc: 0.4908 - val\_loss: 2.1176 - val\_acc: 0.5042
Epoch 5/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.8082 - acc: 0.5387 - val\_loss: 1.7440 - val\_acc: 0.5460
Epoch 6/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.7460 - acc: 0.5410 - val\_loss: 1.6461 - val\_acc: 0.5534
Epoch 7/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.6415 - acc: 0.5443 - val\_loss: 1.6004 - val\_acc: 0.5533
Epoch 8/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.5232 - acc: 0.5604 - val\_loss: 1.4933 - val\_acc: 0.5532
Epoch 9/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.4941 - acc: 0.5569 - val\_loss: 1.5174 - val\_acc: 0.5480
Epoch 10/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.4194 - acc: 0.5683 - val\_loss: 1.4044 - val\_acc: 0.5820
Epoch 11/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.5482 - acc: 0.5413 - val\_loss: 1.4112 - val\_acc: 0.5837
Epoch 12/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.3700 - acc: 0.5936 - val\_loss: 1.3270 - val\_acc: 0.6068
Epoch 13/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.4996 - acc: 0.5661 - val\_loss: 1.4577 - val\_acc: 0.5827
Epoch 14/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.3688 - acc: 0.5923 - val\_loss: 1.3358 - val\_acc: 0.6022
Epoch 15/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.2934 - acc: 0.6107 - val\_loss: 1.2822 - val\_acc: 0.6103
Epoch 16/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.2890 - acc: 0.6184 - val\_loss: 1.2744 - val\_acc: 0.6149
Epoch 17/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.2524 - acc: 0.6235 - val\_loss: 1.2416 - val\_acc: 0.6271
Epoch 18/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.3087 - acc: 0.5967 - val\_loss: 1.2778 - val\_acc: 0.6036
Epoch 19/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.2825 - acc: 0.6079 - val\_loss: 1.2443 - val\_acc: 0.6250
Epoch 20/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.2252 - acc: 0.6283 - val\_loss: 1.2062 - val\_acc: 0.6299
Epoch 21/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1936 - acc: 0.6378 - val\_loss: 1.1706 - val\_acc: 0.6358
Epoch 22/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1629 - acc: 0.6421 - val\_loss: 1.1811 - val\_acc: 0.6373
Epoch 23/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1702 - acc: 0.6427 - val\_loss: 1.1498 - val\_acc: 0.6407
Epoch 24/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1660 - acc: 0.6312 - val\_loss: 1.1318 - val\_acc: 0.6437
Epoch 25/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1155 - acc: 0.6525 - val\_loss: 1.1149 - val\_acc: 0.6498
Epoch 26/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1190 - acc: 0.6542 - val\_loss: 1.1197 - val\_acc: 0.6510
Epoch 27/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1149 - acc: 0.6494 - val\_loss: 1.1188 - val\_acc: 0.6484
Epoch 28/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1193 - acc: 0.6565 - val\_loss: 1.0856 - val\_acc: 0.6616
Epoch 29/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1133 - acc: 0.6558 - val\_loss: 1.3606 - val\_acc: 0.6282
Epoch 30/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1502 - acc: 0.6505 - val\_loss: 1.1089 - val\_acc: 0.6517
Epoch 31/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.1026 - acc: 0.6585 - val\_loss: 1.1285 - val\_acc: 0.6417
Epoch 32/100
35064/35064 [==============================] - 62s 2ms/step - loss: 1.1199 - acc: 0.6501 - val\_loss: 1.0757 - val\_acc: 0.6620
Epoch 33/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0677 - acc: 0.6665 - val\_loss: 1.0811 - val\_acc: 0.6654
Epoch 34/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0495 - acc: 0.6753 - val\_loss: 1.1094 - val\_acc: 0.6570
Epoch 35/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0637 - acc: 0.6742 - val\_loss: 1.0821 - val\_acc: 0.6624
Epoch 36/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0489 - acc: 0.6751 - val\_loss: 1.0975 - val\_acc: 0.6671
Epoch 37/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0444 - acc: 0.6789 - val\_loss: 1.0364 - val\_acc: 0.6782
Epoch 38/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0390 - acc: 0.6841 - val\_loss: 1.0263 - val\_acc: 0.6874
Epoch 39/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0416 - acc: 0.6855 - val\_loss: 1.0734 - val\_acc: 0.6774
Epoch 40/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0740 - acc: 0.6780 - val\_loss: 1.0742 - val\_acc: 0.6791
Epoch 41/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0411 - acc: 0.6904 - val\_loss: 1.0606 - val\_acc: 0.6815
Epoch 42/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0429 - acc: 0.6864 - val\_loss: 1.0398 - val\_acc: 0.6800
Epoch 43/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0337 - acc: 0.6847 - val\_loss: 1.0258 - val\_acc: 0.6855
Epoch 44/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0237 - acc: 0.6882 - val\_loss: 1.0281 - val\_acc: 0.6841
Epoch 45/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0125 - acc: 0.6900 - val\_loss: 1.0402 - val\_acc: 0.6792
Epoch 46/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0469 - acc: 0.6766 - val\_loss: 1.0370 - val\_acc: 0.6790
Epoch 47/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9881 - acc: 0.6974 - val\_loss: 1.0342 - val\_acc: 0.6906
Epoch 48/100
35064/35064 [==============================] - 62s 2ms/step - loss: 0.9991 - acc: 0.7022 - val\_loss: 1.0411 - val\_acc: 0.6862
Epoch 49/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9812 - acc: 0.7067 - val\_loss: 1.0200 - val\_acc: 0.6977
Epoch 50/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0171 - acc: 0.6934 - val\_loss: 1.0304 - val\_acc: 0.6899
Epoch 51/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9824 - acc: 0.7052 - val\_loss: 1.0665 - val\_acc: 0.6757
Epoch 52/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0098 - acc: 0.6949 - val\_loss: 1.0819 - val\_acc: 0.6783
Epoch 53/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0565 - acc: 0.6833 - val\_loss: 1.0659 - val\_acc: 0.6710
Epoch 54/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0104 - acc: 0.6949 - val\_loss: 1.0222 - val\_acc: 0.6875
Epoch 55/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0052 - acc: 0.7008 - val\_loss: 1.0448 - val\_acc: 0.6994
Epoch 56/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9843 - acc: 0.7073 - val\_loss: 1.0435 - val\_acc: 0.6931
Epoch 57/100
35064/35064 [==============================] - 61s 2ms/step - loss: 1.0008 - acc: 0.6994 - val\_loss: 1.0406 - val\_acc: 0.6828
Epoch 58/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9943 - acc: 0.6958 - val\_loss: 1.0122 - val\_acc: 0.6845
Epoch 59/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9792 - acc: 0.6964 - val\_loss: 1.0083 - val\_acc: 0.6859
Epoch 60/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9841 - acc: 0.6998 - val\_loss: 1.0231 - val\_acc: 0.6943
Epoch 61/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9622 - acc: 0.7085 - val\_loss: 1.0169 - val\_acc: 0.6962
Epoch 62/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9362 - acc: 0.7195 - val\_loss: 0.9626 - val\_acc: 0.7092
Epoch 63/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9242 - acc: 0.7181 - val\_loss: 1.0499 - val\_acc: 0.6764
Epoch 64/100
35064/35064 [==============================] - 62s 2ms/step - loss: 0.9569 - acc: 0.7121 - val\_loss: 0.9709 - val\_acc: 0.7070
Epoch 65/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9615 - acc: 0.7124 - val\_loss: 0.9785 - val\_acc: 0.7023
Epoch 66/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9683 - acc: 0.7058 - val\_loss: 1.0656 - val\_acc: 0.6653
Epoch 67/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9485 - acc: 0.7141 - val\_loss: 1.0431 - val\_acc: 0.6783
Epoch 68/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9311 - acc: 0.7231 - val\_loss: 0.9936 - val\_acc: 0.7003
Epoch 69/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9517 - acc: 0.7163 - val\_loss: 0.9951 - val\_acc: 0.6960
Epoch 70/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9239 - acc: 0.7225 - val\_loss: 0.9821 - val\_acc: 0.7070
Epoch 71/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9115 - acc: 0.7291 - val\_loss: 0.9532 - val\_acc: 0.7164
Epoch 72/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9268 - acc: 0.7215 - val\_loss: 0.9490 - val\_acc: 0.7080
Epoch 73/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9254 - acc: 0.7204 - val\_loss: 0.9575 - val\_acc: 0.7101
Epoch 74/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9011 - acc: 0.7286 - val\_loss: 0.9626 - val\_acc: 0.7065
Epoch 75/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8900 - acc: 0.7310 - val\_loss: 0.9356 - val\_acc: 0.7186
Epoch 76/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8830 - acc: 0.7353 - val\_loss: 0.9369 - val\_acc: 0.7187
Epoch 77/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9339 - acc: 0.7204 - val\_loss: 0.9634 - val\_acc: 0.7056
Epoch 78/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8918 - acc: 0.7325 - val\_loss: 1.0537 - val\_acc: 0.6800
Epoch 79/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9278 - acc: 0.7213 - val\_loss: 0.9501 - val\_acc: 0.7126
Epoch 80/100
35064/35064 [==============================] - 62s 2ms/step - loss: 0.9045 - acc: 0.7346 - val\_loss: 0.9644 - val\_acc: 0.7123
Epoch 81/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9048 - acc: 0.7313 - val\_loss: 0.9564 - val\_acc: 0.7101
Epoch 82/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9150 - acc: 0.7283 - val\_loss: 0.9667 - val\_acc: 0.7126
Epoch 83/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9224 - acc: 0.7253 - val\_loss: 0.9658 - val\_acc: 0.7051
Epoch 84/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8885 - acc: 0.7332 - val\_loss: 0.9796 - val\_acc: 0.7085
Epoch 85/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8710 - acc: 0.7400 - val\_loss: 0.9456 - val\_acc: 0.7164
Epoch 86/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8882 - acc: 0.7344 - val\_loss: 0.9703 - val\_acc: 0.7115
Epoch 87/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8786 - acc: 0.7374 - val\_loss: 0.9370 - val\_acc: 0.7202
Epoch 88/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8631 - acc: 0.7438 - val\_loss: 0.9347 - val\_acc: 0.7223
Epoch 89/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8483 - acc: 0.7499 - val\_loss: 0.9289 - val\_acc: 0.7234
Epoch 90/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8491 - acc: 0.7466 - val\_loss: 0.9241 - val\_acc: 0.7217
Epoch 91/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8589 - acc: 0.7441 - val\_loss: 0.9357 - val\_acc: 0.7194
Epoch 92/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9305 - acc: 0.7270 - val\_loss: 1.0138 - val\_acc: 0.6974
Epoch 93/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.9116 - acc: 0.7337 - val\_loss: 0.9598 - val\_acc: 0.7159
Epoch 94/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8875 - acc: 0.7397 - val\_loss: 0.9375 - val\_acc: 0.7219
Epoch 95/100
35064/35064 [==============================] - 62s 2ms/step - loss: 0.8717 - acc: 0.7450 - val\_loss: 0.9353 - val\_acc: 0.7264
Epoch 96/100
35064/35064 [==============================] - 62s 2ms/step - loss: 0.8754 - acc: 0.7432 - val\_loss: 0.9830 - val\_acc: 0.7121
Epoch 97/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8721 - acc: 0.7424 - val\_loss: 0.9316 - val\_acc: 0.7180
Epoch 98/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8572 - acc: 0.7480 - val\_loss: 0.9092 - val\_acc: 0.7301
Epoch 99/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8540 - acc: 0.7509 - val\_loss: 0.9274 - val\_acc: 0.7282
Epoch 100/100
35064/35064 [==============================] - 61s 2ms/step - loss: 0.8415 - acc: 0.7544 - val\_loss: 0.9057 - val\_acc: 0.7328
8766/8766 [==============================] - 6s 691us/step
Test score:  0.9057174706986688
Test accuracy:  0.7328313956678444

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}132}]:} \PY{n}{modelPerf}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{p}{,} \PY{n}{history}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{sequential}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_466 (InputLayer)          (None, 3750, 4)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_467 (InputLayer)          (None, 1500, 2)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
input\_468 (InputLayer)          (None, 1500, 3)      0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_284 (Model)               (None, 175, 10)      10010       input\_466[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_285 (Model)               (None, 175, 10)      2010        input\_467[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
model\_286 (Model)               (None, 175, 10)      3010        input\_468[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
concatenate\_61 (Concatenate)    (None, 175, 30)      0           model\_284[1][0]                  
                                                                 model\_285[1][0]                  
                                                                 model\_286[1][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
reshape\_32 (Reshape)            (None, 175, 30, 1)   0           concatenate\_61[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_24 (Conv2D)              (None, 136, 1, 50)   60050       reshape\_32[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_18 (MaxPooling2D) (None, 6, 1, 50)     0           conv2d\_24[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_91 (Flatten)            (None, 300)          0           max\_pooling2d\_18[0][0]           
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_131 (Dense)               (None, 300)          90300       flatten\_91[0][0]                 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_132 (Dense)               (None, 300)          90300       dense\_131[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_133 (Dense)               (None, 5)            1505        dense\_132[0][0]                  
==================================================================================================
Total params: 257,185
Trainable params: 257,185
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
accurancy:  0.7328313940223591
kappa:  0.6246896898863167

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}133}]:} \PY{n}{tensor\PYZus{}board} \PY{o}{=} \PY{n}{TensorBoard}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./logs/stack\PYZus{}reg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{l+m+mi}{10}
          \PY{n}{nb\PYZus{}filter2}\PY{o}{=}\PY{l+m+mi}{50}
          
          \PY{k}{for} \PY{n}{reg} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}
              \PY{n}{model\PYZus{}eeg} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{250}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
              \PY{n}{model\PYZus{}pulse} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,}\PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
              \PY{n}{model\PYZus{}acc} \PY{o}{=} \PY{n}{submodel\PYZus{}2D}\PY{p}{(}\PY{n}{accelerometer\PYZus{}train}\PY{p}{,} \PY{n}{nb\PYZus{}filter1}\PY{o}{=}\PY{n}{nb\PYZus{}filter1}\PY{p}{,} \PY{n}{kernel1}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{maxPool1}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
          
              \PY{n}{in\PYZus{}eeg} \PY{o}{=}  \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{eeg\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
              \PY{n}{in\PYZus{}pulse} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{pulse\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
              \PY{n}{in\PYZus{}acc} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{accelerometer\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
          
              \PY{n}{out\PYZus{}eeg} \PY{o}{=} \PY{n}{model\PYZus{}eeg}\PY{p}{(}\PY{n}{in\PYZus{}eeg}\PY{p}{)}
              \PY{n}{out\PYZus{}pulse} \PY{o}{=} \PY{n}{model\PYZus{}pulse}\PY{p}{(}\PY{n}{in\PYZus{}pulse}\PY{p}{)}
              \PY{n}{out\PYZus{}acc} \PY{o}{=} \PY{n}{model\PYZus{}acc}\PY{p}{(}\PY{n}{in\PYZus{}acc}\PY{p}{)}
              \PY{c+c1}{\PYZsh{}conv1D output (batch\PYZus{}size, new\PYZus{}steps, filters)}
          
              \PY{n}{merged} \PY{o}{=} \PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{out\PYZus{}eeg}\PY{p}{,} \PY{n}{out\PYZus{}pulse}\PY{p}{,} \PY{n}{out\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} 
          
              \PY{c+c1}{\PYZsh{}batch\PYZus{}size = merged.shape[0]}
          
              \PY{n}{steps} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{merged}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
              \PY{c+c1}{\PYZsh{}print(\PYZdq{}steps\PYZdq{}, steps)}
              \PY{n}{filters} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{merged}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
          
              \PY{n}{stacked\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{steps}\PY{p}{,} \PY{n}{filters}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
          
              \PY{n+nb}{print}\PY{p}{(}\PY{n}{stacked\PYZus{}shape}\PY{p}{)}
              \PY{c+c1}{\PYZsh{} shape (None, 186, 15 = nb\PYZus{}filter1*3) | (batch\PYZus{}size, new\PYZus{}steps, filters)}
          
              \PY{n}{stacked} \PY{o}{=} \PY{n}{Reshape}\PY{p}{(}\PY{p}{(}\PY{n}{steps}\PY{p}{,} \PY{n}{filters}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n}{merged}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{}conv2D inputs (samples, rows, cols, channels)}
          
              \PY{n}{conv3} \PY{o}{=} \PY{n}{Conv2D}\PY{p}{(}\PY{n}{nb\PYZus{}filter}\PY{o}{=}\PY{n}{nb\PYZus{}filter2}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{40}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                             \PY{n}{data\PYZus{}format}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{channels\PYZus{}last}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{(}\PY{n}{stacked}\PY{p}{)}
          
          
              \PY{n}{pool3} \PY{o}{=} \PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n}{conv3}\PY{p}{)}
          
              \PY{n}{flat} \PY{o}{=} \PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{(}\PY{n}{pool3}\PY{p}{)}
          
              \PY{n}{dense1} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{flat}\PY{p}{)}
              \PY{c+c1}{\PYZsh{}drop1 = Dropout(0.5)(dense1)}
          
              \PY{n}{dense2} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{dense1}\PY{p}{)}
              \PY{n}{out} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kernel\PYZus{}regularizer}\PY{o}{=}\PY{n}{regularizers}\PY{o}{.}\PY{n}{l2}\PY{p}{(}\PY{n}{reg}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n}{dense2}\PY{p}{)}
          
              \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{p}{[}\PY{n}{in\PYZus{}eeg}\PY{p}{,} \PY{n}{in\PYZus{}pulse}\PY{p}{,} \PY{n}{in\PYZus{}acc}\PY{p}{]}\PY{p}{,} \PY{n}{out}\PY{p}{)}
          
          
              \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}
          
              \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                            \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizer}\PY{p}{,}
                            \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          
          
              \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{100}
              \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}train}\PY{p}{,} \PY{n}{pulse\PYZus{}train}\PY{p}{,} \PY{n}{accelerometer\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{epochs}\PY{o}{=} \PY{n}{epochs}\PY{p}{,} 
                                  \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{tensor\PYZus{}board}\PY{p}{]}\PY{p}{)}
              \PY{n}{score} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
          
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{for l2.reg:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{reg}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test score: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test accuracy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{score}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
          
              \PY{n}{modelPerf}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{p}{,} \PY{n}{history}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{eeg\PYZus{}test}\PY{p}{,} \PY{n}{pulse\PYZus{}test}\PY{p}{,} \PY{n}{accelerometer\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{sequential}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

          File "<ipython-input-133-3c024f9a61b2>", line 3
        nb\_filter1=10
        \^{}
    IndentationError: unexpected indent
    

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
