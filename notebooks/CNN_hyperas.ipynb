{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from plotnine import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "    \n",
    "#import statsmodels.api as sm\n",
    "\n",
    "#from scipy.stats import mode\n",
    "\n",
    "import random\n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "from pydot import Dot\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import tensorflow as tf\n",
    "#import keras-gpu\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "#from keras.np_utils import probas_to_classes \n",
    "from keras.layers import Dense, Dropout, Input, Reshape\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, merge, Concatenate, Conv2D, MaxPooling2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 158,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Ask tenserflow not to be too gready on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#config = tf.ConfigProto()\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "keras.__version__\n",
    "\n",
    "#with eeg only 5 filters, kernel size 100\n",
    "#Test score:  1.3631985359422534\n",
    "#Test accuracy:  0.6440793987471519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# %load \"C:\\\\Users\\\\i053131\\Desktop\\\\Epilepsie\\\\Dreem\\\\src\\\\utils\\\\error.py\"\n",
    "\n",
    "def AnalyzeError(y_true, y_pred):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    plt.subplot(1,2, 1)\n",
    "    sns.countplot(x=0, data=pd.DataFrame(y_true))\n",
    "    plt.ylim(0, 4000)\n",
    "    plt.subplot(1,2, 2)\n",
    "    sns.countplot(x=0, data=pd.DataFrame(y_pred))\n",
    "    plt.ylim(0, 4000)\n",
    "    fig.suptitle(\"Actual and predicted distribution\", size =  'x-large')\n",
    "    plt.show()\n",
    "    \n",
    "    df_ = pd.DataFrame()\n",
    "    df_[\"Test\"]= y_true\n",
    "    df_[\"Pred\"] = y_pred\n",
    "    df_['error'] = df_.Test != df_.Pred\n",
    "    #sns.countplot(x=\"Test\", data=df_[df_.error])\n",
    "    \n",
    "    error0 = df_[(df_.error) & (df_.Test==0)].count()[0] / df_[df_.Test==0].count()[0]\n",
    "    error1 = df_[(df_.error) & (df_.Test==1)].count()[0] / df_[df_.Test==1].count()[0]\n",
    "    error2 = df_[(df_.error) & (df_.Test==2)].count()[0] / df_[df_.Test==2].count()[0]\n",
    "    error3 = df_[(df_.error) & (df_.Test==3)].count()[0] / df_[df_.Test==3].count()[0]\n",
    "    error4 = df_[(df_.error) & (df_.Test==4)].count()[0] / df_[df_.Test==4].count()[0]\n",
    "\n",
    "    Lerror = [error0, error1, error2, error3, error4]\n",
    "    sns.barplot(x=[0, 1, 2, 3, 4], y=Lerror)\n",
    "    plt.title('Wrongly classified in a phase in percent of the test population for this phase')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying with hyperas\n",
    "# downgrade a package :conda install networkx==1.11\n",
    "\n",
    "def data():\n",
    "    dataPath = \"C:\\\\Users\\\\i053131\\\\Desktop\\\\Epilepsie\\\\Dreem\\\\data\\\\raw\\\\\"\n",
    "\n",
    "    trainOutput = pd.read_csv(dataPath + \"challenge_fichier_de_sortie_dentrainement_classification_en_stade_de_sommeil_a_laide_de_signaux_mesures_par_le_bandeau_dreem.csv\", sep=\";\")\n",
    "    Y = trainOutput[\"label\"]\n",
    "\n",
    "    filetrain= dataPath + \"train.h5\"\n",
    "    filetest= dataPath + \"test.h5\"\n",
    "    h5 = h5py.File(filetrain, \"r\")\n",
    "\n",
    "    eeg_1 = pd.DataFrame(h5['eeg_1'][:])\n",
    "    eeg_2 = pd.DataFrame(h5['eeg_2'][:])\n",
    "    eeg_3 = pd.DataFrame(h5['eeg_3'][:])\n",
    "    eeg_4 = pd.DataFrame(h5['eeg_4'][:])\n",
    "    po_ir = pd.DataFrame(h5['po_ir'][:])\n",
    "    po_r = pd.DataFrame(h5['po_r'][:])\n",
    "    accelerometer_x = pd.DataFrame(h5['accelerometer_x'][:])\n",
    "    accelerometer_y = pd.DataFrame(h5['accelerometer_y'][:])\n",
    "    accelerometer_z = pd.DataFrame(h5['accelerometer_z'][:])\n",
    "\n",
    "    df = pd.concat([eeg_1, eeg_2, eeg_3, eeg_4, po_ir, po_r, accelerometer_x, accelerometer_y, accelerometer_z], \n",
    "                   axis=1, sort = False)\n",
    "    df.columns = range(15000 + 3000 + 4500)\n",
    "    df[\"Y\"] = Y\n",
    "\n",
    "    training, test  = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    X = training.iloc[:,:-1]\n",
    "    X_train = X.values\n",
    "    y = training.iloc[:,-1]\n",
    "    y_train = to_categorical(y.values, num_classes=5)\n",
    "    X_test = test.iloc[:,:-1].values\n",
    "    y_true = test.iloc[:,-1].values\n",
    "    y_test = to_categorical(y_true, num_classes=5)\n",
    "\n",
    "    eeg_train = np.empty(shape=(35064, 3750, 4))\n",
    "    eeg_train[:, :, 0]= X_train[:, :3750] #eeg_1\n",
    "    eeg_train[:, :, 1]= X_train[:, 3750:3750*2] #eeg_2\n",
    "    eeg_train[:, :, 2]= X_train[:, 3750*2:3750*3] #eeg_3\n",
    "    eeg_train[:, :, 3]= X_train[:, 3750*3:3750*4] #eeg_4\n",
    "\n",
    "    eeg_test = np.empty(shape=(8766, 3750, 4))\n",
    "    eeg_test[:, :, 0]= X_test[:, :3750] #eeg_1\n",
    "    eeg_test[:, :, 1]= X_test[:, 3750:3750*2] #eeg_2\n",
    "    eeg_test[:, :, 2]= X_test[:, 3750*2:3750*3] #eeg_3\n",
    "    eeg_test[:, :, 3]= X_test[:, 3750*3:3750*4] #eeg_4\n",
    "\n",
    "    print(\"eeg_train\", eeg_train.shape)\n",
    "\n",
    "    pulse_train = np.empty(shape=(35064, 1500, 2))\n",
    "    pulse_train[:, :, 0]= X_train[:, 3750*4 : 3750*4 + 1500] #po_ir\n",
    "    pulse_train[:, :, 1]= X_train[:, 3750*4 + 1500 : 3750*4 +1500*2] #po_r\n",
    "\n",
    "    pulse_test = np.empty(shape=(8766, 1500, 2))\n",
    "    pulse_test[:, :, 0]= X_test[:, 3750*4 : 3750*4 + 1500] #po_ir\n",
    "    pulse_test[:, :, 1]= X_test[:, 3750*4 + 1500 : 3750*4 + 1500*2] #po_r\n",
    "\n",
    "    print(\"pulse_train\", pulse_train.shape)\n",
    "\n",
    "    accelerometer_train = np.empty(shape=(35064, 1500, 3))\n",
    "    accelerometer_train[:, :, 0]= X_train[:, 3750*4 + 1500*2 : 3750*4 + 1500*3] #accelerometer_x\n",
    "    accelerometer_train[:, :, 1]= X_train[:, 3750*4 + 1500*3 : 3750*4 + 1500*4] #accelerometer_y\n",
    "    accelerometer_train[:, :, 2]= X_train[:, 3750*4 + 1500*4 : 3750*4 + 1500*5] #accelerometer_z\n",
    "\n",
    "    accelerometer_test = np.empty(shape=(8766, 1500, 3))\n",
    "    accelerometer_test[:, :, 0]= X_test[:, 3750*4 + 1500*2 : 3750*4 + 1500*3] #accelerometer_x\n",
    "    accelerometer_test[:, :, 1]= X_test[:, 3750*4 + 1500*3 : 3750*4 + 1500*4] #accelerometer_y\n",
    "    accelerometer_test[:, :, 2]= X_test[:, 3750*4 + 1500*4 : 3750*4 + 1500*5] #accelerometer_z\n",
    "\n",
    "    print(\"accelerometer_train\", pulse_train.shape)\n",
    "\n",
    "    x_train = [eeg_train, pulse_train, accelerometer_train] #check\n",
    "    x_test = [eeg_test, pulse_test, accelerometer_test] \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    ksize1=250; ksize2=100; ksize3=100\n",
    "    nb_filter1={{choice([10, 20, 50])}}\n",
    "    nb_filter2={{choice([50, 100, 200])}}\n",
    "    dsize={{choice([250, 300, 350])}}\n",
    "    reg=10; epochs=100\n",
    "    \n",
    "    def submodel_2D(X_train, nb_filter1=5, nb_filter2=20, kernel1=25,  maxPool1=25):\n",
    "        inputs = Input(shape=X_train.shape[1:3])\n",
    "        conv1 = Conv1D(nb_filter=nb_filter1, kernel_size=kernel1, activation='relu', strides = 1, padding='valid')(inputs)\n",
    "        pool1 = MaxPooling1D(maxPool1)(conv1)\n",
    "        return Model(inputs=inputs, outputs=pool1)\n",
    "    \n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    #K.set_session(sess)\n",
    "    \n",
    "    model_eeg = submodel_2D(x_train[0], nb_filter1=nb_filter1, kernel1=ksize1, maxPool1=20)\n",
    "    model_pulse = submodel_2D(x_train[1], nb_filter1=nb_filter1,kernel1=ksize2, maxPool1=8)\n",
    "    model_acc = submodel_2D(x_train[2], nb_filter1=nb_filter1, kernel1=ksize3, maxPool1=8)\n",
    "\n",
    "    in_eeg =  Input(shape=x_train[0].shape[1:3])\n",
    "    in_pulse = Input(shape=x_train[1].shape[1:3])\n",
    "    in_acc = Input(shape=x_train[2].shape[1:3])\n",
    "    out_eeg = model_eeg(in_eeg)\n",
    "    out_pulse = model_pulse(in_pulse)\n",
    "    out_acc = model_acc(in_acc)\n",
    "    merged = concatenate([out_eeg, out_pulse, out_acc], axis=-1) \n",
    "    steps = int(merged.shape[1])\n",
    "    filters = int(merged.shape[2])\n",
    "    stacked_shape = (None, steps, filters, 1)\n",
    "    stacked = Reshape((steps, filters, 1))(merged)\n",
    "    conv3 = Conv2D(nb_filter=nb_filter2, kernel_size=(40, 30), activation='relu', strides=(1, 1), padding='valid', \n",
    "                   data_format=\"channels_last\")(stacked)\n",
    "    pool3 = MaxPooling2D(pool_size=(20, 1))(conv3)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "    dense1 = Dense(dsize, activation='relu')(flat)\n",
    "    #drop1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(dsize, activation='relu')(dense1)\n",
    "    out = Dense(5, activation='softmax', kernel_regularizer=regularizers.l2(reg))(dense2)\n",
    "    model = Model([in_eeg, in_pulse, in_acc], out)\n",
    "    optimizer=Adam(0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs= epochs, \n",
    "                    validation_data = (x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, batch_size=100)\n",
    "    \n",
    "    #y_probas = model.predict(x_test, batch_size=100)\n",
    "    #y_classes = y_probas.argmax(axis=-1) #keras.np_utils.probas_to_classes(y_probas)\n",
    "    #y_pred = pd.DataFrame(y_classes)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    #kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    #arguments = \"Ksize1: {}, Ksize2: {}, Ksize3: {}, nb_filter1: {}, nb_filter2: {}, densitysize: {}, l2: {}, epochs: {}\".format(ksize1, ksize2, ksize3, nb_filter1, nb_filter2, dsize, reg, epochs)\n",
    "\n",
    "    #model = None\n",
    "    #model_eeg = None\n",
    "    #model_loss = None\n",
    "    #model_acc = None\n",
    "    \n",
    "    #return [arguments, loss, accuracy, kappa]\n",
    "    del model\n",
    "    gc.collect()\n",
    "    #if K.backend() == 'tensorflow':\n",
    "    K.clear_session()\n",
    "    \n",
    "    #return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "    \n",
    "#y_true = test.iloc[:,-1].values\n",
    "#y_test = to_categorical(y_true, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import h5py\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.decomposition import PCA\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.cluster import KMeans\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import precision_score, recall_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import cohen_kappa_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import PolynomialFeatures\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.svm import LinearSVC\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.svm import SVC\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.tree import DecisionTreeClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.tree import export_graphviz\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.ensemble import BaggingClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.tree import DecisionTreeClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import GridSearchCV\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import make_scorer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.ensemble import GradientBoostingClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from IPython.display import Image\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pydot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pydot import Dot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import SGD, Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Input, Reshape\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, merge, Concatenate, Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.merge import concatenate, add\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import plot_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import regularizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random as rn\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'nb_filter1': hp.choice('nb_filter1', [10, 20, 50]),\n",
      "        'nb_filter2': hp.choice('nb_filter2', [50, 100, 200]),\n",
      "        'dsize': hp.choice('dsize', [250, 300, 350]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: dataPath = \"C:\\\\Users\\\\i053131\\\\Desktop\\\\Epilepsie\\\\Dreem\\\\data\\\\raw\\\\\"\n",
      "   3: \n",
      "   4: trainOutput = pd.read_csv(dataPath + \"challenge_fichier_de_sortie_dentrainement_classification_en_stade_de_sommeil_a_laide_de_signaux_mesures_par_le_bandeau_dreem.csv\", sep=\";\")\n",
      "   5: Y = trainOutput[\"label\"]\n",
      "   6: \n",
      "   7: filetrain= dataPath + \"train.h5\"\n",
      "   8: filetest= dataPath + \"test.h5\"\n",
      "   9: h5 = h5py.File(filetrain, \"r\")\n",
      "  10: \n",
      "  11: eeg_1 = pd.DataFrame(h5['eeg_1'][:])\n",
      "  12: eeg_2 = pd.DataFrame(h5['eeg_2'][:])\n",
      "  13: eeg_3 = pd.DataFrame(h5['eeg_3'][:])\n",
      "  14: eeg_4 = pd.DataFrame(h5['eeg_4'][:])\n",
      "  15: po_ir = pd.DataFrame(h5['po_ir'][:])\n",
      "  16: po_r = pd.DataFrame(h5['po_r'][:])\n",
      "  17: accelerometer_x = pd.DataFrame(h5['accelerometer_x'][:])\n",
      "  18: accelerometer_y = pd.DataFrame(h5['accelerometer_y'][:])\n",
      "  19: accelerometer_z = pd.DataFrame(h5['accelerometer_z'][:])\n",
      "  20: \n",
      "  21: df = pd.concat([eeg_1, eeg_2, eeg_3, eeg_4, po_ir, po_r, accelerometer_x, accelerometer_y, accelerometer_z], \n",
      "  22:                axis=1, sort = False)\n",
      "  23: df.columns = range(15000 + 3000 + 4500)\n",
      "  24: df[\"Y\"] = Y\n",
      "  25: \n",
      "  26: training, test  = train_test_split(df, test_size=0.2, random_state=42)\n",
      "  27: X = training.iloc[:,:-1]\n",
      "  28: X_train = X.values\n",
      "  29: y = training.iloc[:,-1]\n",
      "  30: y_train = to_categorical(y.values, num_classes=5)\n",
      "  31: X_test = test.iloc[:,:-1].values\n",
      "  32: y_true = test.iloc[:,-1].values\n",
      "  33: y_test = to_categorical(y_true, num_classes=5)\n",
      "  34: \n",
      "  35: eeg_train = np.empty(shape=(35064, 3750, 4))\n",
      "  36: eeg_train[:, :, 0]= X_train[:, :3750] #eeg_1\n",
      "  37: eeg_train[:, :, 1]= X_train[:, 3750:3750*2] #eeg_2\n",
      "  38: eeg_train[:, :, 2]= X_train[:, 3750*2:3750*3] #eeg_3\n",
      "  39: eeg_train[:, :, 3]= X_train[:, 3750*3:3750*4] #eeg_4\n",
      "  40: \n",
      "  41: eeg_test = np.empty(shape=(8766, 3750, 4))\n",
      "  42: eeg_test[:, :, 0]= X_test[:, :3750] #eeg_1\n",
      "  43: eeg_test[:, :, 1]= X_test[:, 3750:3750*2] #eeg_2\n",
      "  44: eeg_test[:, :, 2]= X_test[:, 3750*2:3750*3] #eeg_3\n",
      "  45: eeg_test[:, :, 3]= X_test[:, 3750*3:3750*4] #eeg_4\n",
      "  46: \n",
      "  47: print(\"eeg_train\", eeg_train.shape)\n",
      "  48: \n",
      "  49: pulse_train = np.empty(shape=(35064, 1500, 2))\n",
      "  50: pulse_train[:, :, 0]= X_train[:, 3750*4 : 3750*4 + 1500] #po_ir\n",
      "  51: pulse_train[:, :, 1]= X_train[:, 3750*4 + 1500 : 3750*4 +1500*2] #po_r\n",
      "  52: \n",
      "  53: pulse_test = np.empty(shape=(8766, 1500, 2))\n",
      "  54: pulse_test[:, :, 0]= X_test[:, 3750*4 : 3750*4 + 1500] #po_ir\n",
      "  55: pulse_test[:, :, 1]= X_test[:, 3750*4 + 1500 : 3750*4 + 1500*2] #po_r\n",
      "  56: \n",
      "  57: print(\"pulse_train\", pulse_train.shape)\n",
      "  58: \n",
      "  59: accelerometer_train = np.empty(shape=(35064, 1500, 3))\n",
      "  60: accelerometer_train[:, :, 0]= X_train[:, 3750*4 + 1500*2 : 3750*4 + 1500*3] #accelerometer_x\n",
      "  61: accelerometer_train[:, :, 1]= X_train[:, 3750*4 + 1500*3 : 3750*4 + 1500*4] #accelerometer_y\n",
      "  62: accelerometer_train[:, :, 2]= X_train[:, 3750*4 + 1500*4 : 3750*4 + 1500*5] #accelerometer_z\n",
      "  63: \n",
      "  64: accelerometer_test = np.empty(shape=(8766, 1500, 3))\n",
      "  65: accelerometer_test[:, :, 0]= X_test[:, 3750*4 + 1500*2 : 3750*4 + 1500*3] #accelerometer_x\n",
      "  66: accelerometer_test[:, :, 1]= X_test[:, 3750*4 + 1500*3 : 3750*4 + 1500*4] #accelerometer_y\n",
      "  67: accelerometer_test[:, :, 2]= X_test[:, 3750*4 + 1500*4 : 3750*4 + 1500*5] #accelerometer_z\n",
      "  68: \n",
      "  69: print(\"accelerometer_train\", pulse_train.shape)\n",
      "  70: \n",
      "  71: x_train = [eeg_train, pulse_train, accelerometer_train] #check\n",
      "  72: x_test = [eeg_test, pulse_test, accelerometer_test] \n",
      "  73: \n",
      "  74: \n",
      "  75: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     ksize1=250; ksize2=100; ksize3=100\n",
      "   4:     nb_filter1=space['nb_filter1']\n",
      "   5:     nb_filter2=space['nb_filter2']\n",
      "   6:     dsize=space['dsize']\n",
      "   7:     reg=10; epochs=100\n",
      "   8:     \n",
      "   9:     def submodel_2D(X_train, nb_filter1=5, nb_filter2=20, kernel1=25,  maxPool1=25):\n",
      "  10:         inputs = Input(shape=X_train.shape[1:3])\n",
      "  11:         conv1 = Conv1D(nb_filter=nb_filter1, kernel_size=kernel1, activation='relu', strides = 1, padding='valid')(inputs)\n",
      "  12:         pool1 = MaxPooling1D(maxPool1)(conv1)\n",
      "  13:         return Model(inputs=inputs, outputs=pool1)\n",
      "  14:     \n",
      "  15:     #config = tf.ConfigProto()\n",
      "  16:     #config.gpu_options.allow_growth = True\n",
      "  17:     #sess = tf.Session(config=config)\n",
      "  18:     #K.set_session(sess)\n",
      "  19:     \n",
      "  20:     model_eeg = submodel_2D(x_train[0], nb_filter1=nb_filter1, kernel1=ksize1, maxPool1=20)\n",
      "  21:     model_pulse = submodel_2D(x_train[1], nb_filter1=nb_filter1,kernel1=ksize2, maxPool1=8)\n",
      "  22:     model_acc = submodel_2D(x_train[2], nb_filter1=nb_filter1, kernel1=ksize3, maxPool1=8)\n",
      "  23: \n",
      "  24:     in_eeg =  Input(shape=x_train[0].shape[1:3])\n",
      "  25:     in_pulse = Input(shape=x_train[1].shape[1:3])\n",
      "  26:     in_acc = Input(shape=x_train[2].shape[1:3])\n",
      "  27:     out_eeg = model_eeg(in_eeg)\n",
      "  28:     out_pulse = model_pulse(in_pulse)\n",
      "  29:     out_acc = model_acc(in_acc)\n",
      "  30:     merged = concatenate([out_eeg, out_pulse, out_acc], axis=-1) \n",
      "  31:     steps = int(merged.shape[1])\n",
      "  32:     filters = int(merged.shape[2])\n",
      "  33:     stacked_shape = (None, steps, filters, 1)\n",
      "  34:     stacked = Reshape((steps, filters, 1))(merged)\n",
      "  35:     conv3 = Conv2D(nb_filter=nb_filter2, kernel_size=(40, 30), activation='relu', strides=(1, 1), padding='valid', \n",
      "  36:                    data_format=\"channels_last\")(stacked)\n",
      "  37:     pool3 = MaxPooling2D(pool_size=(20, 1))(conv3)\n",
      "  38: \n",
      "  39:     flat = Flatten()(pool3)\n",
      "  40:     dense1 = Dense(dsize, activation='relu')(flat)\n",
      "  41:     #drop1 = Dropout(0.5)(dense1)\n",
      "  42:     dense2 = Dense(dsize, activation='relu')(dense1)\n",
      "  43:     out = Dense(5, activation='softmax', kernel_regularizer=regularizers.l2(reg))(dense2)\n",
      "  44:     model = Model([in_eeg, in_pulse, in_acc], out)\n",
      "  45:     optimizer=Adam(0.0001)\n",
      "  46:     model.compile(loss='categorical_crossentropy',\n",
      "  47:                   optimizer=optimizer,\n",
      "  48:                   metrics=['accuracy'])\n",
      "  49:     \n",
      "  50:     model.fit(x_train, y_train, batch_size=100, epochs= epochs, \n",
      "  51:                     validation_data = (x_test, y_test))\n",
      "  52:     score = model.evaluate(x_test, y_test, batch_size=100)\n",
      "  53:     \n",
      "  54:     #y_probas = model.predict(x_test, batch_size=100)\n",
      "  55:     #y_classes = y_probas.argmax(axis=-1) #keras.np_utils.probas_to_classes(y_probas)\n",
      "  56:     #y_pred = pd.DataFrame(y_classes)\n",
      "  57:     loss = score[0]\n",
      "  58:     accuracy = score[1]\n",
      "  59:     #kappa = cohen_kappa_score(y_true, y_pred)\n",
      "  60:     #arguments = \"Ksize1: {}, Ksize2: {}, Ksize3: {}, nb_filter1: {}, nb_filter2: {}, densitysize: {}, l2: {}, epochs: {}\".format(ksize1, ksize2, ksize3, nb_filter1, nb_filter2, dsize, reg, epochs)\n",
      "  61: \n",
      "  62:     #model = None\n",
      "  63:     #model_eeg = None\n",
      "  64:     #model_loss = None\n",
      "  65:     #model_acc = None\n",
      "  66:     \n",
      "  67:     #return [arguments, loss, accuracy, kappa]\n",
      "  68:     del model\n",
      "  69:     gc.collect()\n",
      "  70:     #if K.backend() == 'tensorflow':\n",
      "  71:     K.clear_session()\n",
      "  72:     \n",
      "  73:     #return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}\n",
      "  74:     return {'loss': -accuracy, 'status': STATUS_OK}\n",
      "  75: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeg_train (35064, 3750, 4)\n",
      "pulse_train (35064, 1500, 2)\n",
      "accelerometer_train (35064, 1500, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i053131\\Desktop\\Epilepsie\\Dreem\\notebooks\\temp_model.py:338: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_size=250, activation=\"relu\", strides=1, padding=\"valid\", filters=20)`\n",
      "C:\\Users\\i053131\\Desktop\\Epilepsie\\Dreem\\notebooks\\temp_model.py:338: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_size=100, activation=\"relu\", strides=1, padding=\"valid\", filters=20)`\n",
      "C:\\Users\\i053131\\Desktop\\Epilepsie\\Dreem\\notebooks\\temp_model.py:338: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_size=100, activation=\"relu\", strides=1, padding=\"valid\", filters=20)`\n",
      "C:\\Users\\i053131\\Desktop\\Epilepsie\\Dreem\\notebooks\\temp_model.py:363: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(40, 30), activation=\"relu\", strides=(1, 1), padding=\"valid\", data_format=\"channels_last\", filters=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35064 samples, validate on 8766 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(), \n",
    "                                      notebook_name='CNN_hyperas')\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "#print(\"Evalutation of best performing model:\")\n",
    "#print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
